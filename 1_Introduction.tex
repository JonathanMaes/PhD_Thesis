\chapter{Introduction}\label{ch:Introduction}
\glijbaantje{It is important to draw wisdom from many different places.\\If we take it from only one place, it becomes rigid and stale.}{Uncle Iroh}
%\glijbaantje{If a machine is expected to be infallible then it cannot also be intelligent.}{Alan Turing}

\section{Context}
We live in a society where computers are ubiquitous. % and indispesable.
They have been used for a wide variety of applications, like scientific simulations, graphics rendering, processing well-structured data, running the internet\dots\,
Conventional computers --- consisting of transistors arranged into logic gates --- are well suited for such tasks which require exact mathematical operations to be executed.
However, our world provides many challenging tasks that can not be captured by such simple, exact sets of predefined rules.
The archetypical example of this is driving a car, but this extends far into other complex tasks that are often thought of as requiring some degree of ``intelligence'' --- whatever that word may entail exactly. \par
The field of \idx{machine learning} aims to tackle these rather complex tasks using artificial systems, the most prevalent of which nowadays are \xref{artificial neural networks} that take inspiration from the synaptic connections in a biological brain.
Over the years, neural networks have advanced significantly, most recently evidenced by the rise of large language models which have already had a broad societal impact. % the likes of ChatGPT
However, training these systems to the point where they can perform any of the aforementioned tasks at a decent level requires vast amounts of data to fine-tune their ever increasing number of internal parameters --- accompanied by an unsustainably rising energy cost~\cite{QuantumNeuromorphicOpportunities}.
This stands in stark contrast to biological brains, which are capable of learning patterns based on a far more limited set of data, while consuming orders of magnitude less power than an artificial system of equivalent computational capability~\cite{NeuromorphicSpintronics}.
Furthermore, running such neural networks on conventional hardware is encountering limits: to increase performance, the number of trainable parameters is rapidly being increased, but transistor scaling and power density limitations make it hard to keep up with these increasing requirements. % TODO REF
Hence, while advances in training algorithms and the ever increasing wealth of available data create ever more potent machine learning systems, the underlying approach is leading to a bottleneck. \par
Several factors that contribute to this bottleneck are often quoted in the context of neural networks or other machine learning algorithms running on conventional hardware:
\begin{itemize}
	\item Conventional hardware uses transistors to perform logical operations.
	The ever increasing computational demands have driven \xlabel{Moore's law}, which states that the number of transistors on a chip doubles every couple of years.
	However, this scaling law appears untenable as transistors approach nanometre scales.
	Relatedly, \xlabel{Dennard scaling} initially allowed the power density of chips to remain roughly constant during this downscaling, but current leakage has inevitably led to increased power dissipation over the past two decades.
	In an attempt to patch this, multi-core processing has become prevalent, but the high power density still limits the number of cores that can operate simultaneously without overheating. % This results in so-called areas of ``dark silicon'' that are unused at a given instant.
	Thus, new kinds of energy-efficient hardware need to be explored.
	\item Conventional computers are built according to the von Neumann architecture, where computation (CPU) is separated from memory (RAM).
	However, transferring information from memory to the processing unit wastes a lot of time and energy, leading to the infamous \idx{von Neumann bottleneck}.
	The separation between memory and processing is not fundamentally required: biological brains in the natural world exhibit no such divide as they use a vast network of interconnected neurons, yet they are nonetheless capable of learning, processing and retaining information.
	The field of \idx{in-memory computing} concerns itself with bridging this divide in artificial systems.
	\item Artificial neural networks, like any other machine learning system implemented on conventional hardware, are abstracted far away from the physical domain~\cite{RC_ASI}.
	Most advancements arise from improving their algorithms, but these have to be run on a particular (e.g., von Neumann) computer architecture made up of circuits that handle information encoded by physical devices like transistors.
	These many layers of abstraction render this approach inherently inefficient, as it does not directly exploit the physical properties of the constituent materials for computation~\cite{RC_ASI}.
\end{itemize}
These various drawbacks of current implementations have become more noticeable over the past decade, as portrayed in~\cref{fig:1:Ngram}.
This has led to the advent of \idx{neuromorphic computing} --- a field of research at the intersection of biology, computer science and physics --- which takes inspiration from the brain to develop more efficient hardware for processing complex data in parallel.
While artificial neural networks are indeed inspired by the brain, their implementation on conventional hardware is typically not considered part of neuromorphic computing, though approaches to implement neural networks on more efficient unconventional hardware are. % rather than applying algorithms on existing transistor-based hardware.
Note that neuromorphic computing does not seek to replace conventional computing: the latter will always find applications in systems that require absolute precision. % think of physical simulations, the transmission of data, or something as common as rendering an image on a screen.
Neuromorphic computing, on the other hand, does not seek exact solutions; in the words of Alan Turing, ``if a machine is expected to be infallible then it cannot also be intelligent.''
%This paradigm shift opens the door to innovative approaches for tackling computational challenges, particularly in areas like real-time sensing, pattern recognition, and prediction.
%Noise can explore a high dimensional landscape related to cost function by escaping local minima.

\sidefig[0.6]{1_Introduction/Ngrams.pdf}{
	\label{fig:1:Ngram}
	Popularity of various key concepts from the year 2000 onwards.
	The surge in popularity of alternative brain-inspired and physical computing methods coincides with raising concerns related to the usage of conventional hardware.
	Data provided by the \href{https://books.google.com/ngrams/graph?content=Neuromorphic computing,Reservoir computing,In-memory computing,von Neumann bottleneck,Dennard scaling,Material computation&year_start=2000&year_end=2022&corpus=en&smoothing=3&case_insensitive=false}{Google Books Ngram viewer} throughout the English corpus, smoothed over a 3-year period.
	\newline
}

A related concept is material computation~\cite{NeglectedPillar}, which goes one step further: its goal is to find ways to exploit the inherent computing power present in physical substrates.
Whereas classical computing uses a top-down approach where mathematics is imposed onto a physical material --- for instance to construct logic gates --- material computation works in the opposite direction by instead asking what computations a physical object performs naturally~\cite{RC_ASI}.
By designing materials with specific behaviours, systems can be created that naturally exhibit the desired computational properties, reducing the levels of abstraction that currently exist between physics and computation. \par
Among the various substrates being explored for material computation, magnetic and spintronic hardware stand out as particularly promising candidates~\cite{grollier2020neuromorphic,NeuromorphicSpintronicsProspect}.
For example, ferromagnetic nanomagnets have the ability to store information without power, respond non-linearly to stimuli~\cite{NeuromorphicSpintronics} and can dissipate very little energy during information processing~\cite{ThermodynamicLimitsComputation,SpintronicsEnergyEfficientComputing}.
Ensembles of nanomagnets, such as artificial spin ice (an ordered lattice of interacting bistable nanomagnets where all local interactions can not be satisfied simultaneously), add to these advantages since they can exhibit complex collective behaviour like phase transitions and other emergent dynamics~\cite{NeuromorphicSpintronics,RC_ASI}.
As their fabrication has been well-established over the last decade, all these factors have made them a platform of particular interest for scalable and energy-efficient material computation~\cite{PhD_Stromberg}. \par
This finally brings us to the topic of this thesis: ``\emph{\phdtitle}''.
A more detailed description of artificial spin ice will follow at the end of this introduction; first, the next section will elucidate the concept of reservoir computing.

% REF "NeuromorphicSpintronics" provides other REFs: spintronics compared to other neuromorphic computing approaches using quantum materials by Hoffmann et al.3

\newpage
\section{Reservoir computing}\label{sec:1:RC}
% Introduction to RC can take inspiration from: https://arxiv.org/html/2412.13212v1
At the intersection of neuromorphic computing and material computation lies the paradigm of \idx{reservoir computing}.
To understand its underlying concept, we must first take a step back and have a closer look at the aforementioned artificial neural networks.

\subsection{Origin}
\subsubsection{Artificial neural networks}
An \idx{artificial neural network} (ANN) is a computational model that consists of a network of non-linear summing nodes connected by weighted edges~\cite{EvaluatingRestrictedESNs}.
This model was originally inspired by the brain, where neurons and synapses fulfil a similar role as the nodes and weights, respectively.
By ``training'' the weights, a desired output can be obtained~\cite{EvaluatingRestrictedESNs}. \par

A distinction can be made between feed-forward neural networks (FNNs) and recurrent neural networks (RNNs).
In a FNN, abstract neurons are grouped in layers, between which abstracted synaptic connections (or links) exist that enable activations to propagate through the network~\cite{lukovsevivcius2009reservoir}.
These are mainly used for static data processing, such as image classification. \par

Recurrent neural networks (RNN) are conceptually similar to FNNs, with the important distinction that their connection topology possesses cycles~\cite{lukovsevivcius2009reservoir}.
The existence of cycles has a profound impact, as an RNN may develop self-sustained temporal activation dynamics along its recurrent connection pathways, even in the absence of input --- mathematically, this renders an RNN to be a dynamical system, while feed-forward networks are functions~\cite{lukovsevivcius2009reservoir}. % REF verbatim
This enables an RNN to preserve a non-linear transformation of the input history in its internal state, providing dynamical memory and making RNNs suited for dynamic data processing~\cite{lukovsevivcius2009reservoir,RC_RecentAdvances}.
However, while training methods such as backpropagation~\cite{Backpropagation} are often employed in FNNs, the recurrent connections in RNNs make training very computationally expensive and difficult~\cite{EvaluatingRestrictedESNs,Moon_2021,RC_SuperconductingElectronics}.

\xfig{1_Introduction/NeuralNetworks.pdf}{
	Schematic representation of \textbf{(a)} a feed-forward neural network (FNN), \textbf{(b)} a recurrent neural network (RNN) and \textbf{(c)} reservoir computing (RC).
	In the latter, the reservoir is treated as a black box; it can be of mathematical origin, like a RNN, or a physical system like artificial spin ice (ASI).
}


\subsubsection{Liquid state machines and echo state networks}
To address the shortcomings of the backpropagation method, two concepts were independently proposed: liquid state machines (LSMs)~\cite{maass_LSM} and echo state networks (ESNs)~\cite{jaeger2001echo}, which were later unified under the term ``reservoir computing'' methods~\cite{RC_unification}.
These both start from the concept of RNNs, but instead use a randomly generated network of nodes and weights, where the recurrent connections in the network are not trained, but only the weights in the readout are trained.
The concepts of LSMs and ESNs were unified under the term `reservoir computing methods' in~\cite{RC_unification}.

\subsubsection{Reservoir computing}
Reservoir computing is a machine learning framework where a ``reservoir'' --- an unspecified non-linear dynamical system --- maps input data into a higher-dimensional space, which facilitates the separation of said data by a linear transformation~\cite{KUR-24}.
A weighted sum of the components of the reservoir's state vector then provides the output of the system as a whole: this is called a \idx{single-layer perceptron}. % REFs for RC, specifically training a readout layer: Dale et al., 2017; Jensen and Tufte, 2017; Sillin et al., 2013
These weights can be changed in order to produce a desired output, which is referred to as ``training'' the reservoir.
Crucially, \textit{the properties of the reservoir itself are not modified during training} --- only the perceptron is. \par
Since the reservoir acts as a ``black box'', its properties can not be changed to obtain the desired response\footnote{
	It is possible to adjust a physical reservoir to exhibit the desired properties for a given task~\cite{AdaptiveProgrammableRC,gartside2022reconfigurable}, but this is different from mathematically tuning the weights of the perceptron via a simple procedure like linear regression.
}.
Therefore, it is beneficial for the reservoir to be a system with short-term memory and high dimensionality~\cite{NeuromorphicAFMspintronics,RC_RecentAdvances}.
Conveniently, these are properties exhibited by many physical systems~\cite{RC_DipoleNanomagnets,RC_PassiveFrustratedNM,RC_ASI,RC_RecentAdvances,NeuromorphicOscillators,VowelRecognition4STO,RC_DiffusiveMemristors,RC_MemristorTemporal,gartside2022reconfigurable}.
This allows the usage of physical systems without requiring higher mathematical abstractions --- apart from the single-layer perceptron at the end --- that are for example required for calculating the output of extensive neural networks.
When using a physical reservoir to perform a temporal task, the physics of the system are most effectively harnessed when the timescale of system dynamics matches that of incoming data patterns~\cite{KUR-24}.

\paragraph{Mathematical description}
% TODO: figure with right symbols
In the reservoir computing paradigm, an input signal $u(t)$ perturbs a reservoir $\mathcal{R}$, which can be any non-linear dynamical system, be it physical or abstract.
The state of the reservoir following this perturbation can then be represented by a response vector $\vc{r}(t)$.
The final output $\hat{y}(t)$ of the system is then obtained as a weighted sum $\hat{y}(t) = \vc{r}^\mathrm{T}(t) \cdot \vc{w}$ of the components of this vector $\vc{r}(t)$ --- such a weighted sum is also referred to as a single-layer perceptron.
Depending on the task performed, one or multiple perceptrons can be trained, whose interpretation then depends on the context of the specific task.
% Here, we consider the training of just a single perceptron.
When the desired input-output relation $s(t) \mapsto y(t)$ is known for a limited set of data --- the training set --- the weights $\vc{w}$ can be adjusted to minimise the difference between the output $\hat{y}(t)$ and the desired output $y(t)$.
This is usually done by ordinary least-squares (OLS) regression, which requires only a single matrix inversion.
The optimal weights then remain fixed, and the performance of the reservoir can be evaluated by feeding it the test set --- a previously unknown, yet similar data set as the training set.

% TODO: should I explain the mathematics of the matrix inversion here? Can take inspiration from~\cite{RC_NNN} eq. 4--7.
% Training of the weights $\vc{w}$ via linear regression: \hat{y}_i = x_i^T \cdot w + \epsilon_i or in matrix form y = X w + \epsilon. The OLS algorithm adjusts the weight vector w to minimize the MSE between the estimated output and the target function.

\subsubsection{Multi-reservoir techniques}
Since physical reservoirs can often be hard to scale, multiple techniques have been devised to improve performance by combining multiple reservoirs together~\cite{EvaluatingRestrictedESNs,RotatingNeuronsRC} or by time multiplexing~\cite{appeltant2011information}.
\paragraph{Single dynamical node}
\cite{appeltant2011information}
\paragraph{Rotating neurons reservoir}
\cite{RotatingNeuronsRC}
\subsection{Metrics} \label{sec:1:RC_metrics}
% TODO: cite relevant papers, like \cite{NeuromorphicFewShot}: in the Methods section there is a good mathematical introduction on MG, NARMA, and MC/NL, so can use this as reference that MG is mostly memory-driven.
\subsubsection{Kernel-quality, generalisation-capability, compute quality} \label{sec:1:RC_metrics_KQ}
\cite{RC_ASI}
% REF: https://arxiv.org/html/2405.06561v1 explains the kernel-quality metrics quite well with references. Concerning the generalisation-capability, for example: "Generalisation rank (GR) measures how robust the reservoir is to noise and avoiding overfitting. The intent is to produce a measure of whether the matrix can generalise over inputs that are similar. Low GR indicates good robustness to noise."
\subsubsection{Task-agnostic metrics: nonlinearity, memory capacity, complexity (, parity check)}
% TODO: what does PC actually measure? Explain that its calculation is similar to that of memory capacity, but that the added summation and multiplication introduces some non-linearity aspect to it.
\subsubsection{Attractors etc.} % More test-like than metric-like. NARMA, MG, Lorentz
\paragraph{Mackey-Glass}
\subsubsection{Other} % Memorisation, frequency generation, classification
ICT task~\cite{farronato2022reservoir,grollier2020neuromorphic} (simple image classification) % Tasks similar to ICT are performed in those refs
\subsection{Applications or tasks}
Blood glucose level monitoring~\cite{FewMoleculeReservoir}.
\subsubsection{Digital signal processing} % Chaotic time series, speech recognition, TODO
\subsection{Physical RC platforms}
\subsubsection{Why physical systems can be used for RC}
\subsubsection{Physical platforms suitable for RC}
\paragraph{Magnetic} % Rings and ASI
% TODO: consider adding references 308-314 of Pieter's PhD thesis, which concern RC using nanomagnets
In the context of the \spinengine project, three types of magnetic systems were considered.
These are the magnetic nanorings~\cite{DynamicEmergence_NanomagneticSystem}, focused on by the University of Sheffield, in-plane (IP) artificial spin ice (ASI)~\cite{RC_ASI}, researched by NTNU, and out-of-plane (OOP) ASI~\cite{KUR-24}, the primary interest of ETHZ.
Meanwhile, Ghent University provided simulation support for these magnetic systems.
All three present their own advantages and challenges. \par
The nanoring ensembles show promising RC performance, but did not provide straightforward on-chip input and readout methods as would be desirable for applications.
Using anisotropic magnetoresistance (AMR) only provides a single readout value --- thereby obscuring a lot of the system's dynamics --- and requires the use of lock-in amplifiers~\cite{ArchitecturesNanoringRC,Vidamour2023}.
Readout using ferromagnetic resonance (FMR) can provide a higher-dimensional readout in the form of spin-wave spectra~\cite{swindells2024fingerprinting}, but requires bulky waveguides. \par
The potential for RC in IP ASI has been demonstrated numerically~\cite{RC_ASI}, but the experimental application of input and state readout is not straightforward.
An external field is often used for input, though this is undesirable for on-chip applications; instead, it appears possible to use SOT~\cite{SOT_switching_IP}.
Efficient read-out of IP ASI remains challenging, due to the symmetry of the system coupled with the discontinuous nature of the ASI. \par % TODO END: cite roadmap and include more info
Finally, for the OOP ASI efficient input (SOT) and read-out (AMR) mechanisms had been demonstrated experimentally, but their potential for RC had not.
Therefore, this thesis will fill this gap in knowledge and assess the viability of RC with OOP ASI by making use of numerical simulations.
\paragraph{Electronic}
\paragraph{Other}
Reservoir computing is not limited to microscopic substrates of an electronic or magnetic nature.
In fact, the very first example of reservoir computing employed a bucket of water to perform speech recognition: actuators provided input, generating wave patterns that could be used for computation~\cite{PatternRecognition_Bucket}.
An extreme example of physical reservoir computing is tensegrity --- tensile structures arranged in trusses, consisting of both springs and incompressible bars --- where the body of a robot provides both computation and locomotion~\cite{RC_Tensegrity}.

\section{Artificial spin ice}\label{sec:1:ASI}\indexlabel{artificial spin ice} % TODO END: decide how exactly to use these labels, which words to label like this. Perhaps a good rule of thumb is to label the list of abbreviations, for easy reference to their first occurrence in the text.
Artificial spin ice (ASI) is a class of ferromagnetic metamaterial which consists of an ordered lattice of interacting (bistable) nanomagnets, where it is impossible to satisfy all local interactions~\cite{RC_ASI}. % REF verbatim
The magnetisation direction of the constituent nanomagnets can switch between two states under the influence of nearby magnets or thermal fluctuations.

\subsection{Nanomagnet(ism)}
%\cite{Kittel_TheoryFMDomains} tells a lot about how \textbf{domains} like to align \textbf{in small particles}, saying things like the critical domain size etc. and also referring to Frenkel and Dorfman~\cite{FRENKEL1930} where the idea of a single-domain particle was first conceived based on energy considerations.
\subsubsection{Physics} % See masterproef for this, IP vs. OOP also
\subsection{I/O for RC}\label{sec:1:ASI_IO}
\subsubsection{Input}
\paragraph{SOT}
% For more information on symmetry breaking by in-plane field, see~\cite{SOT_Roadmap} p.29, where also 4 other methods for symmetry breaking are presented because an external field is quite impractical. \par
\cite{SOT_FM_AFM,SOTswitchingCoPt,SOT_Roadmap,vlasov2022optimal}
% Energy efficiency of SOT is discussed in~\cite{SpintronicsEnergyEfficientComputing}
\paragraph{STT} % Briefly
\subsubsection{Output}
\paragraph{AHE} % Mentioned in text, so definitely explain
\cite{AHE,AHE_Culcer}
\paragraph{AMR} % Briefly
\paragraph{FMR} % Briefly
\paragraph{SHE} % Briefly
\cite{SHE}
\subsubsection{Imaging} % Imaging can also be seen as an "output", though this should be considered a class of its own.
% Polarized neutron reflectometry \cite{DynamicEmergence_NanomagneticSystem} is another one of those imaging techniques.
\paragraph{MFM} % Mentioned in text, so definitely explain
\paragraph{PEEM}

\chapter{Introduction}\label{ch:Introduction}
\glijbaantje{It is important to draw wisdom from many different places.\\If we take it from only one place, it becomes rigid and stale.}{Uncle Iroh}
%\glijbaantje{If a machine is expected to be infallible then it cannot also be intelligent.}{Alan Turing}

% TODO END: outline of this introduction (what sections will be discussed)

\section{Context}
We live in a society where computers are ubiquitous. % and indispesable.
They have been used for a wide variety of applications, like scientific simulations, graphics rendering, processing well-structured data, running the internet\dots\,
Conventional computers --- consisting of transistors arranged into logic gates --- are well suited for such tasks which require exact mathematical operations to be executed.
However, our world provides many challenging tasks that can not be captured by such simple, exact sets of predefined rules.
The archetypical example of this is driving a car, but this extends far into other complex tasks that are often thought of as requiring some degree of ``intelligence'' --- whatever that word may entail exactly. \par
The field of \idx{machine learning} aims to tackle these rather complex tasks using artificial systems, the most prevalent of which nowadays are \xref{artificial neural networks} that take inspiration from the synaptic connections in a biological brain.
Over the years, neural networks have advanced significantly, most recently evidenced by the rise of large language models which have already had a broad societal impact~\cite{ImprovingLanguageGPT,GPT-4}.
However, training these systems to the point where they can perform any of the aforementioned tasks at a decent level requires vast amounts of data to fine-tune their ever increasing number of internal parameters --- accompanied by an unsustainably rising energy cost~\cite{QuantumNeuromorphicOpportunities,BLOOM_CarbonFootprint_176Bparam}.
This stands in stark contrast to biological brains, which are capable of learning patterns based on a far more limited set of data, while consuming orders of magnitude less power than an artificial system of equivalent computational capability~\cite{NeuromorphicSpintronics}.
Furthermore, running such neural networks on conventional hardware is encountering limits: to increase performance, the number of trainable parameters is rapidly being increased, but transistor scaling and power density limitations make it hard to keep up with these increasing requirements. % TODO REF
Hence, while advances in training algorithms and the ever increasing wealth of available data create ever more potent machine learning systems, the underlying approach is leading to a bottleneck. \par
Several factors that contribute to this bottleneck are often quoted in the context of neural networks or other machine learning algorithms running on conventional hardware:
\begin{itemize} % TODO: more references for these bullet points
	\item Conventional hardware uses transistors to perform logical operations.
	The ever increasing computational demands have driven \xlabel{Moore's law}, which states that the number of transistors on a chip doubles every couple of years.
	However, this scaling law appears untenable as transistors approach nanometre scales.
	Relatedly, \xlabel{Dennard scaling} initially allowed the power density of chips to remain roughly constant during this downscaling, but current leakage has inevitably led to increased power dissipation over the past two decades.
	In an attempt to patch this, multi-core processing has become prevalent, but the high power density still limits the number of cores that can operate simultaneously without overheating. % This results in so-called areas of ``dark silicon'' that are unused at a given instant.
	Thus, new kinds of energy-efficient hardware need to be explored.
	\item Conventional computers are built according to the von Neumann architecture, where computation (CPU) is separated from memory (RAM).
	However, transferring information from memory to the processing unit wastes a lot of time and energy, leading to the infamous \idx{von Neumann bottleneck}~\cite{TaskAdaptivePRC}.
	The separation between memory and processing is not fundamentally required: biological brains in the natural world exhibit no such divide as they use a vast network of interconnected neurons, yet they are nonetheless capable of learning, processing and retaining information.
	The field of \idx{in-memory computing} concerns itself with bridging this divide in artificial systems.
	\item Artificial neural networks, like any other machine learning system implemented on conventional hardware, are abstracted far away from the physical domain~\cite{RC_ASI}.
	Most advancements arise from improving their algorithms, but these have to be run on a particular (e.g., von Neumann) computer architecture made up of circuits that handle information encoded by physical devices like transistors~\cite{RC_SuperconductingElectronics}.
	These many layers of abstraction render this approach inherently inefficient, as it does not directly exploit the physical properties of the constituent materials for computation~\cite{RC_ASI}.
\end{itemize}
These various drawbacks of current implementations have become more noticeable over the past decade, as portrayed in~\cref{fig:1:Ngram}.
This has led to the advent of \idx{neuromorphic computing} --- a field of research at the intersection of biology, computer science and physics --- which takes inspiration from the brain to develop more efficient hardware for processing complex data in parallel.
While artificial neural networks are indeed inspired by the brain, their implementation on conventional hardware is typically not considered part of neuromorphic computing, though approaches to implement neural networks on more efficient unconventional hardware are. % rather than applying algorithms on existing transistor-based hardware.
Note that neuromorphic computing does not seek to replace conventional computing: the latter will always find applications in systems that require absolute precision. % think of physical simulations, the transmission of data, or something as common as rendering an image on a screen.
Neuromorphic computing, on the other hand, does not seek exact solutions; in the words of Alan Turing, ``if a machine is expected to be infallible then it cannot also be intelligent.''
%This paradigm shift opens the door to innovative approaches for tackling computational challenges, particularly in areas like real-time sensing, pattern recognition, and prediction.
%Noise can explore a high dimensional landscape related to cost function by escaping local minima.

\sidefig[0.6]{1_Introduction/Ngrams.pdf}{
	\label{fig:1:Ngram}
	Popularity of various key concepts from the year 2000 onwards.
	The surge in popularity of alternative brain-inspired and physical computing methods coincides with raising concerns related to the usage of conventional hardware.
	Data provided by the \href{https://books.google.com/ngrams/graph?content=Neuromorphic computing,Reservoir computing,In-memory computing,von Neumann bottleneck,Dennard scaling,Material computation&year_start=2000&year_end=2022&corpus=en&smoothing=3&case_insensitive=false}{Google Books Ngram viewer} throughout the English corpus, smoothed over a 3-year period.
	\newline
}

A related concept is \xlabel{material computation}~\cite{NeglectedPillar}, which goes one step further: its goal is to find ways to exploit the inherent computing power present in physical substrates.
Whereas classical computing uses a top-down approach where mathematics is imposed onto a physical material --- for instance to construct logic gates --- material computation works in the opposite direction by instead asking what computations a physical object performs naturally~\cite{RC_ASI}.
By designing materials with specific behaviours, systems can be created that naturally exhibit the desired computational properties, reducing the levels of abstraction that currently exist between physics and computation. \par
Among the various substrates being explored for material computation, magnetic and spintronic hardware are especially appealing~\cite{grollier2020neuromorphic,NeuromorphicSpintronicsProspect,QuantumNeuromorphicOpportunities}.
Single-domain ferromagnetic nanomagnets, for example, have the ability to store information without power, respond non-linearly to stimuli~\cite{NeuromorphicSpintronics} and can dissipate very little energy during information processing~\cite{ThermodynamicLimitsComputation,SpintronicsEnergyEfficientComputing}.
The \link{magnetostatic interaction}{magnetostatic coupling} within ensembles of nanomagnets provides further advantages, as it can give rise to rich collective behaviour like phase transitions, which the constituent magnets would not exhibit in isolation~\cite{NeuromorphicSpintronicsProspect,RC_ASI}.
A well-established example of such an ensemble is \xref{artificial spin ice} (ASI): an ordered lattice of bistable $\approx \SI{100}{\nano\metre}$-size nanomagnets where the lattice geometry does not allow all local interactions to be simultaneously satisfied, leading to emergent dynamics~\cite{ASIpyrochlores}.
The other aforementioned factors have made them a platform of particular interest for scalable and energy-efficient material computation~\cite{PhD_Stromberg}. \par
This finally brings us to the topic of this thesis: ``\emph{\phdtitle}''.
We will return to a more detailed explanation of artificial spin ice in~\cref{sec:1:ASI}; first,~\cref{sec:1:RC} introduces the general principles of \xref{reservoir computing}.

\newpage
\section{Reservoir computing}\label{sec:1:RC}
% Introduction to RC can take inspiration from: https://arxiv.org/html/2412.13212v1
At the intersection of neuromorphic computing and material computation lies the paradigm of \idx{reservoir computing}.
To explain the underlying concept, we must first take a step back and have a closer look at the aforementioned artificial neural networks.

\subsection{Origin}
\paragraph{Artificial neural networks}
The most ubiquitous machine learning models currently in use revolve around \idx{artificial neural networks} (ANNs).
These were originally inspired by the brain: an ANN consists of a network of non-linear summing nodes (akin to neurons) connected by directed weighted edges (akin to synapses)~\cite{EvaluatingRestrictedESNs,zurada1992introduction,AIsimulateMemoryContinuity}.
Each node holds a numerical value: during operation, this value is determined based on the weighted sum of all nodes that feed into it.
This allows activations to propagate through the network~\cite{lukovsevivcius2009reservoir}.
To prevent these values from accumulating without limit, they are typically bounded to the range $[0,1]$ by applying an \xlabel{activation function} to the weighted sum --- typically a sigmoid, like the logistic map $e^x/(1+e^x)$ or hyperbolic tangent~\cite{SurveyUniversalApproximation}.
A node that uses such an activation function is referred to as an \idx{artificial neuron}.
The non-linearity introduced by the activation function is what enables ANNs to approximate any arbitrary function, allowing them to adequately separate, group and otherwise process input data~\cite{ApproximationFNN,SurveyUniversalApproximation,funahashi1992neural}.
For many real-world tasks, the form of the desired function is unknown, so heuristics are used to ``train'' the weights of all edges based on a given dataset~\cite{EvaluatingRestrictedESNs}.
How exactly this training is done, depends on the structure of the ANN.
Typically, a major distinction is made between \xlabel{feed-forward neural network}s (FNNs) and \xlabel{recurrent neural network}s (RNNs)~\cite{ApproximationRNN}. \par
In a FNN, the nodes are grouped in layers, with connections between all the nodes of two successive layers.
This is schematically illustrated in~\crefSubFigRef{fig:1:NeuralNetworks}{a}: from the blue input node(s) on the left, information propagates to the right through the grey weighted connections, into the green hidden layers, finally yielding one or multiple output values at the red node(s)~\cite{zurada1992introduction}.
FNNs are mainly used for static data processing, such as image classification\footnote{Visual tasks are typically performed using convolutional neural networks --- a particular type of FNN that extracts features from images.}~\cite{ApproximationRNN}.
While~\crefSubFigRef{fig:1:NeuralNetworks}{a} shows a small network with 2 hidden layers containing 9 nodes in total, real-world tasks like image classification require far larger networks with millions or even billions of weights~\cite{ImageNet,BLOOM_CarbonFootprint_176Bparam,GPT-J-6B}. \par
RNNs are conceptually similar to FNNs, with the important distinction that their connection topology possesses cycles, either in a structured or unstructured manner~\cite{lukovsevivcius2009reservoir,Hopfield1982}.
The existence of cycles has a profound impact, as an RNN may develop self-sustained temporal activation dynamics along its recurrent connection pathways, even in the absence of input.
Hence, mathematically speaking, RNNs are dynamical systems while FNNs are functions~\cite{lukovsevivcius2009reservoir}. % REF verbatim
This enables an RNN to preserve a non-linear transformation of the input history in its internal state, providing dynamical memory and making RNNs especially well-suited for dynamic data processing~\cite{RC_RecentAdvances,ApproximationRNN}.
However, whereas FNNs can be trained using methods such as \xlabel{backpropagation}~\cite{Backpropagation} or gradient descent, the recurrent connections in RNNs make training very computationally expensive and difficult~\cite{DifficultyTrainingRNN,EvaluatingRestrictedESNs,Moon_2021,RC_SuperconductingElectronics,funahashi1992neural}.
Some approaches to train RNNs take inspiration from FNN: back-propagation through time (BPTT), for instance, unfolds the recurrent connections in time to obtain an FNN on which backpropagation can be applied~\cite{jaeger2002tutorial}.
However, this is not so straightforward, leading to issues regarding the calculation of the required gradients and a tendency to get stuck in local minima~\cite{RC_Tensegrity,DifficultyTrainingRNN,D-ESN-Improved}. % Other methods are real-time recurrent learning (RTRL) and extended Kalman filtering (EKE), both explained in jaeger2002tutorial.

\vspace{-1em}
\xfig{1_Introduction/NeuralNetworks.pdf}{
	\label{fig:1:NeuralNetworks}
	Schematic representation of \textbf{(a)} a feed-forward neural network (FNN), \textbf{(b)} a recurrent neural network (RNN) and \textbf{(c)} reservoir computing (RC).
	In the latter, the reservoir is treated as a black box; it can be of mathematical origin, like a RNN, or a physical system like artificial spin ice (ASI).
	Grey connections propagate information from left to right, unless indicated by an arrow.
	Blue (red) nodes represent input (output), while green nodes constitute the hidden layers of the network.
}
\vspace{-1em}

\paragraph{Echo state networks and liquid state machines}
To address the shortcomings of conventional RNN training methods like BPTT, two alternative concepts were independently proposed in the early 2000s, laying the foundation for what would eventually be known as reservoir computing.
Jaeger~\cite{jaeger2001echo} presented echo state networks (ESNs), while Maass~\etal~\cite{maass_LSM} presented liquid state machines (LSMs)\footnote{
	Their names reveal some fundamental desirable properties that the random recurrent network should provide.
	In LSMs, the sparse network of spiking neurons projects inputs into a high-dimensional transient state, akin to waves in a liquid.
	The ``echo'' of ESNs refers to the fading memory provided by the internal dynamics of a recurrent network, where loops can retain information from past inputs.
}.
Both are recurrent networks and operate on very similar principles: their main difference lies in their nodes, as LSMs use \xlabel{spiking neurons} while nodes in ESNs use sigmoidal activation functions.
The crucial property that sets them apart from other training methods is that their underlying recurrent network is randomly initialised and, crucially, remains unmodified during training~\cite{ReviewESNs,RC_Tensegrity}.
Instead, only a single linear readout layer is trained via linear regression to obtain the desired output from this random network~\cite{D-LSM,D-ESN-Improved}.
This decoupling of internal dynamics from learning is the core insight that underpins both LSMs and ESNs.
Training ESNs and LSMs is a walk in the park as compared to BPTT, as linear regression is far less computationally demanding and does not require the (sometimes problematic) calculation of gradients~\cite{D-ESN-Improved}. \par
From these principles, it is but a small step to reservoir computing (RC), which generalises these ideas by treating the internal network as a black box~\cite{RC_unification,D-ESN-Improved,RC_Tensegrity}.
In the next section we will explore the reservoir computing framework in more detail.

\subsection{Reservoir computing}
Reservoir computing is a machine learning framework suitable for temporal processing tasks~\cite{BookReservoirComputing}.
The concept revolves around a ``reservoir'' --- an unspecified non-linear dynamical system --- that projects the input data into a higher-dimensional space, thereby facilitating the separation of said data by a linear transformation~\cite{appeltant2011information,KUR-24,RC_ASI}. % TODO: use figure like Fig. 2 in ~\cite{appeltant2011information}
The principle is schematically illustrated in \crefSubFigRef{fig:1:NeuralNetworks}{c}.
A weighted sum of the components of the reservoir's state vector (green nodes) then provides the output (red node) of the system as a whole. % REFs for RC, specifically training a readout layer: Dale et al., 2017; Jensen and Tufte, 2017; Sillin et al., 2013
These weights can be trained with a simple learning algorithm like linear regression in order to produce a desired output~\cite{RC_RecentAdvances, RC_SuperconductingElectronics}.
Crucially, \textit{the properties of the reservoir itself are not modified during training} --- only the final linear readout layer is~\cite{RC_ASI,DynamicEmergence_NanomagneticSystem}. \par
Since the reservoir acts as a black box, its properties can not be changed to obtain the desired response\footnote{
	It is possible to adjust a physical reservoir to exhibit the desired properties for a given task~\cite{AdaptiveProgrammableRC,gartside2022reconfigurable}, but this is different from mathematically tuning the weights of the readout layer via a simple procedure like linear regression.
}.
To solve temporal tasks, it is beneficial for the reservoir to be a high-dimensional non-linear system with short-term memory~\cite{NeuromorphicAFMspintronics,RC_RecentAdvances}.
Reservoirs are not limited to being purely mathematical concepts, as many physical systems naturally possess these properties~\cite{RC_DipoleNanomagnets,RC_PassiveFrustratedNM,RC_ASI,RC_RecentAdvances,NeuromorphicOscillators,VowelRecognition4STO,RC_DiffusiveMemristors,RC_MemristorTemporal,gartside2022reconfigurable}, ranging from a bucket of water~\cite{PatternRecognition_Bucket} to photonic systems~\cite{RC_Photonic} and superconducting electronics~\cite{RC_SuperconductingElectronics}.
This is the concept of \idx{physical reservoir computing} (PRC), which allows the usage of physical systems without further mathematical abstractions --- apart from the linear readout layer at the end~\cite{RC_RecentAdvances}. \par
While this avoids the many layers of abstraction present in conventional machine learning methods, physical constraints can make it difficult to optimise a physical reservoir~\cite{RC_RecentAdvances}, in contrast to RNNs which have full mathematical freedom.
For instance, when using a physical reservoir to perform a temporal task, the physics of the system are most effectively harnessed when the timescale of system dynamics matches that of incoming data patterns~\cite{KUR-24}, but the timescales of physical phenomena are often hard to control.
Since physical reservoirs can often be hard to scale, multiple techniques have been devised to improve performance by combining multiple reservoirs together~\cite{EvaluatingRestrictedESNs,RotatingNeuronsRC} or by time multiplexing~\cite{appeltant2011information}.
Nonetheless, PRC combines many advantages of all the previously discussed concepts: not much data is required for training, which is computationally very cheap due to the single linear readout layer, enabling low energy usage due to the direct usage of a physical substrate for computation.
The combination of these factors has attracted significant interest to (P)RC.

\paragraph{Reservoir requirements}
For a physical substrate to be suitable as a reservoir, several requirements need to be satisfied: high dimensionality, non-linearity, fading memory, and a separation/approximation property~\cite{appeltant2011information}.
High dimensionality and non-linearity cooperate, as a non-linear mapping of inputs from a small-dimensional into a high-dimensional space can enable the separation of otherwise linearly inseparable inputs, making them both essential properties for classification tasks~\cite{VoltageControlled_SuperparamagneticRC,RC_ASI,appeltant2011information,RC_RecentAdvances}. % The dimensionality is related to the number of independent signals obtained from the reservoir~\cite{RC_RecentAdvances}.
In prediction tasks, the non-linearity of a reservoir can enable the extraction of non-linear dependencies from the inputs for better inference. \par
Fading memory~\cite{boyd1985ApproximatingVolterra} originates from the echo state property of ESNs, where recurrent connections preserve information of the past while it slowly fades out as new input values are applied.
This property ensures that the reservoir state is dependent on recent inputs, while being independent of inputs in the distant past, which is useful behaviour for most temporal processing tasks~\cite{ChaoticTimeSeries_ML,appeltant2011information}. \par % If inputs from the distant past are somehow needed, they can be incorporated using "long short-term memory" (Hochreiter and Schmidhuber).
Finally, the \xlabel{separation property} states that distinct signals should yield distinct responses, while the \xlabel{approximation property} requires similar inputs to remain grouped together in the output space, making the system is insensitive to noise~\cite{RCbenchmarksReview1}.
The former benefits from a chaotic regime, while the latter (and fading memory) are more easily fulfilled in a stable regime~\cite{RC_RecentAdvances}.
As such, many physical reservoirs aim to balance on this ``edge of stability'' whenever it is available in the system, to balance between these properties~\cite{appeltant2011information}.

\paragraph{Mathematical description}
% TODO: settle on a set of symbols that can be used, and add them in a (the?) figure
In the reservoir computing paradigm, an input signal $s(t)$ perturbs a reservoir $\mathcal{R}$, which can be any non-linear dynamical system, be it physical or abstract.
The state of the reservoir following this perturbation can then be represented by a $p$-dimensional response vector $\vc{r}(t) \in \mathbb{R}^p$.
The final output $\hat{y}(t)$ of the system is then obtained by a weighted sum $\hat{y}(t) = \vc{r}^\mathrm{T}(t) \cdot \vc{w}$ of the components of the response vector $\vc{r}(t)$.
Depending on the task performed, one or multiple such sums can be trained and interpreted in the context of that specific task~\cite{RC_RecentAdvances}. % For example, temporal pattern classification may provide multiple outputs, while prediction and generation only uses a single one
When the desired input-output relation $s(t) \mapsto y(t)$ is known for a limited set of data --- the training set --- the weights $\vc{w}$ can be adjusted to minimise the difference between the output $\hat{y}(t)$ and the desired output $y(t)$.
This is usually done by ordinary least-squares (OLS) regression, which requires only a single matrix inversion.
The optimal weights then remain fixed, and the performance of the reservoir can be evaluated by feeding it the test set --- a previously unknown, yet similar data set as the training set.

% TODO: should I explain the mathematics of the matrix inversion here? Can take inspiration from~\cite{RC_NNN} eq. 4--7.
% Training of the weights $\vc{w}$ via linear regression: \hat{y}_i = x_i^T \cdot w + \epsilon_i or in matrix form y = X w + \epsilon. The OLS algorithm adjusts the weight vector w to minimize the MSE between the estimated output and the target function.

\subsection{Metrics} \label{sec:1:RC_metrics}
To compare different physical reservoirs, or identify optimal system parameters, metrics are needed that can consistently evaluate how suitable a given physical system is to be used as a reservoir.
In this respect, two types of metrics are often used: those calculated based on the ranks of matrices, and those calculated through a form of linear regression, with the latter being more popular as of late.

\subsubsection{Matrix rank-based metrics}\indexlabel{matrix rank-based metrics} \label{sec:1:RC_metrics_KQ}
Legenstein and Maass~\cite{WhatMakesPowerful} proposed three metrics, based on the readout vector $\vc{r}(t)$ of the reservoir.
By sampling this $p$-dimensional vector at $m$ set intervals, a $p \times m$ matrix can be constructed whose columns are these vectors at different moments in time.
Depending on the choice of input sequence, the rank of this matrix gives information about certain properties of the system~\cite{RC_ASI}.

\paragraph{Kernel-quality}
The \xlabel{kernel-quality} $K$ measures the \xref{separation property}, i.e. how well the reservoir can separate distinct temporal input patterns.
Practically, it is determined by applying $m$ random input sequences, yielding a $p \times m$ matrix --- here, we will use $m=p$ to obtain a square matrix for a more consistent measure.
In the case of discrete-time input, each input sequence consists of a large number of input values (here: 100).
This ensures that the input sequences are maximally distinct, while the rank of this matrix (i.e., the kernel-quality $K$) is a measure for how distinct the responses obtained from the reservoir can be~\cite{Vidamour_2022}.
An more efficient alternative method would be to record the response $\vc{r}$ after each input value, as this would also result in maximally distinct input sequences, though this method strays further from the original definition~\cite{RC_HierarchicalNeuroevolution,RCbenchmarksReview1}.
Hence, a high kernel-quality $K$ indicates that the reservoir exhibits the separation property.
In the extreme case where $K=m$, any desired output can be implemented by the reservoir in combination with its linear readout layer~\cite{WhatMakesPowerful}.

\paragraph{Generalisation-capability}
The \xref{kernel-quality} is insufficient to quantify the computational performance of a reservoir~\cite{WhatMakesPowerful,RC_ASI,IL_Masterproef}.
The \xlabel{generalisation-capability} $G$ is a similar metric, which measures the \xref{approximation property}, i.e. whether similar input sequences yield similar responses.
Practically, it is determined in the same way as the kernel-quality $K$, except that the input sequences are now similar in one way or another.
This can be done by adding a small amount of noise to a given input sequence, but we will follow the method outlined in~\cite{RC_ASI} instead.
The first 60 input values will be shared among all sequences, while the remaining 40 are random for each sequence individually.
This way, the rank of the resulting matrix (i.e., the generalisation-capability $G$) measures how distinct the responses of similar inputs are.
A low value of $G$ is therefore desirable, as it indicates robustness to noise and overfitting, as the reservoir adheres to the approximation property~\cite{RCbenchmarksReview1}.

\paragraph{Compute quality}
To obtain a single metric for the computational capability of a reservoir, Legenstein and Maass proposed to simply subtract the normalised values of both these metrics.
This yields the compute quality $Q = K - G$, where higher values typically indicate better performance.
However, care should be taken with such claims, as high compute quality does not necessarily mean that the reservoir will perform well on all tasks it is given.

\subsubsection{Task-agnostic metrics}
While the matrix rank-based metrics are, strictly speaking, also task-agnostic, the metrics to be discussed now are more commonly referred to when talking about task-agnostic metrics, as the relation between them and particular tasks that the reservoir performs well on is clearer.
A practical definition of non-linearity and memory capacity was given by Love~\etal~\citet{RC_TaskAgnosticMetrics_v2}, which will be used here.

\paragraph{Non-linearity}
The non-linearity of the reservoir can be measured by considering that its response consists of a linear and a non-linear contribution.
The linear contribution can be extracted by training a linear estimator $\hat{\vc{r}}(t)$ on the reservoir output based on the past input $s(t - \tau)$.
More specifically, the estimator
\begin{equation}
	\hat{r}_n(t) = c_n + \sum_{\tau=0}^{k} w_{n,\tau} u(t - \tau) \mathrm{,}
\end{equation}
is trained by adjusting the weights $w$ and $c$ based on the known discrete-time input-output relationship (on the training set).
The cut-off time $k$ must be longer than the relaxation time of the reservoir --- we will use $k=10$.
By measuring the quality of this estimate using the $\mathrm{R}^2$ correlation coefficient, a measure of the linearity of each separate readout node is obtained.
This can be converted to an overall measure for non-linearity as
\begin{equation}
	\mathrm{NL} = 1 - \frac{\sum_{n=1}^p \mathrm{R}^2[\hat{r}_n;r_n]}{p} = 1 - \frac{\sum_{n=1}^p \frac{\mathrm{cov}^2(\hat{r}_n; r_n)}{\sigma^2(\hat{r}_n) \sigma^2(r_n)}}{p} \mathrm{.}
\end{equation}

\paragraph{Linear memory capacity}
Calculating the linear memory capacity can also be done using linear estimators, but by working the other way around.
Instead, an estimator for the past input $s(t - \tau)$ is trained based on the current state $\vc{r}$ of the reservoir.
In other words, the estimator
\begin{equation}
	\hat{s}(t - \tau) = c_n + \sum_{i=1}^{p} w_{i,\tau} r_i(t)
\end{equation}
is now trained.
The linear memory capacity is then obtained by summing the $\mathrm{R}^2$ correlation coefficient of all estimators up to a threshold $k$:
\begin{equation}
	\label{eq:1:MC}
	\mathrm{MC} = \sum_{\tau = 1}^{k} \mathrm{R}^2[\hat{s}(t - \tau); s(t - \tau)]
\end{equation}
Note that the memory capacity can be increased arbitrarily high, by choosing a high cut-off $k$.
However, the memory capacity typically exhibits a plateau beyond a certain value of $k$, before rising dramatically only when $k$ approaches the size of the training set.
Hence, $k$ should be chosen at this plateau; for the systems we will consider, $k=10$ suffices. \par
Note that this measures short-term memory; long-term memory often refers to the knowledge engrained in the weights of an ANN --- the linear readout layer in the case of RC.
Furthermore, this only captures the linear memory capacity: the non-linear memory capacity is far more computationally expensive as it additionally requires a summation over polynomials of different degrees~\cite{RCbenchmarksReview1}.
We therefore limit this discussion to the linear memory capacity.

\paragraph{Parity check}
Closely related to the memory capacity is the parity check metric~\cite{hon2021numerical}.
This is particularly intended for use with binary input, as it defines the parity of a binary input sequence $s(t)$ as
\begin{equation}
	\pi(t-\tau) = \Bigg[\sum_{j = 0}^{\tau} s(t - j) \Bigg] \mathrm{mod}~2 \mathrm{.}
\end{equation}
The parity check is then defined in the same way as the memory capacity in~\cref{eq:1:MC}, except with $\pi(t - \tau)$ instead of $s(t - \tau)$.
This introduces a form of non-linearity into it, though it mostly serves as a measure of the memory capacity in binary systems. % TODO: is this correct?


% TODO: cite relevant papers, like \cite{NeuromorphicFewShot}: in the Methods section there is a good mathematical introduction on MG, NARMA, and MC/NL, so can use this as reference that MG is mostly memory-driven.

\subsubsection{Other} % Memorisation, frequency generation, classification
In addition to these six metrics, reservoirs are often used to perform specific benchmark tasks in literature.
Example of these include % TODO: signal transformation and prediction (Mackey-Glass, sunspots), NARMA, simple image recognition~\cite{farronato2022reservoir,grollier2020neuromorphic}... See https://arxiv.org/html/2405.06561v1
Wringer~\etal~\cite{RCbenchmarksReview1} note that, while less general than metrics, benchmarks enable a more concrete comparison of reservoirs, as opposed to the more vague notion that two reservoirs are simply different based on their metrics.
Wringer~\etal then proceed to provide a comprehensive overview of various benchmarks used throughout literature, with motivations for why they are used for particular reservoirs~\cite{RCbenchmarksReview1}.

\newpage
\section{Nanomagnetism}
\subsection{Physics of nanomagnets}
This thesis focuses on the usage of magnetic systems as reservoirs.
Generally, magnetism and magnetic fields originate from the movement or acceleration of electric charges.
An object is said to have a \idx{magnetic moment} $\vc{\mu}$ if it experiences a torque $\vc{\tau} = \vc{\mu} \times \vc{B}$ when placed in a magnetic field $\vc{B}$~\cite{IntroMagneticMaterials}.
Conversely, a magnetic moment also generates its own magnetic field
\begin{equation}
	\vc{B}(\vc{r}) = \frac{\mu_0}{4 \pi} \ab[\frac{3 \vc{r} (\vc{\mu} \cdot \vc{r})}{r^5} - \frac{\vc{\mu}}{r^3}] \mathrm{,}
\end{equation}
giving rise to the \xref{magnetostatic interaction} through which magnetic moments influence each other. % (\cref{eq:2:E_MS})
Freely rotatable magnetic moments will therefore prefer a parallel alignment along their connecting axis.

\paragraph{Ferromagnetism}
Some materials, like a bar magnet, exhibit a spontaneous \idx{magnetisation} --- they generate a magnetic field seemingly without any electrical current.
This magnetisation arises at the atomic level, from two distinct sources: the angular momentum of electrons around the nucleus, and the intrinsic spin of the elementary particles comprising an atom~\cite{coey2010magnetism}.
The latter is often dominant, and while the magnetic moments of pairs of electrons often cancel out, atoms with unpaired electrons can exhibit a net magnetic moment~\cite{PhD_Leliaert}. % See slide 35 of https://magnetism.eu/esm/2019/slides/simonet-slides1(magnetism_of_atoms).pdf
However, this does not yet mean that a material composed of such atoms will exhibit a macroscopic magnetic moment like a bar magnet, as the moments of nearby atoms are not necessarily aligned. \par
A non-vanishing net magnetic moment is the privilege of \textit{ferromagnetic}\indexlabel{ferromagnetism} materials.
It is only in these materials that the \idx{Heisenberg-Dirac exchange interaction}~\cite{heisenberg1928theorie} provides a strong tendency for nearby atomic magnetic moments to align, leading to a non-zero macroscopic magnetisation within the material.
This alignment is only possible below a material's \idx{Curie temperature}: at room temperature, \ce{Fe}, \ce{Ni} and \ce{Co} are the only pure elements to be ferromagnetic, though many compounds like \ce{Nd2Fe14B} and \ce{SmCo5} are as well. \par
However, the keyword regarding the exchange interaction is ``nearby'': over longer distances, the magnetisation direction can vary throughout the material as other interactions are also at play --- most importantly, the magnetostatic interaction.
Hence, the magnetisation can be described as a vector field $\vc{M}(\vc{r})$ with a resolution of $\lesssim \SI{5}{\nano\metre}$, as the exchange interaction dominates over shorter length scales.
This forms the basis of micromagnetic theory~\cite{mumax3}: over such small distances, the local magnetisation always has approximately the same magnitude; $|\vc{M}(\vc{r})| \approx M_\mathrm{sat}$, the \idx{saturation magnetisation}.
Applying a magnetic field makes the magnetisation tilt towards the field, but does not increase $M_\mathrm{sat}$: it is an intrinsic property that determines the strongest magnetic moment per volume that a given material can provide.

\paragraph{Single-domain nanomagnets}
Integrating $\vc{M}(\vc{r})$ over the volume of a material yields its net magnetic moment $\vc{\mu}$, but this is almost never equal to $M_\mathrm{sat}V$ since the magnetisation can vary significantly over longer distances.
In fact, an object made of magnetic material often does not exhibit a spontaneous total magnetic moment at all, as the magnetostatic interaction typically causes the magnetisation to loop back on itself to minimise stray fields.
Indeed, in a large piece of ferromagnetic material, it is energetically favourable for \SI{}{\micro\metre}-sized \idx{ferromagnetic domains} to form, whose magnetic moments all cancel each other out when no external field is present~\cite{coey2010magnetism}.
Applying an external field then causes some domains to grow and others to shrink, rather than smoothly canting the magnetisation towards the field.
However, in sufficiently small ferromagnetic particles ($\lesssim \SIrange{100}{500}{\nano\metre}$), the exchange interaction is too strong for separate domains to form~\cite{Kittel_TheoryFMDomains}.
This results in a \idx{single-domain nanomagnet} with a nearly uniform magnetisation $\vc{M}(\vc{r}) \approx \vc{\mu}/V$, whose magnitude is roughly equal to the saturation magnetisation $M_\mathrm{sat}$~\cite{FRENKEL1930}. \par % Frenkel and Dorfman first conceived the idea of a single-domain particle based on energy considerations.
When such a nanomagnet exhibits some form of anisotropy, a preferential magnetisation axis will emerge, referred to as the \idx[nolabel]{easy axis}.
This has led to nanomagnets being used for computation, as the two magnetisation directions along the easy axis can be related to bits `0' and `1'~\cite{MQCA_RoomTemp,NML_Carlton,JM_Masterproef}.
The magnetisation can switch between these two stable states under the influence of nearby nanomagnets, external magnetic fields, thermal fluctuations, electrical currents and a variety of other phenomena~\cite{SwitchingForced_EnergyEfficient,BrownThermalFluctuations,neel1949theorie}.
One common origin of anisotropy is the geometry of a nanomagnet: this \idx[nolabel]{shape anisotropy} is caused by the tendency of magnetic moments to align in the same direction, resulting in a low-energy state when the magnetisation aligns with the longest axis of the geometry~\cite{MagneticCharge}.
Anisotropy can also originate from the crystal structure, but this can be suppressed by an appropriate choice of material; for example, permalloy (\ce{Fe20Ni80}) is an alloy specifically designed to exhibit minimal \xlabel{magnetocrystalline anisotropy}.

\paragraph{In-plane and out-of-plane nanomagnets}
Two types of nanomagnets are typically distinguished: in-plane (IP) and out-of-plane (OOP) nanomagnets.
IP nanomagnets can readily be obtained by tailoring the geometry of the ferromagnetic particle: a flat geometry guarantees an in-plane magnetisation direction through the \xref{shape anisotropy}.
This easy plane can be reduced to an \xref{easy axis} by using an elongated two-dimensional shape, like an ellipse or rectangle.
OOP magnets are harder to manufacture, as it is often infeasible to create tall structures on wafers.
Therefore, where IP magnets mostly rely on shape anisotropy to create an easy axis, OOP magnets must use other types of anisotropy to favour a magnetisation direction perpendicular to the substrate.

\subsection{Input and readout for reservoir computing}\label{sec:1:ASI_IO}
To use nanomagnetic systems for RC, methods are required to inject information and read out their resulting magnetic states.
One of the main factors that makes magnetic systems attractive candidates for RC is their ease of electrical interfacing, as many effects can manifest between currents and magnetic materials, enabling efficient input and output.
Electrical control is strongly preferred over external magnetic fields, as it provides far more precise control over the magnetisation.
In particular, for OOP systems --- the main focus of this thesis --- \xref{spin--orbit torque} provides an efficient input method that synergises well with readout via the \xref{anomalous Hall effect}.

\subsubsection{Input}
\paragraph{Spin-transfer torque}
A well-established method for switching nanomagnets is \idx{spin-transfer torque} (STT)~\cite{SlonczewskiSTT}, as has been widely used in e.g., magnetic random-access memory (STT-MRAM).
When an electrical current with a spin polarisation $\vc{\sigma}$ enters a ferromagnetic material, the injected spins will exert a torque $\vc{\tau}_\mathrm{STT} \propto \vc{\mu} \times (\vc{\mu} \times \vc{\sigma})$ on the local magnetisation $\vc{\mu}$, causing it to align with the injected spin polarisation.
This polarisation is typically generated by first passing the current through another ferromagnetic layer, whose magnetisation is known or controlled by other means~\cite{SOT_FM_AFM,mumax3tutorial}.
Hence, if both layers are sufficiently close by for the spin polarisation to survive until the second layer, a sufficiently strong STT will cause the magnetisation of the second layer to align with that of the first layer the current passes through.
In OOP nanomagnets, both layers are therefore typically stacked on top of each other, with the current passing vertically through both of them.
While this is an effective method, such a multilayer structure increases fabrication complexity and requires the magnet stack to be fully conductive, which can be limiting in some situations.

\paragraph{Spin--orbit torque}
More recently, switching through \idx{spin--orbit torque} (SOT) has attracted attention as an alternative to STT, as it eliminates the need for a second magnetic layer or vertically flowing current~\cite{mumax3tutorial}.
Instead, it only requires a heavy-metal layer with strong spin--orbit coupling (e.g., \ce{Pt}, \ce{Ta}, \ce{W} \dots) to be adjacent to the ferromagnet~\cite{SOT_Roadmap}.
Typically, a heterostructure is used where a ferromagnet is patterned on top of a heavy-metal underlayer --- for instance, the OOP magnets considered in~\cref{ch:Applications} are \ce{Co}-\ce{Pt} multilayers~\cite{KUR-24}. % exactly the type of structure that supports SOT.
When an in-plane electrical current passes through the underlayer, a spin polarisation will accumulate at the interface with the ferromagnet as a result of the bulk spin Hall effect~\cite{SHE} and/or interfacial Rashba-Edelstein effect~\cite{SOT_Rashba}. % RE effect due to symmetry being broken at an interface. The bulk spin Hall effect often provides the dominant damping-like torque, while any interfacial Rashba effect contributes mainly to the field-like term. % SOT does not originate from a single underlying phenomenon, but is an umbrella term for various effects that all relate to spin--orbit coupling and end up generating a torque in a magnetic material.
These spins, which are polarised in-plane and perpendicular to the electrical current, can then diffuse into the ferromagnet and exert a torque on its magnetisation~\cite{mumax3tutorial}.
Note that this is indeed distinctly different from STT, where the spin polarisation originates from another magnet and flows straight through the structure~\cite{SOT_Roadmap}. \par
This torques is often decomposed into two orthogonal components: a damping-like and a field-like torque.
The field-like torque $\vc{\tau}_\mathrm{FL} \propto \vc{\mu} \times \vc{\sigma}$ causes a precession of the magnetic moment $\vc{\mu}$ around the spin polarisation $\vc{\sigma}$.
On the other hand, the damping-like component $\vc{\tau}_\mathrm{DL} \propto \vc{\mu} \times (\vc{\mu} \times \vc{\sigma})$ aligns the magnetic moment with the spin polarisation~\cite{SOT_Roadmap,SOT_FM_AFM}.
The latter is functionally similar to STT, earning it the name Slonczewski-like SOT.
It has been shown that the field-like SOT can act as a strong transverse effective field, upwards of $\SI{1}{\tesla}$ per $\SI{e8}{\ampere\per\centi\metre\squared}$~\cite{SOT_Rashba}, making it a very efficient method to switch OOP nanomagnets~\cite{vlasov2022optimal,SOTswitchingCoPt,SHE_CurrentInducedSwitching,SpintronicsEnergyEfficientComputing}. \par
However, SOT alone does not guarantee deterministic switching of an OOP ferromagnetic layer, as the in-plane spin polarisation does not differentiate between the `up' and `down' magnetic states.
To resolve this, additional symmetry breaking is required, more specifically with respect to the plane formed by the electrical current and the interface normal~\cite{SOT_Roadmap}.
This can be achieved in a plethora of ways e.g., by an in-plane magnetic field, an exchange bias layer, a source of OOP spin polarisation, exploitation of crystal symmetries if possible, or geometrical asymmetry in the system~\cite{SOT_firstprinciplesCoPt,SOT_Roadmap}.
The polarity of switching depends on the direction of the applied current, the material, and the symmetry breaking.
Hence, when it is known which current direction corresponds to a preferential `up' or `down' magnetisation, electrical control of the magnetisation can be achieved~\cite{SOT_PMAinsulator}.

\subsubsection{Output}
\paragraph{Anomalous Hall effect}
Our method of choice to electrically obtain output from ensembles of out-of-plane nanomagnets is to make use of the \idx{anomalous Hall effect} (AHE).
In the ordinary Hall effect, an electrical current flowing through a conductor (the Hall bar) generates a transverse voltage, since the current generates a magnetic field, which exerts a Lorentz force on the electrons.
When the conductor is a ferromagnet, an additional contribution appears that is proportional to the magnetisation of the material.
While its exact origin is still a matter of debate, it is thought to originate from spin--orbit coupling: spin-polarised electrons passing through the ferromagnet get scattered, leading to a charge imbalance across the width of the conductor~\cite{AHE_Culcer,AHE}. \par
Rather than express it as a voltage, the Hall effect is more often described by the transverse resistivity (the ratio between transverse electric field and longitudinal current density) $\rho_{xy} = E_x/J_y = R_0 H_z + R_\mathrm{AHE} M_z$, with the Hall bar in the $xy$-plane~\cite{SHE,AHE}.
The first term captures the ordinary Hall effect, the latter the AHE with $M_z$ the OOP magnetisation component and $R_\mathrm{AHE}$ the anomalous Hall coefficient~\cite{AHE}.
Hence, the AHE is greatest when the magnetisation is perpendicular to the conductor plane and has opposite sign for opposite magnetisation directions --- good news for the OOP nanomagnets that we focus on.
This has enabled the usage of AHE for readout in many SOT-MRAM devices, as it can directly be applied in the same heavy-metal/ferromagnet heterostructure as SOT. \par
The readout resolution depends on various factors like $R_\mathrm{AHE}$ and the precise geometry of the Hall bar, with single-magnet resolution achievable. % Typically ~100nm
Since the AHE synergises well with the geometry of OOP nanomagnetic ensembles, it is the method of choice to obtain readout from such reservoirs.

\paragraph{Anisotropic magnetoresistance}
Another phenomenon that can provide readout is \idx{anisotropic magnetoresistance} (AMR): when an electrical current flows through a ferromagnetic material, the resistance weakly depends on the angle $\theta$ between this current and the magnetisation~\cite{AMR}.
More specifically, the total resistance $R(\theta) = R_\perp + (R_\parallel - R_\perp) \cos(2\theta)$.
This makes it possible to distinguish between perpendicular and parallel magnetisation states.
While this is a very accessible technique that only requires a measurement of the resistance, it has several limitations. \par
First off, while permalloy exhibits a relatively strong AMR, the absolute effect remains rather small in comparison to the total resistance --- at most a few percent.
This can make it hard to accurately determine the magnetisation state without the use of lock-in amplifiers~\cite{ArchitecturesNanoringRC,Vidamour2023}.
Furthermore, due to the $\cos(2 \theta)$-dependence, AMR does not reveal information about the magnetisation direction, only about the local angle between the magnetisation and current.
It also requires the system to be electrically connected: this makes it an appropriate choice for e.g. the ring system in the \spinengine project, but prevents it from directly being applied to ensembles of nanomagnets.
In interconnected ferromagnets, it only provides a limited amount of information for RC as a lot of the system's dynamics are obscured.
Ensembles of OOP nanomagnets, in particular, would need the current to flow vertically through each magnet, further complicating the system.

\paragraph{Ferromagnetic resonance}
Another possible method to obtain rich output from a reservoir is \idx{ferromagnetic resonance} (FMR)~\cite{AdaptiveProgrammableRC,gartside2022reconfigurable}.
This involves applying a microwave field and measuring the absorption as a function of frequency.
When the frequency matches a resonance of the internal magnetisation dynamics of a nanomagnet, an absorption peak appears.
The complexity of internal magnetisation dynamics, represented in the frequency domain, can provide a rich and non-linear reservoir output~\cite{AdaptiveProgrammableRC,Gomez-Iriarte_FMR}.
However, the peripheral equipment required to perform the technique renders it energy-inefficient and unsuitable for on-chip applications.
Furthermore, as the wavelengths involved are on the order of centimetres, it does not provide local information --- though for RC purposes it does not need to since the output space is rich regardless.

\subsubsection{Imaging}
Aside from electrical measurements that provide information about the state of (an ensemble of) magnets, it is also possible to directly image the magnetic configuration of a system through microscopy techniques.
While these can also be seen as ``output'', they are distinct from the aforementioned electrical measurements since they can not be integrated on-chip.

\paragraph{Magnetic force microscopy}
A relatively commonly technique is \idx{magnetic force microscopy} (MFM)~\cite{MFM}, which scans a cantilever with a very sharp tip across the sample, similar to atomic force microscopy (AFM).
Whereas AFM uses a non-magnetic tip to measure the sample's texture at an atomic scale, MFM uses a tip coated in ferromagnetic material (e.g., \ce{Co}), giving it a vertical magnetic moment.
This enables a measurement of the out-of-plane\footnote{
	It is also possible to measure the in-plane magnetisation by using an in-plane magnetised tip, but this complicates interpretation as this mixes two components of the magnetisation~\cite{MFM_inplane}.
} magnetic field by scanning the tip at a very low height ($\approx \SI{20}{\nano\metre}$) above the sample~\cite{NML_Carlton,JM_Masterproef}.
However, this does not only measure the magnetic field, but also the atomic forces that a standard AFM tip experiences.
Therefore, an AFM measurement is first performed to obtain the texture profile of the sample, such that a subsequent MFM measurement can keep the magnetic tip at a constant height above the sample, thereby isolating the magnetic field strength from the atomic forces. \par
The result is a greyscale image, where the brightness indicates the vertical component $B_z$ of the magnetic field close to the sample.
For example, an IP nanomagnet will appear as a pair of regions with bright and dark contrast, because the divergence (convergence) of the magnetic field at its north (south) pole results in opposite vertical magnetic fields on either end~\cite{NML_Carlton}.
An MFM image of OOP magnets (\cref{sec:3:MFM}) is easier to interpret, as there is a one-to-one correspondence between $B_z$ and the the magnetisation state. \par
MFM is relatively cheap to set up as compared to other magnetic microscopy techniques.
It can achieve high resolution, typically on the order of $\lesssim \SI{100}{\nano\metre}$, depending on the height and size of the tip~\cite{MFM}.
However, it is very slow; scanning across a $\SI{}{\micro\metre}$-sized sample can take multiple minutes.
Care must also be taken that the sample is not affected by the magnetic moment of the tip, though this influence is often negligible as the tip is made very small~\cite{Probing_MagnetoOptics}.

\paragraph{Other magnetic microscopy techniques}
While MFM is the most commonly available magnetic microscopy technique, X-ray and optical methods are also often used~\cite{DynamicEmergence_NanomagneticSystem}.
These can provide different information about nanomagnetic systems, as they are directly sensitive to the magnetisation rather than stray fields, making them easier to interpret in systems with in-plane magnetisation.
X-ray based measurements like photo-electron emission microscopy (PEEM)~\cite{PEEM} and magnetic transmission X-ray microscopy (MTXM)~\cite{Imaging_MTXM} can make use of the X-ray magnetic circular dichroism (XMCD) effect and achieve an even higher resolution than MFM.
However, they require circularly polarised X-rays, whose availability is typically limited to synchrotron radiation facilities.
Another option is to use optical techniques like the magneto-optical Kerr effect (MOKE)~\cite{KerrFaraday_book}: while these do not require a synchrotron, their resolution is diffraction-limited to several hundred \SI{}{\nano\metre}.
% See also: Polarized neutron reflectometry \cite{DynamicEmergence_NanomagneticSystem}.

\subsection{Magnetic platforms for reservoir computing}\label{sec:1_RC_magnetic} % Rings and ASI
% TODO: consider adding references 308-314 of Pieter's PhD thesis, which concern RC using nanomagnets
\subsubsection{\spinengine}
In the context of the \spinengine project, three types of magnetic systems were considered.
These are the magnetic nanorings~\cite{DynamicEmergence_NanomagneticSystem}, focused on by the University of Sheffield, in-plane (IP) artificial spin ice (ASI)~\cite{RC_ASI}, researched by NTNU, and out-of-plane (OOP) ASI~\cite{KUR-24}, the primary interest of ETHZ.
Meanwhile, Ghent University provided simulation support for these magnetic systems.
All three present their own advantages and challenges. \par
The nanoring ensembles show promising RC performance, but did not provide straightforward on-chip input and readout methods as would be desirable for applications.
Using anisotropic magnetoresistance (AMR) only provides a single readout value --- thereby obscuring a lot of the system's dynamics --- and requires the use of lock-in amplifiers.
Readout using ferromagnetic resonance (FMR) can provide a higher-dimensional readout in the form of spin-wave spectra~\cite{swindells2024fingerprinting}, but requires bulky waveguides. \par
The potential for RC in IP ASI has been demonstrated numerically~\cite{RC_ASI}, but the experimental application of input and state readout is not straightforward.
An external field is often used for input, though this is undesirable for on-chip applications; instead, it appears possible to use SOT~\cite{SOT_switching_IP}.
Efficient read-out of IP ASI remains challenging, due to the symmetry of the system coupled with the discontinuous nature of the ASI. \par % TODO END: cite roadmap and include more info
Finally, for the OOP ASI efficient input (SOT) and read-out (AMR) mechanisms had been demonstrated experimentally, but their potential for RC had not.
Therefore, this thesis will fill this gap in knowledge and assess the viability of RC with OOP ASI by making use of numerical simulations.

\newpage
\section{Artificial spin ice}\label{sec:1:ASI}\indexlabel{artificial spin ice} % TODO END: decide how exactly to use these labels, which words to label like this. Perhaps a good rule of thumb is to label the list of abbreviations, for easy reference to their first occurrence in the text.
Artificial spin ice (ASI) is a class of ferromagnetic metamaterial which consists of an ordered lattice of interacting (bistable) nanomagnets, where it is impossible to satisfy all local interactions~\cite{RC_ASI}. % REF verbatim
The magnetisation direction of the constituent nanomagnets can switch between two states under the influence of nearby magnets or thermal fluctuations.



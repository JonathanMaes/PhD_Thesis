\chapter{Methods: ``\hotspice'' simulator for ASI}\label{ch:Hotspice}
% \glijbaantje{It's not a bug, it's a feature.}{Someone}

\begin{adjustwidth}{2em}{2em} % TODO END: update once published.
    \begin{center}
        \textbf{Material from this chapter has also been published in:} \\
    \end{center}
    \vspace{1em}
    \begin{adjustwidth}{0em}{1.5em}
	    \begin{itemize}
	    	\item[\cite{MAES-24}] J.~Maes, D.~De~Gusem, I.~Lateur, J.~Leliaert, A.~Kurenkov, and B.~Van Waeyenberge.
	    	\newblock The design, verification, and applications of Hotspice: a Monte Carlo simulator for artificial spin ice.
	    	\newblock \emph{ArXiV}, arXiv:\penalty0 2409.05580, 2024.
	    \end{itemize}
    \end{adjustwidth}
    \vspace{0.5em}
    \begin{center}
        \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{.5em}
    \begin{center}
    	The \hotspice simulator discussed in this chapter\\
    	is open-source and available on \href{https://github.com/bvwaeyen/Hotspice}{GitHub}. \\
    \end{center}
    %\vspace{0.5em}
    \begin{center}
    \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{1em}
\end{adjustwidth}

To assess the potential of \xref{Reservoir Computing} in (perpendicular-anisotropy) \xref{Artificial Spin Ice}, as is the main topic of this thesis, a simulation framework is needed that allows efficient exploration of the impact that various system parameters and input methods have on the reservoir performance. \\\par

Nanomagnetic systems are often simulated using micromagnetic codes, such as the finite-difference-based \mumax~\cite{mumax3} and \oommf~\cite{OOMMF} or the finite-element-based \nmag~\cite{Nmag}, which capture the magnetization dynamics of individual nanomagnets in great detail. \par
However, the time between successive switches of a nanomagnet is not necessarily similar to the timescale of micromagnetics.
Furthermore, determining RC metrics requires applying many input cycles --- on the order of 100 for the task-agnostic metrics --- to get a statistically valid result.
In this timeframe, many switches occur.
When the simulated time extends beyond several microseconds, as is typically the case, simulating even a modest number of magnets --- on the order of several dozen --- becomes computationally unfeasible~\cite{leo2021chiral}. \\\par

To address these limitations, specialized ASI simulation tools have been developed.
An example of this is the flatspin simulator~\cite{flatspin}, which implements deterministic spin flipping via a Stoner-Wohlfarth model~\cite{StonerWohlfarth2008}.
Using such higher-level approximations enables the study of collective behaviour in much larger systems and over far longer timescales than is feasible with micromagnetic codes, though at the cost of no longer simulating the internal magnetization structure of individual nanomagnets in detail.
Additionally, Monte Carlo methods are often used to simulate spin ices, including ASI, but these are typically specialized to a select few lattice geometries and often only account for nearest-neighbour interactions, whose strength is often arbitrarily set or calculated separately using micromagnetic codes.~\cite{MeltingASI,sklenar2019field,gilbert2014emergent,zhang2013crystallites} \\\par % REFS: 'gilbert2014emergent': MC sims based on a vertex model and interacting magnetic charges. 'zhang2013crystallites': uses monopoles and NN couplings to model kagome ASI with Metropolis and loop update. 'moller2006artificial': bit broader using dipolar interaction, though seemingly still only for nearest neighbors. 'mengotti2011kagome' use a full dipole model with Ewald summation for PBC. 'lou2023competing': dipole model for half-occupation IP Ising. 'sendetskyi2019continuous': dipole model for square ASI. 'EngineeringRelaxationComputation': KMC on dipole model, seemingly for square arrays. 'sklenar2019field': NN interactions calculated by mumax on quadrupole lattice. 'MeltingASI': 16-vertex ice model

Our goal was to blend these two approaches, resulting in \hotspice: a versatile Monte Carlo simulator meant to capture ASI physics with minimal arbitrary parameters, allowing various lattice configurations to be evaluated. \par
This software approximates each single-domain nanomagnet as a single Ising spin, associating energies with the various ASI states by accounting for the magnetostatic interaction between all magnets.
\hotspice supports both in-plane (IP) and out-of-plane (OOP) ASI, which may contain thousands of magnets. Simulations can span arbitrary timescales, as determined by the switching time of magnets in the system. \\\par

In this chapter, we discuss several model variants that have been implemented, and assess their accuracy in simulating the behaviour of ASI.
These variants differ in their choice of Monte Carlo spin-flip algorithm and their calculation of the magnetostatic interactions and energy barriers. \\\par

\section{Model}
In single-domain IP nanomagnets, the magnetisation prefers to align along the fixed easy axis of the geometry, while for OOP magnets a strong interfacial anisotropy causes a preferential orientation along the $z$-axis.
Either way, it is natural to use an Ising-like approximation for simulating such single-domain nanomagnets.
The position $\vc{r}_i$, axis $\vc{u}_i$ and size of the magnetic moment\footnote{
	The size of the magnetic moment $\mu_i$ corresponds to the total ground state magnetic moment $\abs{\int_{\Omega_i} \vc{M}(\vc{r})d\vc{r}}$, with $\Omega_i$ the shape of magnet $i$ and $\vc{M}(\vc{r})$ its magnetisation in the twofold degenerate ground state. Due to edge relaxation effects, this value is slightly smaller than $M_\mathrm{sat} V_i$.
} $\mu_i$ of each magnet $i$ are fixed and they are only allowed to switch between the `up' ($\uparrow$) and `down' ($\downarrow$) magnetisation states.
Thus, the total magnetic moment vector of magnet $i$ can be expressed as
\begin{equation}
	\vc{\mu}_i = s_i \mu_i \vc{u}_i \mathrm{,}
\end{equation}
where $s_i = \pm 1$ and $\abs{\vc{u}_i} = 1$. \par
The switching rate between these two states is determined by the temperature $T$ and the effective energy barrier $\EBtilde$ separating them.
%A canonical ensemble is used where magnets are considered in contact with a heat bath of temperature $T_i$. % TODO: mention this or not?
For an isolated nanomagnet, the energy barrier $\EB = K_\mathrm{u} V$\footnote{
	This is valid for switching by coherent rotation. Similar to the calculation of $\mu$, edge relaxation effects may cause the effective volume to be slightly smaller.
} originates from its uniaxial shape anisotropy $K_\mathrm{u}$.
Interactions with other magnets or external fields modify the energy landscape, leading to an effective barrier which we will denote with a tilde $\EBtilde$~\cite{leo2021chiral}.
Each magnet can have a unique magnetic moment size $\mu_i$, temperature $T_i$ and energy barrier $E_{\mathrm{B},i}$.
This enables, for instance, modelling some of the disorder due to lithographic variations by assigning a different shape anisotropy to each magnet, typically sampled from a Gaussian distribution with mean $\EB$ and standard deviation $\sigma(\EB)$. \\\par

% We are most interested in simulating ASI, which are often well-ordered and spatially periodic. Therefore, \hotspice chooses to...
Due to the periodic nature of many ASI lattices, \hotspice chooses to perform the simulation on a rectilinear grid, the benefits and details of which will be discussed in~\cref{sec:2:Implementation}.
Each grid point may or may not contain a magnet, and the magnets must either all be of the IP type, or all OOP.
Even though this implementation does not allow complete freedom in the placement of magnets, many popular ASI lattices can be constructed in this manner. \par

\xfig[1.0]{2_Hotspice/ASIs.pdf}{
	\label{fig:2:ASIs}Predefined artificial spin ice (ASI) lattices available in \hotspice. The unit cell of each lattice is delineated by a central dark grey rectangle. The red indicator defines the lattice parameter $a$. In the Ising approximation, the magnetization of in-plane magnets (top) aligns along the major axis of the depicted ellipses. Out-of-plane magnets (bottom) are illustrated as circles.
}

\cref{fig:2:ASIs} showcases the 12 lattices that \hotspice provides out-of-the-box. The pinwheel lattices (a) and (b) are equal to the square lattices (c) and (d), respectively, but with each magnet individually rotated by \ang{45}. Both lattices have two variants (diamond/lucky-knot and closed/open), related by a global \ang{45} rotation of the entire lattice. This gives rise to different boundaries due to the Cartesian character of the underlying grid, which may alter the dynamics of the ASI. Furthermore, the unit cell for lucky-knot pinwheel and open square is more compact, resulting in faster simulation, while diamond pinwheel and closed square are more popular in literature and were therefore implemented separately. \par % TODO: more popular in literature? Examples where these are used?
The remaining four IP and four OOP lattices are also related: the magnets in the OOP lattices (i)-(l) are positioned at the vertices where magnets meet in their respective IP counterparts (e)-(h).
The Cairo lattice (h) can be continuously deformed into the Shakti lattice~\cite{ShaktiCairo}, but note that the point dipole model is no longer appropriate for the latter; instead, a dumbbell model (see~\cref{sec:2:Dumbbell}) would be more accurate.

\section{Energy calculation}
The energy of the system is the driving force behind these Monte Carlo simulations.
In this section, we will list the various energy contributions used in \hotspice, explain how we may account for the finite size of real nanomagnets in this Ising-like model, and show how the effective energy barrier $\EBtilde$ is calculated.

\subsection{Energy contributions}
Three energy contributions have been implemented in \hotspice, supporting both open and periodic boundary conditions (PBC).\footnote{
	Users can implement more energy contributions by inheriting from the \python{hotspice.Energy} class and implementing the \python{abstractmethod}s, taking care to correctly account for open or periodic BC when necessary.
}
\begin{enumerate}
	\item The \textit{magnetostatic interaction energy} between magnets $i$ and $j$
	\begin{equation}
		E_{\mathrm{MS},i,j} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\abs{\vc{r}_{ij}}^3} - \frac{3(\vc{\mu}_i \bcdot \vc{r}_{ij}) (\vc{\mu}_j \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^5}) \mathrm{,}
		\label{eq:E_MS}
	\end{equation}
	with $\mu_0$ the vacuum permeability and $\vc{r}_{ij} = \vc{r}_i - \vc{r}_j$ the vector connecting the two magnetic dipoles $\vc{\mu}_i$ and $\vc{\mu}_j$. \par
	This is the main interaction dictating how nanomagnets influence each other, causing the typical properties of the various ASI lattices, e.g. superferromagnetism in the pinwheel lattice~\cite{li2018pinwheel}.
	Because of its importance, this is the only interaction \hotspice considers by default when an ASI is created.
	Any other energy contributions must explicitly be added to an ASI; see \cref{sec:2:API_energies}.
	This avoids wasting calculations on energies not relevant to the simulation.
	
	\item The \textit{Zeeman energy} of an external field $\vc{B}_\mathrm{ext}$ interacting with magnet $i$
	\begin{equation}
		E_{\mathrm{Z},i} = -\vc{\mu}_i \bcdot \vc{B}_\mathrm{ext} \mathrm{,} \label{eq:E_Z}
	\end{equation}
	where $\vc{B}_\mathrm{ext}$ can be set for each magnet individually. \par
	This energy contribution provides a means for the outside world to interact with the system, and is therefore indispensable when we will be investigating reservoir computing later on.
	Even if input is provided through other means than an external field, this energy contribution can often still be used by considering an effective field instead.
	
	\item The \textit{exchange coupling energy} between nearest neighbours (NN) $i$ and $j$
	\begin{equation}
		E_{\mathrm{exch},i,j} = J \frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\mu_i \mu_j} \mathrm{,} \label{eq:E_exch}
	\end{equation}
	with $J$ the exchange coupling constant, which is constant throughout the ASI. \par
	This interaction is rarely present in ASI, but can for example be relevant in interconnected ASI --- whether by design or due to limited lithographic accuracy.
	We will encounter an example of the latter in \cref{sec:3:OOP:MFM}.
\end{enumerate}

The combined \textit{interaction energy} $E_i$ of a single magnet $i$ with its environment is then given by
\begin{equation}
	E_i = E_{\mathrm{Z},i} + \sum_j E_{\mathrm{MS},i,j} + \sum_{j \in \mathcal{N}_i} E_{\mathrm{exch},i,j} \mathrm{,}
	\label{eq:E}
\end{equation}
where $\mathcal{N}_i$ is the collection of nearest neighbours of magnet $i$.
Which magnets are included in this collection depends on the ASI lattice and which site of the unit cell magnet $i$ is in, and can be defined separately for each ASI lattice. \\\par
Note that all terms in \cref{eq:E} simply change sign when magnet $i$ switches ($\vc{\mu}_i \rightarrow -\vc{\mu}_i$).
The magnetostatic self-energy is not present in \cref{eq:E} because it only contributes a constant offset and does not change when the magnet switches.
As such, $E_i$ represents the total interaction energy of a magnet with its `neighbours',\footnote{
	In this context, a `neighbour' of a magnet can be interpreted more broadly as all magnets it interacts with through a particular energy contribution. For example, the magnetostatic interaction considers all magnets to be `neigbours', unless the user has explicitly set a maximum interaction distance.
} and therefore the change in energy of the whole ASI when magnet $i$ switches is simply $\Delta E_{i,1\rightarrow2} = -2 E_i$.
This is called the \textit{switching energy}, and we will later see (\cref{sec:2:Dynamics}) that it plays a central role in the algorithms used for simulating system dynamics.
It is therefore very advantageous to have such a simple method of calculating the switching energy. \par
When the simulation is initialized, the energy contributions are calculated for all magnets.
Each magnet therefore stores a value in memory for each of the terms in \cref{eq:E}.
Whenever a magnet switches, the energies of its `neighbours' are updated appropriately, which constitutes a major part of the calculation effort required for every step in the simulation.

\subsection{Finite-size corrections to the magnetostatic energy}
\cref{eq:E_MS,eq:E_Z,eq:E_exch} approximate each nanomagnet as a point dipole, but real nanomagnets have a finite spatial extent.
If one assumes a uniform magnetisation throughout each single-domain nanomagnet, then this finite size does not affect the Zeeman energy, nor the exchange energy which can capture any shape-related effects by an appropriate choice of the exchange coupling $J$.
The Zeeman energy could even account for a deviation from uniform magnetisation near the edge of a magnet, which occurs in reality but which we will neglect in the following, by an appropriate choice of $\vc{B}_\mathrm{ext}$. \par
The magnetostatic interaction, however, depends on the relative position, orientation, and shape of all magnets.
This may result in inadequate simulation of closely spaced ASI where the true magnetostatic coupling can be significantly stronger than predicted by a point dipole approximation.
Therefore, two (mutually exclusive) improvements have been implemented in \hotspice, which rescale the magnetostatic interaction energy between magnets.

\subsubsection{Second-order correction for dipoles}
\textit{Politi and Pini}~\cite{Dipolar2Dparticles} have presented a multipole expansion of the magnetostatic interaction, to account for the finite size of 2D nanomagnets (i.e., lateral dimensions $\gg$ thickness), assuming a uniform magnetization.
This results in a second-order correction
\begin{equation}
	E_{\mathrm{MS},i,j} = E_{\mathrm{MS},i,j}^\mathrm{(0)} + E_{\mathrm{MS},i,j}^\mathrm{(2)} \mathrm{,}
\end{equation}
where $E_{\mathrm{MS},i,j}^\mathrm{(0)}$ is the original point dipole magnetostatic interaction given by~\cref{eq:E_MS}. \par
The second-order correction can be written as
\begin{equation}
	E_{\mathrm{MS},i,j}^\mathrm{(2)} = \frac{\mu_0}{4\pi} \frac{3\mathcal{I}_{ij}}{2} \Bigg[3\frac{\vc{\mu}_i^\mathrm{OOP} \bcdot \vc{\mu}_j^\mathrm{OOP}}{\abs{\vc{r}_{ij}}^5} + \frac{\vc{\mu}_i^\mathrm{IP} \bcdot \vc{\mu}_j^\mathrm{IP}}{\abs{\vc{r}_{ij}}^5} -5\frac{(\vc{\mu}_i^\mathrm{IP} \bcdot \vc{r}_{ij}) (\vc{\mu}_j^\mathrm{IP} \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^7} \Bigg] \mathrm{,}
\end{equation}
where $\vc{\mu}_i$ was split into its IP and OOP components, conveniently leading to separate IP and OOP terms as implemented in the two types of ASI in \hotspice. \par
The particular shape of the nanomagnets is encapsulated in the single scalar $\mathcal{I}_{ij} = (\mathcal{I}_i + \mathcal{I}_j)/2$.
These $\mathcal{I}$ are calculated similar to a moment of inertia:
\begin{equation}
	\mathcal{I}_i = \int_{\Omega_i} \abs{\vc{r} - \ab(\int_{\Omega_i} \vc{r} d\vc{r})}^2 d\vc{r} \mathrm{,}
\end{equation}
with $\Omega_i$ the volume of magnet $i$.
We assume all magnets have the same shape, such that $\mathcal{I}_{ij} = \mathcal{I}_i = \mathcal{I}_j$.
Since nanomagnets in ASI are typically rather elliptical (IP ASI) or round (OOP ASI), \hotspice allows the user to set the semi-major axis $a$ and semi-minor axis $b$ of the magnets comprising an ASI.
For such a geometry --- elliptic cylinders --- these moments of inertia reduce to the simple expression $\mathcal{I}_{ij} = \frac{1}{4}(a^2 + b^2)$. % TODO REF: add examples of papers using round or elliptical magnets?
%While this correction can be applied to both IP and OOP magnetic dipoles, it is most effective for OOP systems, as can be seen in~\cref{fig:2:MS_distance}.

\subsubsection{Dumbbell model}\label{sec:2:Dumbbell}
Instead of representing a magnet as a point dipole, one may instead choose to represent it as a pair of magnetic monopoles~\cite{MagneticMonopoles2008,MagneticMonopoleDynamics}.
This introduces a new parameter $d$: the effective distance between the north and south poles of a magnet, with respective positions $\vc{r_N}_i = \vc{r}_i + s_i\frac{d_i}{2}\vc{u}_i$ and $\vc{r_S}_i = \vc{r}_i - s_i\frac{d_i}{2}\vc{u}_i$.
An appropriate choice of $d_i$ (slightly smaller than the physical length $l$ of the nanomagnet~\cite{DDG_Masterproef}) allows this dumbbell model to emulate the spatial extent of a real nanomagnet. \par
The north and south monopoles are assigned magnetic charges $+q_i$ and $-q_i$, respectively, with $q_i=\mu_i/d_i$~\cite{MagneticMonopoles2008}.
This choice yields the same effective dipole moment at long distance.
The interaction energy between two magnetic charges $q$ and $q'$ can be derived from the magnetic version of Coulomb's law~\cite{ForceMagneticDipole} as
\begin{equation}
	E = -\int_\infty^{\vc{r}} \frac{\mu_0}{\num{4}\pi}\frac{qq'}{\abs{\vc{r}}^3} \hat{\vc{r}} \cdot d\vc{r} = \frac{\mu_0}{\num{4}\pi} \frac{qq'}{\abs{\vc{r}}} \mathrm{.}
\end{equation}
The magnetostatic interaction energy between two nanomagnets is then the sum of their four mutual monopole interactions, finally resulting in
\begin{equation}
	E_{\mathrm{MS},i,j} = \frac{\mu_0 \mu_i \mu_j}{4\pi d_i d_j} \Bigg(\frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_N}_j}} + \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_S}_j}}\\ - \frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_S}_j}} - \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_N}_j}}\Bigg) \mathrm{.} \label{eq:2:E_MS_mono}
\end{equation}
The minus sign in the equation appears because north and south poles have opposite charge.
\hotspice is limited to a single value of $d$ for all magnets, due to the structure of the kernels used to calculate the magnetostatic interaction, which will be discussed in~\cref{sec:2:Kernels}.

\subsubsection{Comparison}
To assess whether these two corrections constitute an improvement to the resulting magnetostatic energy, we must quantify their impact by comparing against a known solution.
For this, we use the micromagnetic simulation package \mumax~\cite{mumax3}, which can determine the interaction energy for a given arrangement of ferromagnetic material.
While this solution is not exact, as \mumax uses a finite difference (FD) discretisation, the result will approach the true value for sufficiently small FD cell sizes. \\\par

\cref{fig:2:MS_distance} compares the original point dipole approximation and the two corrections (``finite dipole'' and ``dumbbell'') against the solution obtained with \mumax.
The figure shows 3 typical arrangements of neighbouring magnets in an ASI: two circular OOP magnets and two elliptical IP neighbours aligned along their easy and hard axes.
The magnetostatic interaction energy between the pair of magnets, divided by $\mu^2$ to be independent of magnet volume, is shown as a function of their normalized centre-to-centre distance.
A uniform magnetisation was used in the \mumax simulation, as this is the assumption under which the corrections were derived and because the lowest energy magnetisation state is size-dependent while the figure uses dimensionless units. \\\par

\xfig[1.0]{2_Hotspice/MS_distance.pdf}{
	\label{fig:2:MS_distance}Magnitude of the magnetostatic interaction between two magnets as a function of their normalized center-to-center distance, for the three \hotspice{} calculation methods (point dipole, second-order correction for dipoles, and dumbbell) compared to a micromagnetic \mumax{} calculation. OOP magnets are assumed to be circular with diameter $2r$, IP magnets are ellipses with length $l$ and width $w=4l/11$. Positions of north and south monopoles used in the dumbbell model are shown as red {\color{red}$\bullet$} and blue {\color{blue}$\bullet$} dots and are a distance $d=0.9l$ apart within a magnet.
}

For out-of-plane (OOP) systems, the dumbbell model is inadequate due to the small fringe fields and the limited thickness of the magnets.
Instead, the second-order dipole correction is more appropriate, yielding a significant improvement towards the ideal \mumax curve.
Still, a discrepancy remains for separations below $r_{ij}/2r \lessapprox 1.5$, which could be reduced by even higher-order corrections. \\\par

For IP systems, the dumbbell model constitutes a vast improvement over the standard point dipole treatment.
The dumbbell model does, however, require an additional parameter $d$, which affects the interaction energy.
For the best correspondence with \mumax, the monopole-monopole distance $d$ should be set slightly shorter than the length $l$ of a magnet, typically around $d/l\approx0.9$.
This adjustment accounts for the curvature and corresponding non-uniform magnetisation at the ends of real nanomagnets.
Similar values for $d/l$ were previously found in~\ccite{DDG_Masterproef} for typical nanomagnet shapes like ellipses and stadiums. \par
In contrast, the second-order dipole correction has little effect in IP systems and can even increase the discrepancy with \mumax. It emulates increased spatial extent and therefore always increases the interaction, but for magnets neighbouring along their hard axes (rightmost panel in~\cref{fig:2:MS_distance}) the point dipole model already overestimates the interaction. \par
In conclusion, the dumbbell model is preferred for IP systems, while the second-order dipole correction is most suitable for OOP systems.

\subsection{Effective energy barrier}
\subsubsection{Intrinsic barrier from shape anisotropy}
\subsubsection{Mean-barrier approximation} % and relevant choices with discontinuities
\subsubsection{Asymmetric barrier}
\subsubsection{Exact solution} % OOP only

\section{Dynamics}\label{sec:2:Dynamics}
A Monte Carlo simulator would not be complete without an algorithm to change the state of the system.
\subsection{N\'eel relaxation: temporal evolution}
\subsection{Metropolis-Hastings: sampling equilibrium states}
% TODO: decide whether to talk about nomenclature details etc. here, or in the introduction.
\subsection{Other Monte Carlo algorithms} % Not implemented, explain why Wolff could be useful but why it's not in Hotspice

\section{Implementation}\label{sec:2:Implementation}
Now that the physics underlying the simulator have been extensively discussed, we turn our attention to the implementation in software. \par
First, we discuss why we chose to implement the ASI on an underlying rectilinear grid, and explain how the \textit{`kernel'} --- the lookup table for the magnetostatic interaction --- was implemented.
We then take a look at the performance of \hotspice and how it has improved since its initial version due to improvements to the kernel.
One particular performance-enhancing feature is examined in more detail: the possibility to select multiple magnets at once in the Metropolis-Hastings algorithm.
Finally, we briefly discuss the structure of the package and the functionality included in the various submodules, and finish with a small discussion of things that could have been done better.

\subsection{Grid}
\hotspice{} represents an ASI as a rectilinear grid of non-uniform unit cells, with magnets positioned at selected grid points.
This choice was made based on a trade-off; calculation efficiency against the freedom to place magnets arbitrarily.
We opted to prioritise efficiency and accept the geometrical restriction, as most ASI research focuses on periodic lattices.
Despite the seemingly restrictive nature of the rectilinear grid,~\cref{fig:2:ASIs} illustrates its versatility in forming various periodic lattices, with only the Cairo lattices requiring grid non-uniformity.
As a bonus, real-time visualisation is simple and efficient for a rectilinear grid, as the underlying matrix can directly be cast to a pixel image. \par
By leveraging the unit cell concept in periodic lattices and the efficient indexation of a rectilinear grid in computer memory, several aspects of the calculation can be performed more efficiently than for free-form ASI. The unit cell of each lattice in~\cref{fig:2:ASIs} is depicted as a grey rectangle. Although non-rectilinear unit cells with fewer magnets could be identified for some lattices, \hotspice{} does not consider these to reduce complexity and to maintain a clear connection to the underlying rectilinear grid of the ASI implementation.

\subsection{Kernels}\label{sec:2:Kernels}
The kernel serves to reduce the amount of calculations required for every switch, so there is usually a trade-off between the initialisation time and runtime when simulating dynamics.
\subsubsection{Magnetostatic interaction kernel}
\subsubsection{Perpendicular magnetostatic kernel} % If not yet explained in detail earlier, talk about the perpendicular kernel etc. here
\subsubsection{Numerical error with cut-off kernel} % TODO
A performance improvement presents itself by cutting off the magnetostatic kernel at a certain distance.
Due to the underlying grid, the most natural way of achieving this is to simply reduce the size of the kernel from $2L-1 \times 2L-1$ to a smaller $2K-1 \times 2K-1$ central region. \par % Note that the size has to remain odd for the convolution to still place the magnet in the center.
However, this will result in inaccurate interaction energies being stored.
When multiple magnets switch, this error will accumulate.
Luckily, the error introduced in this manner is bounded from above: whenever a magnet switches back, the error its original switch introduced is cancelled (barring some remaining floating-point error).

\subsection{Multi-switching in Metropolis-Hastings} \label{sec:2:MultiSwitch} % TODO: I already talk about multi-switching in the performance section, should we put this earlier?
\subsubsection{Minimal distance between sampled magnets} % Derive equation
\subsubsection{Selection algorithms}
\paragraph{Poisson disc}
\paragraph{Grid-select}
\paragraph{Hybrid grid-poisson}

\subsection{Performance} % Several factors impacting performance, explain by using that one graph
% TODO: update this introductory text, because now this section comes after the explanation of the kernels.
\subsubsection{History}
The performance of \hotspice has been improved throughout development in various ways.
Calculation on the GPU was made possible early on and the efficiency of updating the magnetostatic interaction was improved drastically.
The number of magnets in the ASI has a major impact on performance, and often forms the determining factor as to whether calculation on the GPU rather than CPU will result in a faster simulation. \par
We start with a summarizing chronological table of these various improvements, and explain each step in more detail in the following paragraphs.
Two tables are shown:~\cref{tab:2:perf_init} for the initialisation time, and~\cref{tab:2:perf_switch} for the time it takes to perform 5000 switches using the N\'eel update algorithm, taking into account the magnetostatic interaction between all magnets.
During initialisation, the magnetostatic kernel is constructed, after which the starting energy $E_i$ of all magnets is calculated.
In the tables, only the last 3 rows use the kernel as it was explained in~\cref{sec:2:Kernels}; the other rows use less efficient kernels as explained below. \par % Can't really call it a "lookup table" if it is being convolved etc., right?
This test was benchmarked on an NVIDIA GeForce RTX 3080 Mobile GPU and an 11th Gen Intel\textregistered{} Core\texttrademark{} i7-11800H @ 2.30GHz.
Simulation times displayed serve an illustrative purpose: absolute values on other hardware may vary significantly, though the general trends within the tables should remain similar.

% Q: should we use L or N=L²? I would prefer L because at some point we talk about 2D indexation, so L makes it clearer that we have a 2D system.
\xtable[tab:2:perf_init]{\textbf{Initialisation time} for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		Simulation size $L$ & 50 & 100 & 150 & 200 & 400 & 1000 \\
		\hline \hline
		CPU | \makecell{Full interaction kernel} & 0.5s & 6.8s & 34.0s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Full interaction kernel} & 2.2s & 5.4s & 11.9s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 1.0s & 3.4s & 7.6s & 14.5s & 91.6s & ? \\
		\hline
		GPU | \makecell{Unitcell-kernel (\cref{sec:2:Kernels})} & 2.4s & 4.6s & 9.7s & 16.3s & 66.2s & 730.4s \\
		\hline \hline
		GPU | \makecell{Final version\\(convolution initialization)} & 1.0s & 1.0s & 1.0s & 1.0s & 1.6s & 23.9s \\
		\hline
		CPU | \makecell{Final version\\(convolution initialization)} & 0.014s & 0.2s & 1.0s & 3.2s & 53.1s & ? \\
		\hline
	\end{tabular}
}

\xtable[tab:2:perf_switch]{\textbf{Time for 5000 switches} using the N\'eel algorithm, for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		Simulation size $L$ & 50 & 100 & 150 & 200 & 400 & 1000 \\
		\hline \hline
		CPU | \makecell{Full interaction kernel} & 9.4s & 217.6s & ? & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Full interaction kernel} & 5.0s & 13.6s & 51.8s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 7.1s & 7.2s & 8.2s & 9.0s & 13.0s & ? \\
		\hline
		GPU | \makecell{Unitcell-kernel (\cref{sec:2:Kernels})} & 7.8s & 7.9s & 8.3s & 8.9s & 11.8s & 36.9s \\
		\hline \hline
		GPU | \makecell{Final version\\(2D indexation)} & 6.8s & 7.0s & 7.2s & 7.6s & 10.2s & 35.9s \\
		\hline
		CPU | \makecell{Final version\\(2D indexation)} & 0.56s & 1.3s & 2.5s & 4.3s & 41s & ? \\
		\hline
	\end{tabular}
}

\paragraph{From CPU to GPU}
Originally, \hotspice performed all calculations on the CPU. % TODO: explain CPU/GPU briefly? Or elsewhere?
This caused the simulation time to rise dramatically as the system size grew (e.g. first row in \cref{tab:2:perf_switch}).
For early versions of \hotspice with an unoptimized kernel, this made it impractical to simulate systems larger than $50 \times 50$ magnets.
By simply switching to GPU-based calculations using the \python{CuPy} library (second row), this upper limit was increased significantly.
The limiting factor instead became memory consumption of the unoptimized kernel. \par
Do note, however, that for small systems the GPU performs worse than the CPU.
This remains true independent of the kernel implementation.
GPUs are optimized for parallel processing, and therefore reach a bottleneck when the simulation takes on a more sequential nature~\cite{owens2008gpu}.
Simulating a single switch requires several distinct operations that do not lend themselves to parallelisation.
When operating on small arrays, this means relatively fewer parallel calculations can be performed, thereby increasing the importance of being able to perform distinct mathematical operations in quick succession.
The CPU excels at the latter. % TODO: references for GPU performance and operation

\paragraph{Kernel improvements}
The first two rows `\textit{Full interaction kernel}' in the tables use the original method, where the kernel was far more naïve: it was simply an $N \times N$ matrix which stored the magnetostatic interaction between each pair of magnets.
The initialisation time on CPU (first row in~\cref{tab:2:perf_init}) bears witness to this, as $t_\mathrm{init} \propto L^4 = N^2$.
This is because the initialisation of the magnetostatic interaction energy $E_\mathrm{MS}$ requires iterating over all magnets and adding these energies sequentially.
On GPU, parallelisation of each individual term in this summation approximately reduces this to $\propto L^2 = N$. \par
This method obviously wasted a lot of memory: for periodic lattices, the matrix would contain many identical values. For a system as small as $170 \times 170$, this type of kernel already exceeded the \SI{8}{\giga\byte} of available memory.
While it was possible to reduce memory usage by instead using a sparse matrix and limiting the interaction distance, this would not drastically improve performance due to additional indexation overhead.
Clearly, a more efficient approach was required. \\\par

An extreme solution is to not use a kernel at all, as in the third row `\textit{No kernel}'.
Instead, for every switching magnet, its interaction energy with all other magnets was calculated from scratch.
The tables show that this clearly gets rid of the memory issue and yields a surprisingly fast simulation.
However, the initialisation for larger systems now takes a long time, because without a kernel this requires $N^2$ operations resulting in $t_\mathrm{init} \propto N=L^2$ on GPU for small systems.
Still, in all situations it is beneficial not to use the unoptimized kernel. \\\par

To reduce the initialisation time, two improvements had to be combined. \par
First, a kernel as described earlier in~\cref{sec:2:Kernels} had to be implemented, which overcomes the memory issue by using the periodic nature of most ASI to only store interactions for a single unit cell.
This is used in the fourth row `\textit{Unitcell-kernel}', but does not constitute a significant improvement on its own aside from enabling $1000 \times 1000$ systems for the first time. \par
Secondly, however, this new kernel enables the usage of a convolution to initialise the magnetostatic interaction energy $E_\mathrm{MS}$, rather than the sequential sum used before.
This is what the final two rows `\textit{Final version}' use, where we once again compare CPU and GPU as this is the final version.
Another minor improvement in the final version was to use 2D indexation (rather than a flat index $i = n_x y + x$ which did not synergize well with the convolution), yielding another \SI{15}{\percent} performance increase.

\paragraph{Final performance}
In the end, the upper limit for feasible calculation time has become $L \approx 1000$ on GPU and $L \approx 400$ on CPU.
Comparing the last two rows of \cref{tab:2:perf_switch} reveals that GPU outperforms CPU for $L \gtrapprox 300$.
However, this is for the N\'eel algorithm which only performs a single switch at a time.
Performing multiple switches simultaneously in Metropolis-Hastings benefits greatly from the unitcell-kernel, as it can use the same principle of convolution as was already used in the initialisation.
With this algorithm, the GPU can maintain the advantage for systems as small as $L \gtrapprox 60$.

\subsubsection{Multi-switching}
The N\'eel algorithm, used in the preceding discussion about performance, does not implement a multi-switching procedure.
This makes it very inefficient for large systems: for a system of $N$ magnets to change its macrostate significantly, on the order of $N$ switches must occur.
Even though the time required to perform a single switch with the N\'eel algorithm stays roughly constant up to $L<400$, the time per Monte Carlo sweep\footnote{
	A Monte Carlo ``step'' refers to attempting to switch a single magnet. By a ``Monte Carlo sweep'' (MCS), we refer to $N$ attempted switches when the simulation contains $N$ magnets~\cite{NumericalDynamicalNiedermayer}. Note that the distinction between attempted switches and actual switches only exists in the Metropolis-Hastings algorithm. % TODO END: Could also decide to put this in a \begin{definition} environment somewhere earlier.
} will scale linearly with the number of magnets if each iteration only switches a single magnet.
The ability to perform multiple switches simultaneously in the Metropolis-Hastings algorithm (described earlier in \cref{sec:2:MultiSwitch}) alleviates this problem, allowing for a significantly higher amount of Monte Carlo sweeps per second. \\\par
\cref{fig:2:Performance} shows the performance of the multi-switching algorithm as a function of system size, both for calculation on the CPU (a) and GPU (b).
The multi-switching algorithm selects a certain number of magnets per second, as depicted by the blue curve: these are candidates for switching.
Dividing this value by $N$, the number of magnets in the system, gives the number of Monte Carlo sweeps per second, depicted by the black curve.
This is the main performance metric in Monte Carlo simulations. \par
Only a subset of the selected magnets will switch, as depicted by the red curve.
This switching rate is highly dependent on the specific conditions of the simulation --- particularly the ratio $\EBtilde/T$ --- making it infeasible to make general statements.
There exist situations where no magnets switch or where any selected magnet switches.
In the figure, values of $a$, $T$, and $\EB$ were chosen which result in a reasonable switching rate.

\xfigs[0.47]{2_Hotspice/Performance_CPU.pdf}{CPU}{2_Hotspice/Performance_GPU.pdf}{GPU}{
	\label{fig:2:Performance}Performance of OOP square ASI as a function of system size, using the Metropolis-Hastings algorithm with multi-switching ($Q=0.05$). Lattice spacing $a = \SI{1}{\micro\metre}$, temperature $T = \SI{100}{\kelvin}$ and energy barrier $\EB = 0$.
} % Q: Performance for IP ASI might be worse. Add a Pinwheel figure as well?

There is once again a stark contrast between CPU and GPU. \par
On CPU, the sampling rate starts off rather constant for small systems, as overhead dominates.
For $N>100$ magnets, the sampling rate increases proportional to $N$, leading to a stable performance (MCS/s) independent of $N$.
However, performance drops dramatically once the system size increases beyond $\approx 80 \times 80$. % While this is reminiscent of a lack of garbage collection while building the figure, this is not the case here as the drop remains even when starting the figure at $L=80$.
We hypothesize that this is due to exceeding a CPU cache size. \par
%On the CPU used for~\cref{fig:2:Performance} (the aforementioned i7-11800H), the L1 and L2 cache hold \SI{48}{\kilo\byte} and \SI{1.25}{\mega\byte}, respectively.
% Q: Are any of the below hypotheses reasonable? Which is most reasonable?
% Hypothesis 1: The size occupied by a 72x72 ASI is 1.25MB. It is not unreasonable that, soon after reaching this point, some arrays can no longer be stored on this cache level, impacting performance.
% Hypothesis 2: The size of mm.m at 78x78 is 48kB, so we might be exceeding the L1 cache beyond this point.
% Hypothesis 3: The size of the dipole kernel at this point is around 220kB. Doesn't seem to match up with anything.
% TODO: Revisit this at some point, it is odd that on another CPU (Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz) the cut-off happens at the same system size, indicating that it is not cache-size-related because that CPU has 256kB L2 and 32kB L1, both different from the UGent laptop.
The GPU, on the other hand, hits its stride for exactly those systems that are too large for the CPU to handle.
Around $70 \times 70$, the sampling rate increases rapidly, resulting in an increased number of MC sweeps per second (MCS/s) that reach a local maximum for $200 \times 200$ magnets.
Beyond this, the sampling rate stagnates, eventually settling at around \SI{150e3}{samples\per\second}, corresponding to an ever decreasing MCS/s as the parallelism of the GPU gets exhausted.

\subsubsection{RNG}
Another factor which may contribute to the performance is the Random Number Generator (RNG).
Both update algorithms rely on generating a large amount of random numbers to select the next magnet(s) to switch. % TODO: talk a bit about XORWOW etc. and how it actually doesn't really impact performance

\subsection{Package structure} % Structure of Hotspice, explain all modules (core, ASI, energies, (config? with GPU)...)
Hotspice is written as a Python 3.10 package and can perform simulations on either the CPU or GPU.
The optimal hardware choice depends on the size of the ASI and the update scheme used.
By default, \hotspice runs on the CPU using the popular NumPy and SciPy libraries.
For GPU-accelerated array manipulation, the CuPy v11.4~\cite{CuPy} library is used, but this is an optional dependency. \par
\paragraph{\python{hotspice.config}: GPU/CPU choice}
By default, \hotspice runs on the CPU. To run on the GPU, the environment variable \python{HOTSPICE_USE_GPU} must be set to \python{"true"} before the the \hotspice package is loaded in a Python script with \python{import hotspice}. It is not possible to switch between GPU/CPU within a script.
\subsubsection{ASI}
\paragraph{\python{hotspice.ASI}: predefined ASI lattices}
This module provides two abstract classes from which all ASIs should inherit: \python{hotspice.ASI.IP_ASI} or \python{hotspice.ASI.OOP_ASI}.
Various lattices are available, as previously shown in~\cref{fig:2:ASIs}.
These all follow the same pattern: \python{hotspice.ASI.<ASI_name>(a, n, **kwargs)}, with two positional arguments.
The first argument is the lattice parameter, as defined by the red indicator in \cref{fig:2:ASIs}.
The second argument is the size of the underlying grid (one can also specify \python{nx} and \python{ny} separately).
For the predefined lattices in the \python{hotspice.ASI} module, these two parameters are enough information to create a rudimentary ASI.
\paragraph{Energies}\label{sec:2:API_energies}
Three predefined energy contributions are provided in the \python{hotspice.energies} module, though they can be accessed from the main \python{hotspice} namespace because they are used so often.
The magnetostatic interaction is implemented by the \python{hotspice.DipolarEnergy} or \python{hotspice.DiMonopolarEnergy} classes, with the latter using the monopole approximation to calculate the interaction.
By default, an ASI object takes only the \python{hotspice.DipolarEnergy} into account.
When relevant, the user has to explicitly add a \python{ZeemanEnergy} or \python{ExchangeEnergy}.
It is possible to set longer-range exchange interactions (next-nearest neighbour etc.) by manually setting the \python{local_interaction} field of an \python{ExchangeEnergy} object.
% TODO: create a custom "API" environment that we can put near all the different parts of the model to show code outside the main text?
\subsubsection{RC} % io and experiments modules
\subsubsection{Utilities} % Explain utils module (but only those functions/classes relevant to users) and perhaps mention the existence of plottools
\paragraph{GUI}
A graphical user interface (GUI) is available for \hotspice, which allows the user to directly interact with the ASI and observe changes in realtime. It can be run by calling \python{hotspice.gui.show(mm)}, with \python{mm} the ASI object, resulting in the window shown in~\cref{fig:2:GUI}.

\xfig[1.0]{2_Hotspice/GUI.png}{
	\label{fig:2:GUI}The \hotspice graphical user interface. This example shows an $80 \times 50$ pinwheel ASI.
}

The state of the ASI is prominently displayed, and the bottom left panel shows a few useful statistics such as the elapsed time. The ASI display is controlled by the central bottom panel, which changes between 4 display modes.
By default, the magnetisation is shown as an averaged field, with the averaging method determined by the ASI lattice.
The second mode displays individual arrows, which either show each magnet's magnetisation direction or the effective field $\vc{H}_\mathrm{eff}$ it experiences. % Q: is it H? I wrote H because it does not add the magnetic moment. On a similar note, should we write B_ext or H_ext in the definition of the Zeeman energy?
In the third mode, the energy contributions ($E_\mathrm{MC}$, $E_\mathrm{Z}$, $E_\mathrm{Exch}$ and their sum $E_i$) and the resulting effective energy barrier $\EBtilde$ for each magnet can be shown, both along the easy and hard axis.
The last option is to show the spatial distribution (i.e., the value for each magnet) of some parameters like the temperature $T$, shape anisotropy $\EB$ and size of the magnetic moment $\mu$. \par
Buttons in the sidebar on the right allow the user to interact with the ASI by progressing through time, setting an initial state, or --- for more complex simulations --- apply custom functions.
By clicking with the mouse on the ASI plot, it is also possible to interact with the ASI at a granular level, switching one or multiple magnets. Finally, the red box in the bottom right controls the update algorithm.

\subsection{Advantages and disadvantages of the \hotspice approach} % Hindsight is 20/20
\subsubsection{Grid}
\subsubsection{Input/output RC}
\subsubsection{GPU/CPU}
In hindsight, for the purposes of this thesis it was not necessary to go through all the effort to port \hotspice to the GPU.
In~\cref{ch:Applications}, I rarely use large systems, and often use the N\'eel algorithm for which multi-switching was not implemented.
Therefore, the vast majority of calculations in that chapter will be performed on CPU. 
In hindsight, we mostly worked on small systems with the N\'eel algorithm (to simulate RC in OOP ASI), making all the effort of allowing \hotspice to run on the GPU unnecessary. Still, the effort was not wasted as it has expanded the envelope of applicability for \hotspice, particularly when using the Metropolis-Hastings algorithm.
The fact that the current implementation requires making the choice to use GPU or CPU before the \python{import hotspice} statement can be limiting in specific situations. A spiritual successor to \hotspice should address this limitation.

\section{Verification} % See paper
Now that we know the ins and outs of the model used by \hotspice, all that remains is to verify its correct implementation.
To this end, we simulate several systems of increasing complexity for which analytical solutions are available.
The examples discussed here are all equilibrium problems, so we will use the Metropolis-Hastings algorithm to verify that it indeed samples the equilibrium state space.
Special attention will be given to the issue of ``critical slowing down'' which plagues this algorithm near phase transitions.
\subsection{Hexagon}
Before progressing to equilibrium problems, we briefly check that the magnetostatic interaction and its underlying kernels all work as expected.
The magnetostatic interaction energy of any given arrangement of nanomagnets can be calculated analytically and compared to \hotspice.
As an example that is neither too simple nor complex, we use a regular hexagon in a vortex state. The energy of all magnets should equal the following analytical solution, which is the sum of the magnetostatic interaction energy between nearest neighbours $\circled{1}$, next-nearest neighbours $\circled{2}$ and magnets on opposite sides of the hexagon $\circled{3}$:
\begin{align*}
	E_{\mathrm{MS},i} =&\, \circled{1} + 2 \times \circled{2} + 2 \times \circled{3} \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} -2\frac{\mu_0 m^2}{4\pi \frac{3 \sqrt{3} a^3}{8}} \Big[3\cos^2(\pi/3) - \cos(2\pi/3)\Big] -2\frac{\mu_0 m^2}{4\pi \frac{a^3}{8}} \Big[3\cos^2(\pi/6) - \cos(\pi/3)\Big] \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} \bigg[29+\frac{20}{3\sqrt{3}}\bigg] \mathrm{.}
\end{align*}
The following Python function verifies that, indeed, \hotspice gives the expected total energy $E_{\mathrm{MS},i}$ for all 6 magnets $i$ in the system.
\begin{lstlisting}
import numpy as np
import hotspice

def test_hexagon(l=470e-9, s=10e-9, m=1.1278401e-15):
	mm = hotspice.ASI.IP_Kagome(a := (l+2*s)/np.tan(30*np.pi/180), nx=5, ny=3, moment=m, pattern="vortex")
	E_sim = mm.get_energy('dipolar').E[mm.m.astype(bool)]
	E_exact = -1e-7*m*m/a**3*(29+20*np.sqrt(3)/9)
	print("OK" if np.allclose(E_sim, E_exact, atol=0) else "Fail")

test_hexagon() # OK
\end{lstlisting}

\subsection{Non-interacting spin ensemble}
\subsection{Exchange-coupled OOP square system}
\subsubsection{Critical slowing down}
\subsection{Exchange- and magnetostatically-coupled OOP square system}
\subsection{Square-to-pinwheel transition angle}\label{sec:2:Verification_IP_SquarePinwheel}
The in-plane square and pinwheel ASI lattices can be continuously transformed into each other by rotating each individual magnet by \ang{45}.
Despite being this closely related, their ground state magnetic ordering differs significantly.
Square ASI has an antiferromagnetic (AFM) ground state, where all vertices have a net zero magnetisation.
Meanwhile, pinwheel ASI exhibits superferromagnetic order, where all magnets with similarly-oriented easy axes are magnetized in the same direction~\cite{ApparentFMpinwheel}.
Therefore, a critical angle $\ang{0} < \alpha_c < \ang{45}$ must exist where the ground state transitions between these two extremes. \par
For the dipole model, theoretical calculations predict this transition at $\alpha_c = \arcsin(\sqrt{3}/3) = \ang{35.3}$~\cite{AFM-FM-transition-Pinwheel,MagicAngle}.
For the dumbbell model, the transition angle depends on the distance $d$ between monopoles, but is always larger than for the dipole model~\cite{AFM-FM-transition-Pinwheel}. \par
The result of a \hotspice simulation using both models is shown in \cref{fig:2:Pinwheel_angle}.
To quantify this transition, we measure the fraction of vertices with net zero magnetisation --- this value is 0 for superferromagnetic order while it is 1 for AFM order.
For the dipole model, the transition occurs at $\approx \ang{35}$ as expected, while the dumbbell model indeed transitions at a larger rotation angle.
% TODO: debate (with myself) whether or not to look for some more experimental references to include the following sentences.
% While this transition was experimentally observed to be gradual from \ang{35} to nearly \ang{45}, these experiments likely could not reach an equilibrium state due to defects, rapid quenching, or finite correlation times, all of which freeze domain walls in the system. Metropolis-Hastings samples the equilibrium state space, resulting in the sharp transition seen in the figure.

\xfig[0.6]{2_Hotspice/Pinwheel_angle.pdf}{
	\label{fig:2:Pinwheel_angle}Fraction of vertices with net zero magnetisation at equilibrium, as square ASI (left) transitions to pinwheel ASI (right) by rotating individual magnets. The theoretical transition angle $\alpha_c \approx \ang{35.3}$ for the dipole model is indicated by the vertical dotted line.
}


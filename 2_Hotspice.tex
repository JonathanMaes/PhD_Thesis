\chapter{Methods: ``\hotspice'' simulator for ASI}\label{ch:Hotspice}
% \glijbaantje{It's not a bug, it's a feature.}{Someone}

\begin{adjustwidth}{2em}{2em} % TODO END: update once published.
    \begin{center}
        \textbf{Material from this chapter has also been published in:} \\
    \end{center}
    \vspace{1em}
    \begin{adjustwidth}{0em}{1.5em}
	    \begin{itemize}
	    	\item[\cite{MAES-24}] J.~Maes, D.~De~Gusem, I.~Lateur, J.~Leliaert, A.~Kurenkov, and B.~Van Waeyenberge.
	    	\newblock The design, verification, and applications of Hotspice: a Monte Carlo simulator for artificial spin ice.
	    	\newblock \emph{ArXiV}, arXiv:\penalty0 2409.05580, 2024.
	    \end{itemize}
    \end{adjustwidth}
    \vspace{0.5em}
    \begin{center}
        \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{.5em}
    \begin{center}
    	The \hotspice simulator discussed in this chapter\\
    	is open-source and available on \href{https://github.com/bvwaeyen/Hotspice}{GitHub}. \\
    \end{center}
    %\vspace{0.5em}
    \begin{center}
    \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{1em}
\end{adjustwidth}

To assess the potential of \xref{Reservoir Computing} in (perpendicular-anisotropy) \xref{Artificial Spin Ice}, as is the main topic of this thesis, a simulation framework is needed that allows efficient exploration of the impact that various system parameters and input methods have on the reservoir performance. \\\par

Nanomagnetic systems are often simulated using micromagnetic codes, such as the finite-difference-based \mumax~\cite{mumax3} and \oommf~\cite{OOMMF} or the finite-element-based \nmag~\cite{Nmag}, which capture the magnetization dynamics of individual nanomagnets in great detail. \par
However, the time between successive switches of a nanomagnet is not necessarily similar to the timescale of micromagnetics.
Furthermore, determining RC metrics requires applying many input cycles --- on the order of 100 for the task-agnostic metrics --- to get a statistically valid result.
In this timeframe, many switches occur.
When the simulated time extends beyond several microseconds, as is typically the case, simulating even a modest number of magnets --- on the order of several dozen --- becomes computationally unfeasible~\cite{leo2021chiral}. \\\par

To address these limitations, specialized ASI simulation tools have been developed.
An example of this is the flatspin simulator~\cite{flatspin}, which implements deterministic spin flipping via a Stoner-Wohlfarth model~\cite{StonerWohlfarth2008}.
Using such higher-level approximations enables the study of collective behaviour in much larger systems and over far longer timescales than is feasible with micromagnetic codes, though at the cost of no longer simulating the internal magnetization structure of individual nanomagnets in detail.
Additionally, Monte Carlo methods are often used to simulate spin ices, including ASI, but these are typically specialized to a select few lattice geometries and often only account for nearest-neighbour interactions, whose strength is often arbitrarily set or calculated separately using micromagnetic codes.~\cite{MeltingASI,sklenar2019field,gilbert2014emergent,zhang2013crystallites} \\\par % REFS: 'gilbert2014emergent': MC sims based on a vertex model and interacting magnetic charges. 'zhang2013crystallites': uses monopoles and NN couplings to model kagome ASI with Metropolis and loop update. 'moller2006artificial': bit broader using dipolar interaction, though seemingly still only for nearest neighbors. 'mengotti2011kagome' use a full dipole model with Ewald summation for PBC. 'lou2023competing': dipole model for half-occupation IP Ising. 'sendetskyi2019continuous': dipole model for square ASI. 'EngineeringRelaxationComputation': KMC on dipole model, seemingly for square arrays. 'sklenar2019field': NN interactions calculated by mumax on quadrupole lattice. 'MeltingASI': 16-vertex ice model

Our goal was to blend these two approaches, resulting in \hotspice: a versatile Monte Carlo simulator meant to capture ASI physics with minimal arbitrary parameters, allowing various lattice configurations to be evaluated. \par
This software approximates each single-domain nanomagnet as a single Ising spin, associating energies with the various ASI states by accounting for the magnetostatic interaction between all magnets.
\hotspice supports both in-plane (IP) and out-of-plane (OOP) ASI, which may contain thousands of magnets. Simulations can span arbitrary timescales, as determined by the switching time of magnets in the system. \\\par

In this chapter, we discuss several model variants that have been implemented, and assess their accuracy in simulating the behaviour of ASI.
These variants differ in their choice of Monte Carlo spin-flip algorithm and their calculation of the magnetostatic interactions and energy barriers. \\\par

\section{Model}
In single-domain IP nanomagnets, the magnetisation prefers to align along the fixed easy axis of the geometry, while for OOP magnets a strong interfacial anisotropy causes a preferential orientation along the $z$-axis.
Either way, it is natural to use an Ising-like approximation for simulating such single-domain nanomagnets.
The position $\vc{r}_i$, axis $\vc{u}_i$ and size of the magnetic moment\footnote{
	The size of the magnetic moment $\mu_i$ corresponds to the total ground state magnetic moment $\abs{\int_{\Omega_i} \vc{M}(\vc{r})d\vc{r}}$, with $\Omega_i$ the shape of magnet $i$ and $\vc{M}(\vc{r})$ its magnetisation in the twofold degenerate ground state. Due to edge relaxation effects, this value is slightly smaller than $M_\mathrm{sat} V_i$.
} $\mu_i$ of each magnet $i$ are fixed and they are only allowed to switch between the `up' ($\uparrow$) and `down' ($\downarrow$) magnetisation states.
Thus, the total magnetic moment vector of magnet $i$ can be expressed as
\begin{equation}
	\vc{\mu}_i = s_i \mu_i \vc{u}_i \mathrm{,}
\end{equation}
where $s_i = \pm 1$ and $\abs{\vc{u}_i} = 1$. \par
The switching rate between these two states is determined by the effective energy barrier $\EBtilde$ that separates them, as well as the temperature $T$.
For an isolated nanomagnet, the energy barrier $\EB = K_\mathrm{u} V$\footnote{
	This is valid for switching by coherent rotation. Similar to the calculation of $\mu$, edge relaxation effects may cause the effective volume to be slightly smaller.
} originates from its uniaxial shape anisotropy $K_\mathrm{u}$.
Interactions with other magnets or external fields modify the energy landscape, leading to an effective barrier which we denote as $\EBtilde$~\cite{leo2021chiral}.
Each magnet can have a unique magnetic moment size $\mu_i$, temperature $T_i$ and energy barrier $E_{\mathrm{B},i}$.
This enables, for instance, modelling some of the disorder due to lithographic variations by assigning a different shape anisotropy to each magnet, typically sampled from a Gaussian distribution with mean $\EB$ and standard deviation $\sigma(\EB)$. \\\par

% We are most interested in simulating ASI, which are often well-ordered and spatially periodic. Therefore, \hotspice chooses to...
Due to the periodic nature of many ASI lattices, \hotspice chooses to perform the simulation on a rectilinear grid, the benefits and details of which will be discussed in~\cref{sec:2:Implementation}.
Each grid point may or may not contain a magnet, and the magnets must either all be of the IP type, or all OOP.
Even though this implementation does not allow complete freedom in the placement of magnets, many popular ASI lattices can be constructed in this manner. \par

\xfig[1.0]{2_Hotspice/ASIs.pdf}{
	Predefined artificial spin ice (ASI) lattices available in \hotspice.
	The unit cell of each lattice is delineated by a central dark grey rectangle.
	The red indicator defines the lattice parameter $a$.
	In the Ising approximation, the magnetization of in-plane magnets (top) aligns along the major axis of the depicted ellipses.
	Out-of-plane magnets (bottom) are illustrated as circles.
	\label{fig:2:ASIs}
}

\cref{fig:2:ASIs} showcases the 12 lattices that \hotspice provides out-of-the-box. \par
The pinwheel and square lattices come in two variants, related by a global \ang{45} rotation of the entire lattice.
This gives rise to different boundaries due to the Cartesian character of the underlying grid, which may alter the dynamics of the ASI.
Furthermore, the unit cell for lucky-knot pinwheel (b) and open square (d) is more compact than for the more popular diamond pinwheel (a) and closed square (c), resulting in faster simulation. \par % TODO REF: more popular in literature? Examples where these are used?
Magnets in the pinwheel lattices (a) and (b) are placed at the same location as in the square lattices (c) and (d), respectively, but with each magnet rotated by \ang{45}.
The same can be said of the triangle (f) and kagome (g) lattices where individual magnets are rotated by \ang{90}.
The Cairo lattice (h) can be continuously deformed into the Shakti lattice~\cite{ShaktiCairo}, but note that the point dipole model is no longer appropriate for the latter; instead, a dumbbell model (see~\cref{sec:2:Dumbbell}) would be more accurate. \par
The remaining four IP and four OOP lattices are also related: the magnets in the OOP lattices (i)-(l) are positioned at the vertices where magnets meet in their respective IP counterparts (e)-(h).

\section{Energy calculation}
The energy of the system is the driving force behind these Monte Carlo simulations.
In this section, we will list the various energy contributions used in \hotspice, explain how we may account for the finite size of real nanomagnets in this Ising-like model, and show how the effective energy barrier $\EBtilde$ is calculated.

\subsection{Energy contributions}
Three energy contributions have been implemented in \hotspice, supporting both open and periodic boundary conditions (PBC).\footnote{
	Users can implement more energy contributions by inheriting from the \python{hotspice.Energy} class and implementing the \python{abstractmethod}s, taking care to correctly account for open or periodic BC when necessary.
}
\begin{enumerate}
	\item The \textit{magnetostatic interaction energy} between magnets $i$ and $j$
	\begin{equation}
		E_{\mathrm{MS},i,j} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\abs{\vc{r}_{ij}}^3} - \frac{3(\vc{\mu}_i \bcdot \vc{r}_{ij}) (\vc{\mu}_j \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^5}) \mathrm{,}
		\label{eq:E_MS}
	\end{equation}
	with $\mu_0$ the vacuum permeability and $\vc{r}_{ij} = \vc{r}_i - \vc{r}_j$ the vector connecting the two magnetic dipoles $\vc{\mu}_i$ and $\vc{\mu}_j$. \par
	This is the main interaction dictating how nanomagnets influence each other, causing the typical properties of the various ASI lattices, e.g. superferromagnetism in the pinwheel lattice~\cite{li2018pinwheel}.
	Because of its importance, this is the only interaction \hotspice considers by default when an ASI is created.
	Any other energy contributions must explicitly be added to an ASI; see \cref{sec:2:API_energies}.
	This avoids wasting calculations on energies not relevant to the simulation.
	
	\item The \textit{Zeeman energy} of an external field $\vc{B}_\mathrm{ext}$ interacting with magnet $i$
	\begin{equation}
		E_{\mathrm{Z},i} = -\vc{\mu}_i \bcdot \vc{B}_\mathrm{ext} \mathrm{,} \label{eq:E_Z}
	\end{equation}
	where $\vc{B}_\mathrm{ext}$ can be set for each magnet individually. \par
	This energy contribution provides a means for the outside world to interact with the system, and is therefore indispensable when we will be investigating reservoir computing later on.
	Even if input is provided through other means than an external field, this energy contribution can often still be used by considering an effective field instead.
	
	\item The \textit{exchange coupling energy} between nearest neighbours (NN) $i$ and $j$
	\begin{equation}
		E_{\mathrm{exch},i,j} = J \frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\mu_i \mu_j} \mathrm{,} \label{eq:E_exch}
	\end{equation}
	with $J$ the exchange coupling constant, which is constant throughout the ASI. \par
	This interaction is rarely present in ASI, but can for example be relevant in interconnected ASI --- whether by design or due to limited lithographic accuracy.
	We will encounter an example of the latter in \cref{sec:3:OOP:MFM}.
\end{enumerate}

The combined \textit{interaction energy} $E_i$ of a single magnet $i$ with its environment is then given by
\begin{equation}
	E_i = E_{\mathrm{Z},i} + \sum_j E_{\mathrm{MS},i,j} + \sum_{j \in \mathcal{N}_i} E_{\mathrm{exch},i,j} \mathrm{,}
	\label{eq:E}
\end{equation}
where $\mathcal{N}_i$ is the collection of nearest neighbours of magnet $i$.
Which magnets are included in this collection depends on the ASI lattice and which site of the unit cell magnet $i$ is in, and can be defined separately for each ASI lattice. \\\par
Note that all terms in \cref{eq:E} simply change sign when magnet $i$ switches ($\vc{\mu}_i \rightarrow -\vc{\mu}_i$).
The magnetostatic self-energy is not present in \cref{eq:E} because it only contributes a constant offset and does not change when the magnet switches.
As such, $E_i$ represents the total interaction energy of a magnet with its `neighbours',\footnote{
	In this context, a `neighbour' of a magnet can be interpreted more broadly as all magnets it interacts with through a particular energy contribution. For example, the magnetostatic interaction considers all magnets to be `neigbours', unless the user has explicitly set a maximum interaction distance.
} and therefore the change in energy of the whole ASI when magnet $i$ switches is simply $\Delta E_{i,1\rightarrow2} = -2 E_i$.
This is called the \textit{switching energy}, and we will later see (\cref{sec:2:Dynamics}) that it plays a central role in the algorithms used for simulating system dynamics.
It is therefore very advantageous to have such a simple method of calculating the switching energy. \par
When the simulation is initialized, the energy contributions are calculated for all magnets.
Each magnet therefore stores a value in memory for each of the terms in \cref{eq:E}.
Whenever a magnet switches, the energies of its `neighbours' are updated appropriately, which constitutes a major part of the calculation effort required for every step in the simulation.

\subsection{Finite-size corrections to the magnetostatic energy}\label{sec:2:finite}
\cref{eq:E_MS,eq:E_Z,eq:E_exch} approximate each nanomagnet as a point dipole, but real nanomagnets have a finite spatial extent.
If one assumes a uniform magnetisation throughout each single-domain nanomagnet, then this finite size does not affect the Zeeman energy, nor the exchange energy which can capture any shape-related effects by an appropriate choice of the exchange coupling $J$.
The Zeeman energy could even account for a deviation from uniform magnetisation near the edge of a magnet, which occurs in reality but which we will neglect in the following, by an appropriate choice of $\vc{B}_\mathrm{ext}$. \par
The magnetostatic interaction, however, depends on the relative position, orientation, and shape of all magnets.
This may result in inadequate simulation of closely spaced ASI where the true magnetostatic coupling can be significantly stronger than predicted by a point dipole approximation.
Therefore, two (mutually exclusive) improvements have been implemented in \hotspice, which rescale the magnetostatic interaction energy between magnets.

\subsubsection{Second-order correction for dipoles}
\textit{Politi and Pini}~\cite{Dipolar2Dparticles} have presented a multipole expansion of the magnetostatic interaction, to account for the finite size of 2D nanomagnets (i.e., lateral dimensions $\gg$ thickness), assuming a uniform magnetization.
This results in a second-order correction
\begin{equation}
	E_{\mathrm{MS},i,j} = E_{\mathrm{MS},i,j}^\mathrm{(0)} + E_{\mathrm{MS},i,j}^\mathrm{(2)} \mathrm{,}
\end{equation}
where $E_{\mathrm{MS},i,j}^\mathrm{(0)}$ is the original point dipole magnetostatic interaction given by~\cref{eq:E_MS}. \par
The second-order correction can be written as
\begin{equation}
	E_{\mathrm{MS},i,j}^\mathrm{(2)} = \frac{\mu_0}{4\pi} \frac{3\mathcal{I}_{ij}}{2} \Bigg[3\frac{\vc{\mu}_i^\mathrm{OOP} \bcdot \vc{\mu}_j^\mathrm{OOP}}{\abs{\vc{r}_{ij}}^5} + \frac{\vc{\mu}_i^\mathrm{IP} \bcdot \vc{\mu}_j^\mathrm{IP}}{\abs{\vc{r}_{ij}}^5} -5\frac{(\vc{\mu}_i^\mathrm{IP} \bcdot \vc{r}_{ij}) (\vc{\mu}_j^\mathrm{IP} \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^7} \Bigg] \mathrm{,}
\end{equation}
where $\vc{\mu}_i$ was split into its IP and OOP components, conveniently leading to separate IP and OOP terms as implemented in the two types of ASI in \hotspice. \par
The particular shape of the nanomagnets is encapsulated in the single scalar $\mathcal{I}_{ij} = (\mathcal{I}_i + \mathcal{I}_j)/2$.
These $\mathcal{I}$ are calculated similar to a moment of inertia:
\begin{equation}
	\mathcal{I}_i = \int_{\Omega_i} \abs{\vc{r} - \ab(\int_{\Omega_i} \vc{r} d\vc{r})}^2 d\vc{r} \mathrm{,}
\end{equation}
with $\Omega_i$ the volume of magnet $i$.
We assume all magnets have the same shape, such that $\mathcal{I}_{ij} = \mathcal{I}_i = \mathcal{I}_j$.
Since nanomagnets in ASI are typically rather elliptical (IP ASI) or round (OOP ASI), \hotspice allows the user to set the semi-major axis $a$ and semi-minor axis $b$ of the magnets comprising an ASI.
For such a geometry --- elliptic cylinders --- these moments of inertia reduce to the simple expression $\mathcal{I}_{ij} = \frac{1}{4}(a^2 + b^2)$. % TODO REF: add examples of papers using round or elliptical magnets?
%While this correction can be applied to both IP and OOP magnetic dipoles, it is most effective for OOP systems, as can be seen in~\cref{fig:2:MS_distance}.

\subsubsection{Dumbbell model}\label{sec:2:Dumbbell}
Instead of representing a magnet as a point dipole, one may instead choose to represent it as a pair of magnetic monopoles~\cite{MagneticMonopoles2008,MagneticMonopoleDynamics}.
This introduces a new parameter $d$: the effective distance between the north and south poles of a magnet, with respective positions $\vc{r_N}_i = \vc{r}_i + s_i\frac{d_i}{2}\vc{u}_i$ and $\vc{r_S}_i = \vc{r}_i - s_i\frac{d_i}{2}\vc{u}_i$.
An appropriate choice of $d_i$ (slightly smaller than the physical length $l$ of the nanomagnet~\cite{DDG_Masterproef}) allows this dumbbell model to emulate the spatial extent of a real nanomagnet. \par
The north and south monopoles are assigned magnetic charges $+q_i$ and $-q_i$, respectively, with $q_i=\mu_i/d_i$~\cite{MagneticMonopoles2008}.
This choice yields the same effective dipole moment at long distance.
The interaction energy between two magnetic charges $q$ and $q'$ can be derived from the magnetic version of Coulomb's law~\cite{ForceMagneticDipole} as
\begin{equation}
	E = -\int_\infty^{\vc{r}} \frac{\mu_0}{\num{4}\pi}\frac{qq'}{\abs{\vc{r}}^3} \hat{\vc{r}} \cdot d\vc{r} = \frac{\mu_0}{\num{4}\pi} \frac{qq'}{\abs{\vc{r}}} \mathrm{.}
\end{equation}
The magnetostatic interaction energy between two nanomagnets is then the sum of their four mutual monopole interactions, finally resulting in
\begin{equation}
	E_{\mathrm{MS},i,j} = \frac{\mu_0 \mu_i \mu_j}{4\pi d_i d_j} \Bigg(\frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_N}_j}} + \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_S}_j}}\\ - \frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_S}_j}} - \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_N}_j}}\Bigg) \mathrm{.} \label{eq:2:E_MS_mono}
\end{equation}
The minus sign in the equation appears because north and south poles have opposite charge.
\hotspice is limited to a single value of $d$ for all magnets, due to the structure of the kernels used to calculate the magnetostatic interaction, which will be discussed in~\cref{sec:2:Kernels}.

\subsubsection{Comparison}
To assess whether these two corrections constitute an improvement to the resulting magnetostatic energy, we must quantify their impact by comparing against a known solution.
For this, we use the micromagnetic simulation package \mumax~\cite{mumax3}, which can determine the interaction energy for a given arrangement of ferromagnetic material.
While this solution is not exact, as \mumax uses a finite difference (FD) discretisation, the result will approach the true value for sufficiently small FD cell sizes. \\\par

\cref{fig:2:MS_distance} compares the original point dipole approximation and the two corrections (``finite dipole'' and ``dumbbell'') against the solution obtained with \mumax.
The figure shows 3 typical arrangements of neighbouring magnets in an ASI: two circular OOP magnets and two elliptical IP neighbours aligned along their easy and hard axes.
The magnetostatic interaction energy between the pair of magnets, divided by $\mu^2$ to be independent of magnet volume, is shown as a function of their normalized centre-to-centre distance.
A uniform magnetisation was used in the \mumax simulation, as this is the assumption under which the corrections were derived and because the lowest energy magnetisation state is size-dependent while the figure uses dimensionless units. \\\par

\xfig[1.0]{2_Hotspice/MS_distance.pdf}{
	Magnitude of the magnetostatic interaction between two magnets as a function of their normalized center-to-center distance, for the three \hotspice{} calculation methods (point dipole, second-order correction for dipoles, and dumbbell) compared to a micromagnetic \mumax{} calculation.
	OOP magnets are assumed to be circular with diameter $2r$, IP magnets are ellipses with length $l$ and width $w=4l/11$.
	Positions of north and south monopoles used in the dumbbell model are shown as red {\color{red}$\bullet$} and blue {\color{blue}$\bullet$} dots and are a distance $d=0.9l$ apart within a magnet.
	\label{fig:2:MS_distance}
}

For out-of-plane (OOP) systems, the dumbbell model is inadequate due to the small fringe fields and the limited thickness of the magnets.
Instead, the second-order dipole correction is more appropriate, yielding a significant improvement towards the ideal \mumax curve.
Still, a discrepancy remains for separations below $r_{ij}/2r \lessapprox 1.5$, which could be reduced by even higher-order corrections. \\\par

For IP systems, the dumbbell model constitutes a vast improvement over the standard point dipole treatment.
The dumbbell model does, however, require an additional parameter $d$, which affects the interaction energy.
For the best correspondence with \mumax, the monopole-monopole distance $d$ should be set slightly shorter than the length $l$ of a magnet, typically around $d/l\approx0.9$.
This adjustment accounts for the curvature and corresponding non-uniform magnetisation at the ends of real nanomagnets.
Similar values for $d/l$ were previously found in~\ccite{DDG_Masterproef} for typical nanomagnet shapes like ellipses and stadiums. \par
In contrast, the second-order dipole correction has little effect in IP systems and can even increase the discrepancy with \mumax. It emulates increased spatial extent and therefore always increases the interaction, but for magnets neighbouring along their hard axes (rightmost panel in~\cref{fig:2:MS_distance}) the point dipole model already overestimates the interaction. \par
In conclusion, the dumbbell model is preferred for IP systems, while the second-order dipole correction is most suitable for OOP systems.

\subsection{Effective energy barrier}\label{sec:2:E_B_eff}
\subsubsection{Intrinsic barrier from shape anisotropy}
\subsubsection{Mean-barrier approximation} % and relevant choices with discontinuities
\subsubsection{Asymmetric barrier}
\subsubsection{Exact solution} % OOP only

\section{Dynamics}\label{sec:2:Dynamics}
A Monte Carlo simulator would not be complete without an algorithm to change the state of the system.
\subsection{N\'eel relaxation: temporal evolution}
\subsection{Metropolis-Hastings: sampling equilibrium states}
% TODO: decide whether to talk about nomenclature details etc. here, or in the introduction.
\subsection{Other Monte Carlo algorithms} % Not implemented, explain why Wolff could be useful but why it's not in Hotspice

\section{Implementation}\label{sec:2:Implementation}
Now that the physics underlying the simulator have been extensively discussed, we turn our attention to the software implementation. \par
First, we discuss why we chose to implement the ASI on an underlying rectilinear grid, and explain how the \textit{`kernel'} --- the lookup table for the magnetostatic interaction --- was implemented.
We then take a look at the performance of \hotspice and how it has improved since its initial version due to improvements to the kernel.
One particular performance-enhancing feature is examined in more detail: the possibility to select multiple magnets at once in the Metropolis-Hastings algorithm.
Finally, we briefly discuss the structure of the software package and the functionality included in the various submodules, and finish with a short discussion of things that could be improved.

\subsection{Grid}
\hotspice{} represents an ASI as a rectilinear grid of non-uniform unit cells, with magnets positioned at selected grid points.
This choice was made based on the trade-off between calculation efficiency and the freedom to place magnets arbitrarily.
We opted to prioritise efficiency and accept the geometrical restriction, as most ASI research focuses on periodic lattices.
All quantities are therefore stored in 2D matrices of size $L_x \times L_y$, upon which operations can be performed efficiently.
We will denote these matrices with bold upright capitalised symbols: e.g., $\vc{S}$ contains the states $s_i$, $\vc{E_\mathrm{MC}}$ the magnetostatic energies, $\vc{\upmu}$ the magnitudes of magnetic moments... \par % Q: other suggestions? I can not capitalise the magnitudes of magnetic moments, because that would just be capital M which is too reminiscent of the magnetisation (in H = B/mu0 - M). So I just put it upright. Is that sufficient?
Despite the seemingly restrictive nature of the rectilinear grid,~\cref{fig:2:ASIs} illustrates its versatility in forming various periodic lattices, with only the Cairo lattices requiring grid non-uniformity.
As a bonus, real-time visualisation is simple and efficient using this approach, as the underlying matrix can directly be cast to a pixel image. \par
By leveraging the unit cell concept in periodic lattices and the efficient indexation of a rectilinear grid in computer memory, several aspects of the calculation can be performed more efficiently than for free-form ASI.
%The unit cell of each lattice in~\cref{fig:2:ASIs} is depicted as a grey rectangle.
Although non-rectilinear unit cells with fewer magnets can be identified for some lattices, such unit cells would increase complexity without significant benefit: the amount of sites in a unit cell only determines the required memory, not the performance of the simulation.

\subsection{Kernels for magnetostatic interaction}\label{sec:2:Kernels}
Pre-calculated ``kernels'' are used to efficiently update the magnetostatic interaction after each switch. 
The need to calculate these kernels beforehand increases the initialisation time, but the reduced runtime when simulating dynamics more than makes up for this.

\subsubsection{Magnetostatic interaction kernel}
For each magnet $i$, a kernel $\vc{k}^{(i)}$ stores the magnitude of the magnetostatic interaction between itself and all other magnets.
By calculating these values beforehand, when the ASI is created, the magnetostatic interaction energy between magnets $i$ and $j$ can readily be calculated as
\begin{equation}
	E_{\mathrm{MS},i,j}= s_i s_j \vc{k}^{(i)}_j \mathrm{,}
\end{equation}
which can only change sign, since the states $s_i = \pm 1$ and $s_j = \pm 1$ are the only variables in the system. \par
Due to the fact that magnets are placed on a rectilinear grid, the kernel of magnet $i$ can also be written as an $L_x \times L_y$ matrix $\vc{K}^{(i)}_{ab}$ where $ab$ denotes a position on the ASI grid.
For example, $\vc{K}^{(i)}_{2,3}$ stores the interaction strength between magnet $i$ and the magnet at index\footnote{The indexation used throughout this section starts counting at 1, as is typical for matrix notation. This is not to be confused with indexation in source code, which starts at 0.} $(2,3)$ on the grid.
If no magnet was placed at index $(a,b)$ on the grid, then $\vc{K}^{(i)}_{ab} = 0, \forall i$. \\\par

However, storing such a kernel for all magnets $i$ would require storing $\order{N^2}$ values in memory.
By leveraging unit cells, this storage requirement can be reduced to $\order{N}$.
Each occupied grid point in the unit cell is assigned a unique index $q = 1,\dots,\widetilde{N}$, with $\widetilde{N}$ the number of magnets in a single unit cell.
Hence, each magnet in the ASI is associated with a specific value of $q$. \par
This way, all magnets with the same value of $q$ share the same layout of surrounding magnets\footnote{
	In some ASI, it is possible that different sites inside a single unit cell also experience the same surrounding layout.
	This can be used to further improve memory usage (but not performance) by only storing unique kernels.
	This was not implemented as this constitutes only a minor improvement; for the lattices in~\cref{fig:2:ASIs}, this would at best half the number of kernels.
}, apart from a different cut-off at the border in case of open boundary conditions.
Therefore, if two magnets occupy equivalent positions $q$ in the unit cell --- say, $i$ at index $(x,y)$ on the grid and $j$ at $(x+\Delta x, y+\Delta y)$ --- then
\begin{equation}
	\label{eq:2:Kernel_equivalence}
	\vc{K}^{(i)}_{ab} = \vc{K}^{(j)}_{a+\Delta x,b+\Delta y} \quad \mathrm{,} \quad \forall a,b:
	\begin{cases}
		-\Delta x < a \leq L_x - \Delta x \\
		-\Delta y < b \leq L_y - \Delta y \\
		% Or more rigorously, but even more overcomplicated:
		% \max(-\Delta x, 0) \leq a < L_x - \min(\Delta x, 0) \\
		% \max(-\Delta y, 0) \leq b < L_y - \min(\Delta y, 0) \\
	\end{cases} \quad\mathrm{.}
\end{equation}
In other words, their kernels are identical apart from an offset by $(\Delta x, \Delta y)$, at least in the area that remains inside the kernel after this offset.
Due to this equivalence, and the fact that $- L_x < \Delta x < L_x$ and $- L_y < \Delta y < L_y$, all possible interactions that a magnet at site $q$ can experience can be stored in a single $(2L_x-1) \times (2L_y-1)$ matrix $\vc{\mathcal{K}}^{(q)}_{ab}$, a ``unit cell kernel''.

\xfig[1.0]{2_Hotspice/Kernel_IP_Pinwheel.pdf}{
	\label{fig:2:Kernel_IP_Pinwheel}
	Two examples showing the kernel shifting with respect to the ASI grid for multiplication (convolution), when a magnet switches.
	The ASI itself is shown on the left.
	Thin dotted lines indicate individual cells in the simulation grid while thick solid lines delineate the boundaries of unit cells.
	Letters indicate the index of a magnet within its unit cell.
	Each site in the unit cell corresponds to a kernel, shown in the right.
	The values stored in the kernel are designed such that shifting the switching magnet to the centre of the kernel and multiplying overlapping cells ($s_{ab}$ of the ASI with $\vc{K}_{ab}$ of the kernel) yields the magnetostatic interaction energy between the switching magnet and all other magnets at positions $ab$.
	The figure shows this for two example magnets indicated in \textcolor{lightblue}{blue} and \textcolor{lightred}{red}.
}

\cref{fig:2:Kernel_IP_Pinwheel} shows the structure of such unit cell kernels $\vc{\mathcal{K}}$ to clarify the interpretation of how they store the magnetostatic interaction strength between a particular magnet and any other magnet in the system.
This is most easily illustrated by an example.
Consider the blue magnet in the $5 \times 5$ pinwheel lattice on the left side of the figure, which occupies position $q=\mathrm{A}$ in the unit cell.
The corresponding $9 \times 9$ kernel $\vc{\mathcal{K}}^\mathrm{(A)}$ is shown schematically, also in blue.
It stores the magnetostatic interactions in such a way that, when the ASI grid is shifted onto the kernel to put the blue magnet at the centre of the kernel, the interaction strength of the blue magnet with any other magnet in the ASI is stored in the kernel elements where those other magnets end up.
So, in the figure, these are the elements in kernel A shown to contain a grey magnet\footnote{
	The exact values stored at these elements are omitted as they are irrelevant to illustrate the concept. See \cref{fig:2:Kernel_PBC} for an example of the particular values stored in kernel A for a larger $16 \times 16$ lattice.
}. \par
The concept is illustrated a second time, now for the red magnet in the pinwheel ASI, which instead occupies position $q=\mathrm{B}$ in the unit cell and therefore uses a different kernel; the red kernel $\vc{\mathcal{K}}^\mathrm{(B)}$. \\\par

This construction can be used to efficiently update the magnetostatic energy $E_\mathrm{MC}$ of all magnets whenever a magnet switches.
Recall that the grid stores the state $s_i$ of all magnets in a matrix $\vc{S}$ and the size of their magnetic moment $\mu_i$ in $\vc{\upmu}$.
When a magnet at unit cell index $q$ switches, the change of magnetostatic energy for all other magnets can be calculated by shifting $\vc{S}$ in the same way as the ASI was shifted in \cref{fig:2:Kernel_IP_Pinwheel}, and performing a pointwise multiplication (Hadamard product, denoted by $\odot$) with the respective elements of the kernel $\vc{\mathcal{K}}^{(q)}$.
Denoting the grid index of the switching $q$-site magnet as $(x,y)$ and using the states $\vc{S}$ after the switch, this yields a new matrix
\begin{equation}
	\label{eq:2:Kernel_update}
	\Delta \vc{E_\mathrm{MC}} = 2\mu_{xy} s_{xy}\vc{\upmu} \odot \vc{S} \odot \ab(\vc{\mathcal{K}}^{(q)}_{ab})_{\substack{L_x < a + x \leq 2L_x \\ L_y < a + y \leq 2L_y}} \quad\mathrm{,}
\end{equation}
containing the change in magnetostatic energy $\Delta E_\mathrm{MC}$ for each grid point, which can then simply be added to the current values of $E_\mathrm{MC}$. \par
The kernel is also used to initialise the magnetostatic energy $\left. E_\mathrm{MS} \right|_{t=0}$ of each magnet at the start of the simulation.
This is done by performing a convolution ($*$) rather than a pointwise multiplication:
\begin{equation}
	\label{eq:2:Kernel_init}
	\left. \vc{E_\mathrm{MC}} \right|_{t=0} = \sum_q \vc{Q_q} \odot \vc{\upmu} \odot \vc{S} \odot \ab((\vc{\upmu} \odot \vc{S}) * \vc{\mathcal{K}}^{(q)}) \mathrm{,}
\end{equation}
where only the central $L_x \times L_y$ area of the convolution is calculated and the matrix $\vc{Q_q}$ contains 1 at sites with unit cell index $q$, otherwise 0. \\\par

The exact values stored in the kernel depend on whether any finite-size corrections to the magnetostatic interaction (\cref{sec:2:finite}) were used and whether the ASI is of the IP or OOP type. For example, an IP kernel using the point dipole approximation contains elements
\begin{equation}
	\label{eq:2:Kernel_detailed}
	\vc{\mathcal{K}}^{(q)}_{ab} = \frac{\mu_0}{4 \pi} \frac{
		u_x^{(q)} u_x^{(ab)} \ab(1 - 3 \ab(\Delta x)^2) + u_y^{(q)} u_y^{(ab)} \ab(1 - 3 \ab(\Delta y)^2) - 3\Delta x \Delta y \ab(u_x^{(q)} u_y^{(ab)} + u_x^{(ab)} u_y^{(q)})
	}{
		\sqrt{\ab(\ab(\Delta x)^2 + \ab(\Delta y)^2)^5}
	} \mathrm{,}
\end{equation}
with $\Delta x$ and $\Delta y$ the x- and y-distance between the central magnet $(q)$ and a magnet at position $(ab)$ in the kernel (if a magnet exists at the corresponding point in the ASI grid) and $\vc{u}^{(i)}$ a unit vector along the easy axis of magnet $i$. \par
With this form for the kernel, the magnetostatic interaction between a magnet $i$ at grid-index $(v,w)$ and another magnet $j$ at index $(v+x, w+y)$ can be calculated as
\begin{equation}
	\label{eq:2:Kernel_unitcell}
	E_{\mathrm{MS},i,j} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(q_i)}_{L_x+x, L_y+y} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(q_j)}_{L_x-x, L_y-y} \quad \mathrm{,}
\end{equation}
if the indexation of $\vc{\mathcal{K}}^{(q)}$ starts at $(1,1)$.
Note that $\mu_i$ and $\mu_j$ were not included in the unitcell-kernel to allow magnets in different unit cells but with the same unitcell-index $q$ to have a different magnetic moment.
This versatility comes at the cost of two additional multiplications per interaction, which has a non-negligible performance impact as \cref{eq:2:Kernel_unitcell} is by far the most common operation performed during a meaningful simulation. \\\par

\subsubsection{Periodic boundary conditions}
The rectilinear grid enables the straightforward implementation of first-order periodic boundary conditions (PBC), which account for the eight nearest replicas of the ASI --- two horizontally, two vertically and four diagonally.
An open-boundary kernel $\vc{\mathcal{K}}^{(q)}$ can be transformed into a PBC kernel by just adding eight offset copies of the kernel to itself.
Specifically, four copies are shifted along the axes by $\pm L_x$ or $\pm L_y$, while the remaining four are shifted diagonally by $(\pm L_x, \pm L_y)$.
This is illustrated in~\cref{fig:2:Kernel_PBC} for an example of pinwheel ASI. \par

\xfig[1.0]{2_Hotspice/Kernel_PBC.pdf}{
	\label{fig:2:Kernel_PBC}
	Comparison of the magnetostatic interaction kernel for open (left) and periodic (right) boundary conditions, for a $16 \times 16$ pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}) using the point dipole approximation.
	The magnet of the unit cell site associated to this particular kernel is shown in the center with a black outline.
	The color of other magnets indicates the magnitude of the magnetostatic interaction between the central and the colored magnets if they both point `up'. This alignment is preferable for \textcolor{blue}{blue} magnets (low energy) and unstable for \textcolor{red}{red} magnets (high energy).
}

This construction works because elements near the edge of a kernel represent interactions between distant magnets on opposite sides of the ASI, which are the interactions most affected by PBC.
Since the middle region of the open-boundary kernel already stores the interaction between nearby magnets, PBC can be applied by re-using these central values and moving the kernel by $\pm L_x$ and/or $\pm L_y$. \\\par

This approach has the advantage that PBC do not impact performance, as they are baked into the kernel, but is limited to ASI consisting of an integer number of unit cells as systems with truncated unit cells along an edge can not properly be tiled.
Higher-order PBC --- beyond only the 8 nearest copies --- can not be calculated based on an open-boundary kernel since it does not include interactions with such distant magnets.
Due to the rapid $1/r^3$ decline of the magnetostatic interaction over distance, the use for such higher-order PBC would be very limited, and they were therefore not implemented.
% For very small systems, higher-order PBC can be relevant, but it is recommended to instead increase the system size to increase the fidelity of the Monte Carlo simulation.
% To explain the usage of the magnetostatic kernel, we can possibly refer to ``Real-space observation of emergent magnetic monopoles and associated Dirac strings in artificial kagome spin ice'', which notes that the energy is just a convolution of the interaction potential with the lattice sites at which there are magnets (which, for the Ewald summation, means that in Fourier space it is just a multiplication of the Fourier-transformed versions of those functions), which is pretty much what we are doing during multi-switching.

\subsubsection{Perpendicular magnetostatic kernel} % If not yet explained in detail earlier, talk about the perpendicular kernel etc. here
If the simulation accounts for the asymmetric energy barrier, as detailed in \cref{sec:2:E_B_eff}, then two additional kernels are needed for each site in the unit cell. \par
The first is calculated for a situation where the central magnet maintains its usual orientation while all other magnets are rotated counter-clockwise by \ang{90}.
This kernel will be used whenever a magnet switches, to update the perpendicular magnetostatic energy $\left. E_{\mathrm{MC}} \right|_{\perp}$ of all other magnets.
The underlying equation is very similar to \cref{eq:2:Kernel_detailed}, but with $u_x^{(q)} \rightarrow -u_y^{(q)}$ and $u_y^{(q)} \rightarrow u_x^{(q)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(q)}$ in \cref{eq:2:Kernel_update}. \par
The second additional kernel is the opposite of the first one; the central magnet is rotated \ang{90} counter-clockwise while all other magnets maintain their usual orientation.
This kernel is needed only once, to calculate the initial value of $\left. E_{\mathrm{MC}} \right|_{\perp}$ for all magnets.
The underlying equation for this one is also very similar to \cref{eq:2:Kernel_detailed}, but now with $u_x^{(ab)} \rightarrow -u_y^{(ab)}$ and $u_y^{(ab)} \rightarrow u_x^{(ab)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(q)}$ in \cref{eq:2:Kernel_init}.

\subsubsection{Numerical error with cut-off kernel} % TODO: expand + FIGURE
A performance improvement presents itself by cutting off the magnetostatic kernel at a certain distance, though only in the case of open BC.
Due to the underlying grid, the most natural way of achieving this is to simply reduce the size of the kernel from $2L-1 \times 2L-1$ to a smaller $2K-1 \times 2K-1$ central region. \par % Note that the size has to remain odd for the convolution to still place the magnet in the center.
However, this will result in inaccurate interaction energies being stored.
When multiple magnets switch, this error will accumulate.
Luckily, the error introduced in this manner is bounded from above: whenever a magnet switches back, the error its original switch introduced is cancelled (barring some remaining floating-point error).

\subsection{Multi-switching in Metropolis-Hastings} \label{sec:2:MultiSwitch}
\subsubsection{Minimal distance between sampled magnets} % Derive equation
\subsubsection{Selection algorithms}
\paragraph{Poisson disc}
\paragraph{Grid-select}
\paragraph{Hybrid grid-poisson}

\subsection{Performance}
\subsubsection{History}
The performance of \hotspice has been improved throughout development in various ways.
Calculation on the GPU was made possible early on and the efficiency of updating the magnetostatic interaction was improved drastically.
The number of magnets in the ASI has a major impact on performance, and often forms the determining factor as to whether calculation on the GPU rather than CPU will result in a faster simulation. \par
We start with a summarizing chronological table of these various improvements, and explain each step in more detail in the following paragraphs.
Two tables are shown:~\cref{tab:2:perf_init} for the initialisation time, and~\cref{tab:2:perf_switch} for the time it takes to perform 5000 switches using the N\'eel update algorithm, taking into account the magnetostatic interaction between all magnets.
During initialisation, the magnetostatic kernel is constructed, after which the starting energy $E_i$ of all magnets is calculated.
In the tables, only the last 3 rows use the kernel as it was explained in~\cref{sec:2:Kernels}; the other rows use less efficient kernels as explained below. \par % Can't really call it a "lookup table" if it is being convolved etc., right?
This test was benchmarked on an NVIDIA GeForce RTX 3080 Mobile GPU and an 11th Gen Intel\textregistered{} Core\texttrademark{} i7-11800H @ 2.30GHz.
Simulation times displayed serve an illustrative purpose: absolute values on other hardware may vary significantly, though the general trends within the tables should remain similar.

% Q: should we use L or N=L²? I would prefer L because at some point we talk about 2D indexation, so L makes it clearer that we have a 2D system.
\xtable[tab:2:perf_init]{\textbf{Initialisation time} for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 0.5s & 6.8s & 34.0s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 2.2s & 5.4s & 11.9s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 1.0s & 3.4s & 7.6s & 14.5s & 91.6s & ? \\
		\hline
		GPU | \makecell{Unitcell-kernel (see \S\ref{sec:2:Kernels})} & 2.4s & 4.6s & 9.7s & 16.3s & 66.2s & 730.4s \\
		\hline \hline
		GPU | \makecell{Final version\\(convolution initialization)} & 1.0s & 1.0s & 1.0s & 1.0s & 1.6s & 23.9s \\
		\hline
		CPU | \makecell{Final version\\(convolution initialization)} & 0.014s & 0.2s & 1.0s & 3.2s & 53.1s & ? \\
		\hline
	\end{tabular}
}

\xtable[tab:2:perf_switch]{\textbf{Time for 5000 switches} using the N\'eel algorithm, for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 9.4s & 217.6s & ? & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 5.0s & 13.6s & 51.8s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 7.1s & 7.2s & 8.2s & 9.0s & 13.0s & ? \\
		\hline
		GPU | \makecell{Unitcell-kernel (see \S\ref{sec:2:Kernels})} & 7.8s & 7.9s & 8.3s & 8.9s & 11.8s & 36.9s \\
		\hline \hline
		GPU | \makecell{Final version\\(2D indexation)} & 6.8s & 7.0s & 7.2s & 7.6s & 10.2s & 35.9s \\
		\hline
		CPU | \makecell{Final version\\(2D indexation)} & 0.56s & 1.3s & 2.5s & 4.3s & 41s & ? \\
		\hline
	\end{tabular}
}

\paragraph{From CPU to GPU}
Originally, \hotspice performed all calculations on the CPU. % TODO: explain CPU/GPU briefly? Or elsewhere?
This caused the simulation time to rise dramatically as the system size grew (e.g. first row in \cref{tab:2:perf_switch}).
For early versions of \hotspice with an unoptimized kernel, this made it impractical to simulate systems larger than $50 \times 50$ magnets.
By simply switching to GPU-based calculations using the \python{CuPy} library (second row), this upper limit was increased significantly.
The limiting factor instead became memory consumption of the unoptimized kernel. \par
Do note, however, that for small systems the GPU performs worse than the CPU.
This remains true independent of the kernel implementation.
GPUs are optimized for parallel processing, and therefore reach a bottleneck when the simulation takes on a more sequential nature~\cite{owens2008gpu}.
Simulating a single switch requires several distinct operations that do not lend themselves to parallelisation.
When operating on small arrays, this means relatively fewer parallel calculations can be performed, thereby increasing the importance of being able to perform distinct mathematical operations in quick succession.
The CPU excels at the latter. % TODO: references for GPU performance and operation

\paragraph{Kernel improvements}
The first two rows `\textit{Pairwise kernel}' in the tables use the na\"ive kernels $\vc{k}$ (or, equivalently, $\vc{K}$) described at the start of \cref{sec:2:Kernels}.
Their values were simply arranged in a big $N \times N$ matrix which stored the magnetostatic interaction between each pair of magnets.
The initialisation time on CPU (first row in~\cref{tab:2:perf_init}) bears witness to this, as $t_\mathrm{init} \propto L^4 = N^2$.
This is because calculating the initial magnetostatic interaction energy $E_\mathrm{MS}$ with this kernel structure requires iterating over all magnets and summing the interaction energy with each other magnet.
On GPU, parallelisation of this sum in each such iteration approximately reduces this to $\propto L^2 = N$. \par
Clearly, for large arrays, the need to store $\order{N^2}$ elements quickly becomes prohibitive.
For a system as small as $170 \times 170$, this type of kernel already exceeded the \SI{8}{\giga\byte} of available memory, as indicated in the table by ``Mem''.
For periodic lattices, the matrix will contain many identical values, wasting a lot of memory.
While it was possible to alleviate this by instead using a sparse matrix and limiting the interaction distance, memory usage was not the only issue: also the performance per switch (\cref{tab:2:perf_switch}) was rapidly declining for increasing $N$, both on CPU and GPU. % Also: sparse matrices probably have more indexation overhead
Clearly, a more efficient approach was required. \\\par

An extreme solution would be to use no kernel at all, as in the third row `\textit{No kernel}'.
Instead, for every switching magnet, its interaction energy with all other magnets was calculated from scratch.
The tables show that this clearly gets rid of the memory issue and yields a surprisingly fast simulation.
However, the initialisation for larger systems now takes a long time, because without a kernel this requires $\order{N^2}$ operations resulting in $t_\mathrm{init} \propto N=L^2$ on GPU for small systems.
Still, in all situations it is beneficial not to use the unoptimized kernel. \\\par

To reduce the initialisation time, two improvements had to be combined. \par
First, a kernel as described earlier in~\cref{sec:2:Kernels} had to be implemented, which overcomes the memory issue by using the periodic nature of most ASI to only store interactions for a single unit cell.
This is used in the fourth row `\textit{Unitcell-kernel}', but does not constitute a significant improvement on its own aside from enabling $1000 \times 1000$ systems for the first time. \par
Secondly, however, this new kernel enables the usage of a convolution to initialise the magnetostatic interaction energy $E_\mathrm{MS}$, rather than the sequential sum used before.
This is what the final two rows `\textit{Final version}' use, where we once again compare CPU and GPU as this is the final version.
Another minor improvement in the final version was to use 2D indexation (rather than a flat index $i = n_x y + x$ which did not synergize well with the convolution), yielding another \SI{15}{\percent} performance increase.

\paragraph{Final performance}
In the end, the upper limit for feasible calculation time has become $L \approx 1000$ on GPU and $L \approx 400$ on CPU.
Comparing the last two rows of \cref{tab:2:perf_switch} reveals that GPU outperforms CPU for $L \gtrapprox 300$.
However, this is for the N\'eel algorithm which only performs a single switch at a time.
Performing multiple switches simultaneously in Metropolis-Hastings benefits greatly from the unitcell-kernel, as it can use the same principle of convolution as was already used in the initialisation.
With this algorithm, the GPU can maintain the advantage for systems as small as $L \gtrapprox 60$.

\subsubsection{Multi-switching}
The N\'eel algorithm, used in the preceding discussion about performance, does not implement a multi-switching procedure.
This makes it very inefficient for large systems: for a system of $N$ magnets to change its macrostate significantly, on the order of $N$ switches must occur.
Even though the time required to perform a single switch with the N\'eel algorithm stays roughly constant up to $L<400$, the time per Monte Carlo sweep\footnote{
	A Monte Carlo ``step'' refers to attempting to switch a single magnet. By a ``Monte Carlo sweep'' (MCS), we refer to $N$ attempted switches when the simulation contains $N$ magnets~\cite{NumericalDynamicalNiedermayer}. Note that the distinction between attempted switches and actual switches only exists in the Metropolis-Hastings algorithm. % TODO END: Could also decide to put this in a \begin{definition} environment somewhere earlier.
} will scale linearly with the number of magnets if each iteration only switches a single magnet.
The ability to perform multiple switches simultaneously in the Metropolis-Hastings algorithm (described earlier in \cref{sec:2:MultiSwitch}) alleviates this problem, allowing for a significantly higher amount of Monte Carlo sweeps per second. \\\par
\cref{fig:2:Performance} shows the performance of the multi-switching algorithm as a function of system size, both for calculation on the CPU (a) and GPU (b).
The multi-switching algorithm selects a certain number of magnets per second, as depicted by the blue curve: these are candidates for switching.
Dividing this value by $N$, the number of magnets in the system, gives the number of Monte Carlo sweeps per second, depicted by the black curve.
This is the main performance metric in Monte Carlo simulations. \par
Only a subset of the selected magnets will switch, as depicted by the red curve.
This switching rate is highly dependent on the specific conditions of the simulation --- particularly the ratio $\EBtilde/T$ --- making it infeasible to make general statements.
There exist situations where no magnets switch or where any selected magnet switches.
In the figure, values of $a$, $T$, and $\EB$ were chosen which result in a reasonable switching rate.

\xfigsnocap[0.47]{2_Hotspice/Performance_CPU.pdf}{2_Hotspice/Performance_GPU.pdf}{
	Performance of OOP square ASI as a function of system size, on \textbf{(a)} CPU and \textbf{(b)} GPU. The Metropolis-Hastings algorithm was used with multi-switching $Q=0.05$, lattice spacing $a = \SI{1}{\micro\metre}$, temperature $T = \SI{100}{\kelvin}$ and energy barrier $\EB = 0$.
	\label{fig:2:Performance}
} % Q: Performance for IP ASI might be worse. Add a Pinwheel figure as well?

There is once again a stark contrast between CPU and GPU. \par
On CPU, the sampling rate starts off rather constant for small systems, as overhead dominates.
For $N>100$ magnets, the sampling rate increases proportional to $N$, leading to a stable performance (MCS/s) independent of $N$.
However, performance drops dramatically once the system size increases beyond $\approx 80 \times 80$. % While this is reminiscent of a lack of garbage collection while building the figure, this is not the case here as the drop remains even when starting the figure at $L=80$.
We hypothesize that this is due to exceeding a CPU cache size. \par
%On the CPU used for~\cref{fig:2:Performance} (the aforementioned i7-11800H), the L1 and L2 cache hold \SI{48}{\kilo\byte} and \SI{1.25}{\mega\byte}, respectively.
% Q: Are any of the below hypotheses reasonable? Which is most reasonable?
% Hypothesis 1: The size occupied by a 72x72 ASI is 1.25MB. It is not unreasonable that, soon after reaching this point, some arrays can no longer be stored on this cache level, impacting performance.
% Hypothesis 2: The size of mm.m at 78x78 is 48kB, so we might be exceeding the L1 cache beyond this point.
% Hypothesis 3: The size of the dipole kernel at this point is around 220kB. Doesn't seem to match up with anything.
% TODO: Revisit this at some point, it is odd that on another CPU (Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz) the cut-off happens at the same system size, indicating that it is not cache-size-related because that CPU has 256kB L2 and 32kB L1, both different from the UGent laptop.
The GPU, on the other hand, hits its stride for exactly those systems that are too large for the CPU to handle.
Around $70 \times 70$, the sampling rate increases rapidly, resulting in an increased number of MC sweeps per second (MCS/s) that reach a local maximum for $200 \times 200$ magnets.
Beyond this, the sampling rate stagnates, eventually settling at around \SI{150e3}{samples\per\second}, corresponding to an ever decreasing MCS/s as the parallelism of the GPU gets exhausted.

\subsubsection{RNG}
Another factor which may contribute to the performance is the Random Number Generator (RNG).
Both update algorithms rely on generating a large amount of random numbers to select the next magnet(s) to switch. % TODO: talk a bit about XORWOW etc. and how it actually doesn't really impact performance

\subsection{Package structure} % Structure of Hotspice, explain all modules (core, ASI, energies, (config? with GPU)...)
Hotspice is written as a Python 3.10 package and can perform simulations on either the CPU or GPU.
The optimal hardware choice depends on the size of the ASI and the update scheme used.
By default, \hotspice runs on the CPU using the popular NumPy and SciPy libraries.
For GPU-accelerated array manipulation, the CuPy v11.4~\cite{CuPy} library is used, but this is an optional dependency. \par
\paragraph{\python{hotspice.config}: GPU/CPU choice}
By default, \hotspice runs on the CPU. To run on the GPU, the environment variable \python{HOTSPICE_USE_GPU} must be set to \python{"true"} before the the \hotspice package is loaded in a Python script with \python{import hotspice}. It is not possible to switch between GPU/CPU within a script.
\subsubsection{ASI}
\paragraph{\python{hotspice.ASI}: predefined ASI lattices}
This module provides two abstract classes from which all ASIs should inherit: \python{hotspice.ASI.IP_ASI} or \python{hotspice.ASI.OOP_ASI}.
Various lattices are available, as previously shown in~\cref{fig:2:ASIs}.
These all follow the same pattern: \python{hotspice.ASI.<ASI_name>(a, n, **kwargs)}, with two positional arguments.
The first argument is the lattice parameter, as defined by the red indicator in \cref{fig:2:ASIs}.
The second argument is the size of the underlying grid (one can also specify \python{nx} and \python{ny} separately).
For the predefined lattices in the \python{hotspice.ASI} module, these two parameters are enough information to create a rudimentary ASI.
\paragraph{Energies}\label{sec:2:API_energies}
Three predefined energy contributions are provided in the \python{hotspice.energies} module, though they can be accessed from the main \python{hotspice} namespace because they are used so often.
The magnetostatic interaction is implemented by the \python{hotspice.DipolarEnergy} or \python{hotspice.DiMonopolarEnergy} classes, with the latter using the monopole approximation to calculate the interaction.
By default, an ASI object takes only the \python{hotspice.DipolarEnergy} into account.
When relevant, the user has to explicitly add a \python{ZeemanEnergy} or \python{ExchangeEnergy}.
It is possible to set longer-range exchange interactions (next-nearest neighbour etc.) by manually setting the \python{local_interaction} field of an \python{ExchangeEnergy} object.
% TODO: create a custom "API" environment that we can put near all the different parts of the model to show code outside the main text?
\subsubsection{RC} % io and experiments modules
\subsubsection{Utilities} % Explain utils module (but only those functions/classes relevant to users) and perhaps mention the existence of plottools
\paragraph{GUI}
A graphical user interface (GUI) is available for \hotspice, which allows the user to directly interact with the ASI and observe changes in realtime. It can be run by calling \python{hotspice.gui.show(mm)}, with \python{mm} the ASI object, resulting in the window shown in~\cref{fig:2:GUI}.

\xfig[1.0]{2_Hotspice/GUI.png}{
	The \hotspice graphical user interface.
	This example shows a spatially averaged view of an $80 \times 50$ pinwheel ASI, \SI{2}{\micro\second} after it was initialised in a random state.
	\label{fig:2:GUI}
}

The state of the ASI is prominently displayed, and the bottom left panel shows a few useful statistics such as the elapsed time. The ASI display is controlled by the central bottom panel, which changes between 4 display modes.
By default, the magnetisation is shown as an averaged field, with the averaging method determined by the ASI lattice.
The second mode displays individual arrows, which either show each magnet's magnetisation direction or the effective field $\vc{H}_\mathrm{eff}$ it experiences. % Q: is it H? I wrote H because it does not add the magnetic moment. On a similar note, should we write B_ext or H_ext in the definition of the Zeeman energy?
In the third mode, the energy contributions ($E_\mathrm{MC}$, $E_\mathrm{Z}$, $E_\mathrm{Exch}$ and their sum $E_i$) and the resulting effective energy barrier $\EBtilde$ for each magnet can be shown, both along the easy and hard axis.
The last option is to show the spatial distribution (i.e., the value for each magnet) of some parameters like the temperature $T$, shape anisotropy $\EB$ and size of the magnetic moment $\mu$. \par
Buttons in the sidebar on the right allow the user to interact with the ASI by progressing through time, setting an initial state, or --- for more complex simulations --- apply custom functions.
By clicking with the mouse on the ASI plot, it is also possible to interact with the ASI at a granular level, switching one or multiple magnets. Finally, the red box in the bottom right controls the update algorithm.

\subsection{Advantages and disadvantages of the \hotspice approach} % Hindsight is 20/20
\subsubsection{Grid}
\subsubsection{Input/output RC}
\subsubsection{GPU/CPU}
In hindsight, for the purposes of this thesis it was not necessary to go through all the effort to port \hotspice to the GPU.
In~\cref{ch:Applications}, I rarely use large systems, and often use the N\'eel algorithm for which multi-switching was not implemented.
Therefore, the vast majority of calculations in that chapter will be performed on CPU. 
In hindsight, we mostly worked on small systems with the N\'eel algorithm (to simulate RC in OOP ASI), making all the effort of allowing \hotspice to run on the GPU unnecessary. Still, the effort was not wasted as it has expanded the envelope of applicability for \hotspice, particularly when using the Metropolis-Hastings algorithm.
The fact that the current implementation requires making the choice to use GPU or CPU before the \python{import hotspice} statement can be limiting in specific situations. A spiritual successor to \hotspice should address this limitation.

\section{Verification} % See paper
Now that we know the ins and outs of the model used by \hotspice, all that remains is to verify its correct implementation.
To this end, we simulate several systems of increasing complexity for which analytical solutions are available.
The examples discussed here are all equilibrium problems, so we will use the Metropolis-Hastings algorithm to verify that it indeed samples the equilibrium state space.
Special attention will be given to the issue of ``critical slowing down'' which plagues this algorithm near phase transitions.
\subsection{Hexagon}
Before progressing to equilibrium problems, we briefly check that the magnetostatic interaction and its underlying kernels all work as expected.
The magnetostatic interaction energy of any given arrangement of nanomagnets can be calculated analytically and compared to \hotspice. \par
As an example that is neither too simple nor complex, we consider a regular hexagon in a vortex state. The energy of all magnets should equal the following analytical solution, which is the sum of the magnetostatic interaction energy between nearest neighbours $\circled{1}$, next-nearest neighbours $\circled{2}$ and magnets on opposite sides of the hexagon $\circled{3}$:
\begin{align*}
	E_{\mathrm{MS},i} =&\, \circled{1} + 2 \times \circled{2} + 2 \times \circled{3} \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} -2\frac{\mu_0 m^2}{4\pi \frac{3 \sqrt{3} a^3}{8}} \Big[3\cos^2(\pi/3) - \cos(2\pi/3)\Big] -2\frac{\mu_0 m^2}{4\pi \frac{a^3}{8}} \Big[3\cos^2(\pi/6) - \cos(\pi/3)\Big] \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} \bigg[29+\frac{20}{3\sqrt{3}}\bigg] \mathrm{.}
\end{align*}
The following Python function verifies that, indeed, \hotspice gives the expected total energy $E_{\mathrm{MS},i}$ for all 6 magnets $i$ in the system.
\begin{lstlisting}
import numpy as np
import hotspice

def test_hexagon(l=470e-9, s=10e-9, m=1.1278401e-15):
	mm = hotspice.ASI.IP_Kagome(a := (l+2*s)/np.tan(30*np.pi/180), nx=5, ny=3, moment=m, pattern="vortex")
	E_sim = mm.get_energy('dipolar').E[mm.m.astype(bool)]
	E_exact = -1e-7*m*m/a**3*(29+20*np.sqrt(3)/9)
	print("OK" if np.allclose(E_sim, E_exact, atol=0) else "Fail")

test_hexagon() # OK
\end{lstlisting}

\subsection{Non-interacting spin ensemble}

When an external magnetic field of magnitude $B$ is applied to a non-interacting ensemble of Ising spins, it follows directly from the partition function that the average magnetization follows the relation
\begin{equation}
	\frac{\langle M \rangle}{M_0} = \tanh\ab(\frac{\mu B}{\kBT}) \mathrm{.}
\end{equation}
\cref{fig:2:Noninteracting} demonstrates that \hotspice{} correctly reproduces the expected result.

\sidefig{2_Hotspice/Verification/Noninteracting_IP.pdf}{
	\label{fig:2:Noninteracting}
	Average magnetisation of a non-interacting ensemble of spins, as a function of the applied magnetic field magnitude $B$.
	\newline\newline\newline
}

\subsection{Exchange-coupled Ising system}
The 2D square-lattice exchange-coupled Ising model is one of the few exactly solvable systems in statistical physics~\cite{ExactlySolvedModelsStatMech}.
An analytical solution is known for the temperature-dependence of its average magnetisation
\begin{equation}
	M = \sqrt[8]{1 - \sinh^{-4}(2J/\kBT)} \mathrm{,}
	\label{eq:2:Verification_exchange_M}
\end{equation}
with $J$ the exchange coupling constant~\cite{Correlations2DIsing,IsingSpontaneousMagnetization,coey2010magnetism}.
This implies that the system exhibits a second-order phase transition at the critical temperature~\cite{ExactlySolvedModelsStatMech}
\begin{equation}
	T_c = \frac{2J}{\kB \ln(1 + \sqrt{2})} \mathrm{.}
\end{equation}
Furthermore, an analytical solution is available for the nearest-neighbour correlation
\begin{equation}
	\langle \sigma_{1} \sigma_{2} \rangle = 
	\begin{cases}
		\sqrt{1+k} \ab[\frac{1-k}{\pi}K(k) + \frac{1}{2} \ab] &\text{for } T < T_c \mathrm{,} \\ 
		\sqrt{1+k} \ab[\frac{1-k}{\pi k}K(1/k) + \frac{1}{2} \ab] &\text{for } T > T_c \mathrm{.} \\ 
	\end{cases}
\end{equation}
with $K$ the complete elliptic integral of the first kind and $k=\sinh^{-2}(2J/\kBT)$~\cite{Correlations2DIsing}. \\\par

The result of a \hotspice simulation of this Ising system is shown in \cref{fig:2:Verification_exchange}, as calculated for an $800 \times 800$ lattice.
Maximal multi-sampling ($Q=+\infty$) for the Metropolis-Hastings algorithm was used to verify that --- at least for systems with such short-range coupling --- this sort of extreme multi-sampling still converges to the correct solution.
Since the theoretical curves for $M$ and $\langle \sigma_{1} \sigma_{2} \rangle$ are monotonically decreasing~\cite{MCinStatPhys}, the final state from the previous temperature step was retained as the starting point for the next step. This preserves progress already made towards equilibrium and avoids the disruption that resetting to the uniform state would cause. \par

\xfig{2_Hotspice/Verification/OOP_Exchange.pdf}{
	\label{fig:2:Verification_exchange}
	\textbf{(a)} Average magnetisation and \textbf{(b)} nearest-neighbour (NN) correlation as a function of temperature.
	Markers show the \hotspice result for an $800 \times 800$ OOP square-lattice exchange-coupled Ising system using the Metropolis-Hastings algorithm with maximal multi-switching.
	The simulation was performed for 3 values of $N$ --- the number of Monte Carlo steps per site performed at each temperature step.
	Discrepancies due to critical slowing down above $T_c$ improve with increasing $N$.
}

The \hotspice result corresponds well to the theoretical predictions both below $T_c$ and in the high-temperature limit.
However, just above $T_c$, the average magnetisation evolves only slowly towards the expected value.
Increasing the number of Monte Carlo sweeps per temperature step brings the system closer to the theoretical equilibrium and reduces the temperature range above $T_c$ where the system has not yet reached equilibrium again.
This slow convergence near the critical point is a symptom of the well-known phenomenon called ``critical slowing down''.
% TODO: why the discrepancy in the correlation?

\subsubsection{Critical slowing down}
Critical slowing down (CSD) originates from a divergence in the autocorrelation time $\tau$ near a critical point, causing subsequent Monte Carlo configurations to be highly correlated~\cite{NumericalDynamicalNiedermayer,CompStatPhys,StatisticalMechanicsAlgorithmsComputations}.
As a result, the system explores the phase space very slowly, particularly with single-spin flip algorithms like Metropolis-Hastings~\cite{StatisticalMechanicsAlgorithmsComputations}.
Although cluster algorithms like the Wolff algorithm~\cite{Wolff} can mitigate this effect, they are not intended for application beyond the 2D Ising system.
% TODO: extend this discussion
\subsection{Exchange- and magnetostatically-coupled Ising system}
\subsection{Square-to-pinwheel transition angle}\label{sec:2:Verification_IP_SquarePinwheel}
The in-plane square and pinwheel ASI lattices can be continuously transformed into each other by rotating each individual magnet by \ang{45}.
Despite being this closely related, their ground state magnetic ordering differs significantly.
Square ASI has an antiferromagnetic (AFM) ground state, where all vertices have a net zero magnetisation.
Meanwhile, pinwheel ASI exhibits superferromagnetic order, where all magnets with similarly-oriented easy axes are magnetized in the same direction~\cite{ApparentFMpinwheel}.
Therefore, a critical angle $\ang{0} < \alpha_c < \ang{45}$ must exist where the ground state transitions between these two extremes. \par
For the dipole model, theoretical calculations predict this transition at $\alpha_c = \arcsin(\sqrt{3}/3) = \ang{35.3}$~\cite{AFM-FM-transition-Pinwheel,MagicAngle}.
For the dumbbell model, the transition angle depends on the distance $d$ between monopoles, but is always larger than for the dipole model~\cite{AFM-FM-transition-Pinwheel}. \\\par
The result of a \hotspice simulation using both models is shown in \cref{fig:2:Pinwheel_angle}.
To quantify this transition, we measure the fraction of vertices with net zero magnetisation --- this value is 0 for superferromagnetic order while it is 1 for AFM order.
For the dipole model, the transition occurs at $\approx \ang{35}$ as expected, while the dumbbell model indeed transitions at a larger rotation angle.
% TODO: debate (with myself) whether or not to look for some more experimental references to include the following sentences.
% While this transition was experimentally observed to be gradual from \ang{35} to nearly \ang{45}, these experiments likely could not reach an equilibrium state due to defects, rapid quenching, or finite correlation times, all of which freeze domain walls in the system. Metropolis-Hastings samples the equilibrium state space, resulting in the sharp transition seen in the figure.

\xfig[0.6]{2_Hotspice/Verification/Pinwheel_angle.pdf}{
	Fraction of vertices with net zero magnetisation at equilibrium, as square ASI (left) transitions to pinwheel ASI (right) by rotating individual magnets.
	The theoretical transition angle $\alpha_c \approx \ang{35.3}$ for the dipole model is indicated by the vertical dotted line.
	\label{fig:2:Pinwheel_angle}
}

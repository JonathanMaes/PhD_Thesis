\chapter{Methods: ``\hotspice'' simulator for ASI}\label{ch:Hotspice}
% \glijbaantje{It's not a bug, it's a feature.}{Someone}

\begin{adjustwidth}{2em}{2em} % TODO END: update once published.
    \begin{center}
        \textbf{Material from this chapter has also been published in:} \\
    \end{center}
    \vspace{1em}
    \begin{adjustwidth}{0em}{1.5em}
	    \begin{itemize}
	    	\item[\cite{MAES-24}] J.~Maes, D.~De~Gusem, I.~Lateur, J.~Leliaert, A.~Kurenkov, and B.~Van Waeyenberge.
	    	\newblock The design, verification, and applications of Hotspice: a Monte Carlo simulator for artificial spin ice.
	    	\newblock \emph{ArXiV}, arXiv:\penalty0 2409.05580, 2024.
	    \end{itemize}
    \end{adjustwidth}
    \vspace{.5em}
    \begin{center}
        \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{.0em}
    \begin{center}
    	The \hotspice simulator discussed in this chapter\\
    	is open-source and available on \href{https://github.com/bvwaeyen/Hotspice}{GitHub}.
    \end{center}
    %\vspace{0em}
    \begin{center}
    \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{1em}
\end{adjustwidth}

To assess the potential of \link{reservoir computing}{Reservoir Computing} in (perpendicular-anisotropy) \link{artificial spin ice}{Artificial Spin Ice}, as is the main topic of this thesis, a simulation framework is needed to efficiently explore the impact of various system parameters and input methods on reservoir performance. \\\par

Nanomagnetic systems are often simulated using \indexlabel{micromagnetic simulations}micromagnetic codes, such as the finite-difference-based \mumax~\cite{mumax3} and \oommf~\cite{OOMMF} or the finite-element-based \nmag~\cite{Nmag}, which capture the magnetisation dynamics of individual nanomagnets in great detail. \par
However, the time between successive switches of a nanomagnet is not necessarily similar to the timescale of micromagnetics.
Furthermore, determining RC metrics requires applying many input cycles --- on the order of 100 for the task-agnostic metrics --- to get a statistically valid result~\cite{RC_TaskAgnosticMetrics_v2}.
In this timeframe, many switches occur.
When the simulated time extends beyond several microseconds, as is typically the case, simulating even a modest number of magnets --- on the order of several dozen --- becomes computationally unfeasible~\cite{leo2021chiral}. \\\par

To address these limitations, specialised ASI simulation tools have been developed.
An example of this is the flatspin simulator~\cite{flatspin}, which implements deterministic spin flipping via a \xlabel{Stoner-Wohlfarth} model~\cite{StonerWohlfarth2008}.
Using such higher-level approximations enables the study of collective behaviour in much larger systems and over far longer timescales than is feasible with micromagnetic simulations, though at the cost that the internal magnetisation structure of individual nanomagnets is no longer simulated in detail.
Additionally, Monte Carlo methods are often used to simulate spin ices, including ASI, but these are typically specialised to a select few lattice geometries and often only account for nearest-neighbour interactions, whose strength is often arbitrarily set or calculated separately using micromagnetic codes.~\cite{MeltingASI,sklenar2019field,gilbert2014emergent,zhang2013crystallites} \\\par % REFS: 'gilbert2014emergent': Monte Carlo sims based on a vertex model and interacting magnetic charges. 'zhang2013crystallites': uses monopoles and NN couplings to model kagome ASI with Metropolis and loop update. 'moller2006artificial': bit broader using dipolar interaction, though seemingly still only for nearest neighbors. 'mengotti2011kagome' use a full dipole model with Ewald summation for PBC. 'lou2023competing': dipole model for half-occupation IP Ising. 'sendetskyi2019continuous': dipole model for square ASI. 'EngineeringRelaxationComputation': KMC on dipole model, seemingly for square arrays. 'sklenar2019field': NN interactions calculated by mumax on quadrupole lattice. 'MeltingASI': 16-vertex ice model

Our goal was to blend these two approaches, resulting in \hotspice: a versatile Monte Carlo simulator meant to capture ASI physics with minimal parameters, allowing various lattice configurations to be evaluated.
This software approximates each single-domain nanomagnet as a single Ising spin, associating energies with the various ASI states by accounting for the \link{magnetostatic interaction}{magnetostatic (MS) interaction} between all magnets.
\hotspice supports both in-plane (IP) and out-of-plane (OOP) ASI, which may contain thousands of magnets. Simulations can span arbitrary timescales, as determined by the switching time of magnets in the system. \\\par

In this chapter, we discuss several model variants that have been implemented, and assess their accuracy in simulating the behaviour of ASI.
These variants differ in their choice of Monte Carlo spin-flip algorithm and their calculation of the magnetostatic interactions and energy barriers. \\\par

\section{Model}\indexlabel{point dipole model}
In single-domain IP nanomagnets, the magnetisation prefers to align along the fixed \xref{easy axis} of the geometry, while for OOP magnets a strong interfacial anisotropy with the substrate causes a preferential orientation along the $z$-axis.
Either way, it is natural to use an Ising-like approximation for simulating such single-domain nanomagnets.
The easy axis, position $\vc{r}_i$ and size of the magnetic moment\footnote{
	\label{fn:2:moment_integral}
	The size of the magnetic moment $\mu_i$ corresponds to the total ground state magnetic moment $\abs{\int_{\Omega_i} \vc{M}(\vc{r})d\vc{r}}$, with $\Omega_i$ the geometry of magnet $i$ and $\vc{M}(\vc{r})$ its magnetisation in the twofold degenerate ground state.
	Due to edge relaxation effects, this value is slightly smaller than $M_\mathrm{sat} V_i$.
} $\mu_i$ of each magnet $i$ are fixed and they are only allowed to switch between the `up' ($\uparrow$, $s_i=1$) and `down' ($\downarrow$, $s_i=-1$) magnetisation states along the easy axis.
Thus, the total magnetic moment vector $\vc{\mu}_i$ of magnet $i$ can be expressed as
\begin{equation}
	\vc{\mu}_i = s_i \mu_i \vc{u}_i \mathrm{,}
\end{equation}
where $s_i = \pm 1$, and $\vc{u}_i$ is a fixed unit vector parallel to the easy axis. \par
The switching rate between these two states is determined by the \xref{effective energy barrier} $\EBeff$ that separates them, as well as the temperature $T$.
For an isolated nanomagnet, the energy barrier $\EB$ originates from its \xref{shape anisotropy}.
Interactions with other magnets or \xref{external magnetic fields} modify the energy landscape, leading to an effective barrier which we denote as $\EBeff$~\cite{leo2021chiral}.
Each magnet can have a unique magnetic moment size $\mu_i$, temperature $T_i$ and energy barrier $E_{\mathrm{B},i}$.
This enables, for instance, modelling some of the disorder due to lithographic variations by assigning a different shape anisotropy to each magnet, typically sampled from a Gaussian distribution with mean $\EB$ and standard deviation $\sigma(\EB)$.

\xfig[1.0]{2_Hotspice/ASIs.pdf}{
	Predefined artificial spin ice (ASI) lattices available in \hotspice.
	The unit cell of each lattice is delineated by a central dark grey rectangle.
	The red indicator defines the lattice parameter $a$.
	In the Ising approximation, the magnetisation of in-plane magnets (top) aligns along the major axis of the depicted ellipses.
	Out-of-plane magnets (bottom) are illustrated as circles.
	\label{fig:2:ASIs}
}

Due to the periodic nature of many ASI lattices, \hotspice chooses to perform the simulation on a rectilinear grid as this allows efficient matrix and array manipulations.
Each grid point may or may not contain a magnet, and the magnets must either all be of the IP type, or all OOP.
The benefits and details of this approach will be elaborated on in~\cref{sec:2:Implementation}.
Even though this implementation does not allow complete freedom in the placement of magnets, many popular ASI lattices can be constructed in this manner.
\cref{fig:2:ASIs} showcases the 12 lattices that \hotspice provides out-of-the-box. \par
The pinwheel and square lattices come in two variants, related by a global \ang{45} rotation of the entire lattice~\cite{ApparentFMpinwheel}.
This gives rise to different boundaries due to the Cartesian character of the underlying grid, which can alter the dynamics of the ASI.
Furthermore, the unit cell for lucky-knot pinwheel (b) and open square (d) is more compact than for the more popular diamond pinwheel (a) and closed square (c), resulting in faster simulation~\cite{AdvancesASI}. \par % REF shows more popular lattices
Magnets in the pinwheel lattices (a) and (b) are placed at the same location as in the square lattices (c) and (d), respectively, but with each magnet rotated by \ang{45}.
The same can be said of the triangle (f) and kagome (g) lattices where individual magnets are rotated by \ang{90}.
The Cairo lattice (h) can be continuously deformed into the Shakti lattice~\cite{ShaktiCairo}, but note that the \xref{point dipole model} is no longer appropriate for the latter; instead, a dumbbell model (see~\cref{sec:2:Dumbbell}) would be more accurate. \par
The remaining four IP and four OOP lattices are also related: the magnets in the OOP lattices (i)-(l) are positioned at the vertices where magnets meet in their respective IP counterparts (e)-(h).

\section{Energy calculation}
The energy of the system is an essential quantity in these Monte Carlo simulations.
In this section, we will list the various energy contributions used in \hotspice, explain how we may account for the finite size of real nanomagnets within this Ising-like model, and show how the effective energy barrier $\EBeff$ is calculated.

\subsection{Energy contributions}
Three \idx{energy contributions} have been implemented\footnote{
	Users can implement more energy contributions by inheriting from the \python{hotspice.Energy} class and implementing the \python{abstractmethod}s, taking care to correctly account for open or periodic BC when necessary.
} in \hotspice, supporting both open and periodic boundary conditions (PBC).
\begin{enumerate}
	\item The \emph{\xlabel{magnetostatic interaction} energy} between magnets $i$ and $j$
	\begin{equation}
		\label{eq:2:E_MS}
		E_{\mathrm{MS},i,j} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\abs{\vc{r}_{ij}}^3} - \frac{3(\vc{\mu}_i \bcdot \vc{r}_{ij}) (\vc{\mu}_j \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^5}) \mathrm{,}
	\end{equation}
	with $\mu_0$ the vacuum permeability and $\vc{r}_{ij} = \vc{r}_j - \vc{r}_i$ the vector connecting the two magnetic dipoles $\vc{\mu}_i$ and $\vc{\mu}_j$. \par
	This is the main interaction dictating how separate nanomagnets influence each other, causing the typical properties of the various ASI lattices, e.g. superferromagnetism in the pinwheel lattice~\cite{li2018pinwheel}.
	Because of its importance, this is the only interaction \hotspice considers by default when an ASI is created.
	An ASI stores a list of energy contributions: when needed, the user must explicitly add other energy contributions to this list; see \cref{sec:2:API}.
	This avoids wasting calculations on energies not relevant to the simulation.
	
	\item The \idx{Zeeman energy} of an \xlabel{external magnetic field} $\vc{B}_\mathrm{ext}$ interacting with magnet $i$
	\begin{equation}
		\label{eq:2:E_Z}
		E_{\mathrm{Z},i} = -\vc{\mu}_i \bcdot \vc{B}_\mathrm{ext} \mathrm{,}
	\end{equation}
	where $\vc{B}_\mathrm{ext}$ can be set for each magnet individually. \par
	This energy contribution provides a means for the outside world to interact with the system, and is therefore indispensable when we will be investigating reservoir computing later on.
	Even if input is provided through other means than an external field, this energy contribution can often still be used by considering an effective field instead.
	
	\item The \textit{\xlabel{exchange coupling} energy} between nearest neighbours (NN) $i$ and $j$
	\begin{equation}
		\label{eq:2:E_exch}
		E_{\mathrm{exch},i,j} = J \frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\mu_i \mu_j} \mathrm{,}
	\end{equation}
	with $J$ the exchange coupling constant, which is constant throughout the ASI. \par
	This interaction is rarely present in ASI, but can for example be relevant in interconnected ASI --- whether by design or due to limited lithographic accuracy.
	We will encounter an example of the latter in \cref{sec:3:OOP:MFM}.
\end{enumerate}

The combined \idx{interaction energy} $E_i$ of a single magnet $i$ with its environment is then given by
\begin{equation}
	\label{eq:2:E}
	E_i = E_{\mathrm{Z},i} + \sum_j E_{\mathrm{MS},i,j} + \sum_{j \in \mathcal{N}_i} E_{\mathrm{exch},i,j} \mathrm{,}
\end{equation}
where $\mathcal{N}_i$ is the collection of nearest neighbours of magnet $i$.
Which magnets are included in this collection depends on the ASI lattice and which site of the unit cell magnet $i$ is in, and can be defined separately for each ASI lattice. \\\par
Note that all terms in \cref{eq:2:E} simply change sign\footnote{
	Energy contributions which are independent of the state $s_i$, such as the magnetostatic self-energy (i.e., the energy due to the magnetisation profile $\vc{M}(\vc{r})$ throughout the magnet), are not included in \cref{eq:2:E} because they are constant and therefore do not affect the ASI dynamics.
} when magnet $i$ switches ($\vc{\mu}_i \rightarrow -\vc{\mu}_i$): if the switch occurs at time $t$, then $E_i(t + dt) = -E_i(t - dt)$.
%As such, $E_i$ represents the total interaction energy of a magnet with its `neighbours',\footnote{
%	In this context, a `neighbour' of a magnet can be interpreted more broadly as all magnets it interacts with through a particular energy contribution. For example, the magnetostatic interaction considers all magnets to be `neighbours', unless the user has explicitly set a maximum interaction distance.
%}.
Therefore, the change in energy of the ASI when that magnet switches is simply $\Delta E_{i,1\rightarrow2} = -2 E_i$.
This is called the \idx{switching energy}, and we will see in the next few sections that it plays a central role in both algorithms used for simulating system dynamics as well as the calculation of the \xref{effective energy barrier} $\EBeff$.
It is therefore very advantageous that the switching energy is so cheap to compute. \\\par
When the simulation is initialised, the energy contributions are calculated for all magnets.
Each magnet therefore stores a value in memory for each of the terms in \cref{eq:2:E}.
Whenever a magnet switches, the energies of all magnets it interacts with are updated appropriately, which constitutes a major part of the calculation effort required for every step in the simulation.

\subsection{Finite-size corrections to the magnetostatic energy}\label{sec:2:finite}
\cref{eq:2:E_MS,eq:2:E_Z,eq:2:E_exch} approximate each nanomagnet as a \link{point dipole model}{point dipole}, but real nanomagnets have a finite spatial extent.
If one assumes a uniform magnetisation throughout each single-domain nanomagnet, then this finite size does not affect the \xref{Zeeman energy}, nor the \link{exchange coupling}{exchange energy} which can capture any shape-related effects by an appropriate choice of the exchange coupling $J$.
The Zeeman energy could even account for a deviation from uniform magnetisation near the edge of a magnet, which occurs in reality but which we will neglect in the following, by an appropriate choice of $\vc{B}_\mathrm{ext}$. \par
The \xref{magnetostatic interaction}, however, depends on the relative position, orientation, and shape of all magnets.
This may result in inadequate simulation of closely spaced ASI where the true magnetostatic coupling can be significantly stronger than predicted by a point dipole approximation.
Therefore, two (mutually exclusive) improvements have been implemented in \hotspice, which rescale the magnetostatic interaction energy between magnets.

\subsubsection{Second-order correction for dipoles}\indexlabel{finite dipole model}
\textit{Politi and Pini}~\cite{Dipolar2Dparticles} have presented a multipole expansion of the magnetostatic interaction, to account for the finite size of 2D nanomagnets (i.e., lateral dimensions $\gg$ thickness), assuming a uniform magnetisation.
This results in a second-order correction
\begin{equation}
	E_{\mathrm{MS},i,j} = E_{\mathrm{MS},i,j}^\mathrm{(0)} + E_{\mathrm{MS},i,j}^\mathrm{(2)} \mathrm{,}
\end{equation}
where $E_{\mathrm{MS},i,j}^\mathrm{(0)}$ is the original \link{point dipole model}{point dipole} \xref{magnetostatic interaction} given by~\cref{eq:2:E_MS}. \par
The second-order correction can be written as
\begin{equation}
	\label{eq:2:E_MS_order2}
	E_{\mathrm{MS},i,j}^\mathrm{(2)} = \frac{\mu_0}{4\pi} \frac{3\mathcal{I}_{ij}}{2} \Bigg[3\frac{\vc{\mu}_i^\mathrm{OOP} \bcdot \vc{\mu}_j^\mathrm{OOP}}{\abs{\vc{r}_{ij}}^5} + \frac{\vc{\mu}_i^\mathrm{IP} \bcdot \vc{\mu}_j^\mathrm{IP}}{\abs{\vc{r}_{ij}}^5} -5\frac{(\vc{\mu}_i^\mathrm{IP} \bcdot \vc{r}_{ij}) (\vc{\mu}_j^\mathrm{IP} \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^7} \Bigg] \mathrm{,}
\end{equation}
where $\vc{\mu}_i$ was split into its IP and OOP components, conveniently leading to separate IP and OOP terms as implemented in the two types of ASI in \hotspice. \par
The particular shape of the nanomagnets is encapsulated in the single scalar $\mathcal{I}_{ij} = (\mathcal{I}_i + \mathcal{I}_j)/2$.
These $\mathcal{I}$ are calculated similar to a moment of inertia:
\begin{equation}
	\mathcal{I}_i = \int_{\Omega_i} \abs{\vc{r} - \ab(\int_{\Omega_i} \vc{r} d\vc{r})}^2 d\vc{r} \mathrm{,}
\end{equation}
with $\Omega_i$ the volume of magnet $i$.
We assume all magnets have the same shape, such that $\mathcal{I}_{ij} = \mathcal{I}_i = \mathcal{I}_j$.
For elliptic cylinders, these moments of inertia reduce to the simple expression $\mathcal{I}_{ij} = \frac{1}{4}(a^2 + b^2)$.
Therefore, \hotspice assumes the magnets in an ASI to be round (OOP ASI, e.g.~\cite{PerpendicularMagnetizationASI}) or elliptical (IP ASI), and allows the user to set the semi-major axis $a$ and semi-minor axis $b$ of the magnets comprising an ASI.
Although IP magnets are typically more stadium-shaped~\cite{EmergentChiralityRatchet,clocking-protocol}, the moment of inertia for an ellipse does not differ by much --- if necessary, an appropriate value of $a$ and $b$ can account for this.
%While this correction can be applied to both IP and OOP magnetic dipoles, it is most effective for OOP systems, as can be seen in~\cref{fig:2:MS_distance}.

\subsubsection{Dumbbell model}\label{sec:2:Dumbbell}
Instead of representing a magnet as a \link{point dipole model}{point dipole}, one may instead choose to represent it as a pair of \idx{magnetic monopoles}~\cite{MagneticMonopoles2008,MagneticMonopoleDynamics}.
This introduces a new parameter $d$: the effective distance between the north and south poles of a magnet, with respective positions $\vc{r_N}_i = \vc{r}_i + s_i\frac{d_i}{2}\vc{u}_i$ and $\vc{r_S}_i = \vc{r}_i - s_i\frac{d_i}{2}\vc{u}_i$.
An appropriate choice of $d_i$ (slightly smaller than the physical length $l$ of the nanomagnet~\cite{DDG_Masterproef}) allows this \idx{dumbbell model} to emulate the spatial extent of a real nanomagnet. \par
The north and south monopoles are assigned \indexlabel{magnetic charge}magnetic charges $+q_i$ and $-q_i$, respectively, with $q_i=\mu_i/d_i$~\cite{MagneticMonopoles2008}.
This choice yields the same effective dipole moment at long distance.
The interaction energy between two magnetic charges $q$ and $q'$ can be derived from the magnetic version of Coulomb's law~\cite{ForceMagneticDipole} as
\begin{equation}
	E = -\int_\infty^{\vc{r}} \frac{\mu_0}{\num{4}\pi}\frac{qq'}{\abs{\vc{r}}^3} \vc{r} \cdot d\vc{r} = \frac{\mu_0}{\num{4}\pi} \frac{qq'}{\abs{\vc{r}}} \mathrm{.}
\end{equation}
The magnetostatic interaction energy between two nanomagnets is then the sum of their four mutual monopole interactions, finally resulting in
\begin{equation}
	E_{\mathrm{MS},i,j} = \frac{\mu_0 \mu_i \mu_j}{4\pi d_i d_j} \Bigg(\frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_N}_j}} + \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_S}_j}}\\ - \frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_S}_j}} - \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_N}_j}}\Bigg) \mathrm{.} \label{eq:2:E_MS_mono}
\end{equation}
The minus sign in the equation appears because north and south poles have opposite charge.
\hotspice is limited to a single value of $d$ for all magnets, due to the structure of the kernels used to calculate the magnetostatic interaction, which will be discussed in~\cref{sec:2:Kernels}.

\subsubsection{Comparison}
To assess whether these two corrections constitute an improvement to the resulting \link{magnetostatic interaction}{magnetostatic energy}, we must quantify their impact by comparing them against a known solution.
For this, we use the \link{micromagnetic simulations}{micromagnetic simulation} package \mumax~\cite{mumax3}, which can determine the magnetostatic interaction energy for a given arrangement of ferromagnetic material.
While this solution is not exact, as \mumax uses a \xlabel{finite-difference} (FD) discretisation, the result will approach the true value for sufficiently small FD cell sizes. \\\par

\cref{fig:2:MS_distance} compares the original \link{point dipole model}{point dipole approximation} and the two corrections (``\link{finite dipole model}{finite dipole}'' and ``\link{dumbbell model}{dumbbell}'') against the solution obtained with \mumax.
The figure shows 3 typical arrangements of neighbouring magnets in an ASI: two circular OOP magnets and two elliptical IP neighbours aligned along their \link{easy axis}{easy} and hard axes.
The magnetostatic interaction energy between the pair of magnets, divided by $\mu^2$ to be independent of material parameters and magnet size, is shown as a function of their normalised centre-to-centre distance.
A uniform magnetisation was used in the \mumax simulation, as this is the assumption under which the corrections were derived and because the non-uniform lowest energy magnetisation state is size-dependent while the figure uses dimensionless units. \\\par

\xfig[1.0]{2_Hotspice/MS_distance.pdf}{
	Magnitude of the magnetostatic interaction between two magnets as a function of their normalised centre-to-centre distance, for the three \hotspice{} calculation methods (\link{point dipole model}{point dipole}, \link{finite dipole model}{second-order correction for dipoles}, and \link{dumbbell model}{dumbbell}) compared to a \link{micromagnetic simulations}{micromagnetic} \mumax{} calculation.
	OOP magnets are assumed to be circular with diameter $2r$, IP magnets are ellipses with length $l$ and width $w=4l/11$.
	Positions of north and south \link{magnetic monopoles}{monopoles} used in the dumbbell model are shown as red {\color{red}$\bullet$} and blue {\color{blue}$\bullet$} dots and are a distance $d=0.9l$ apart within a magnet.
	\label{fig:2:MS_distance}
}

For out-of-plane (OOP) systems, the \xref{dumbbell model} is inadequate due to the small fringe fields and the limited thickness of the magnets.
Instead, the \link{finite dipole model}{second-order dipole correction} is more appropriate, yielding a significant improvement towards the ideal \mumax curve.
Still, a discrepancy remains for separations below $r_{ij}/2r \lessapprox 1.5$, which could be reduced by even higher-order corrections. \\\par

For IP systems, the dumbbell model constitutes a vast improvement over the standard \link{point dipole model}{point dipole} treatment.
The dumbbell model does, however, require an additional parameter $d$, which affects the interaction energy.
For the best correspondence with \mumax, the monopole-monopole distance $d$ should be set slightly shorter than the length $l$ of a magnet, typically around $d/l\approx0.9$.
This adjustment accounts for the curvature and corresponding non-uniform magnetisation at the ends of real nanomagnets.
Similar values for $d/l$ were previously found in~\ccite{DDG_Masterproef} for typical nanomagnet shapes like ellipses and stadiums. \par
In contrast, the second-order dipole correction has little effect in IP systems and can even increase the discrepancy with \mumax. It emulates increased spatial extent and therefore always increases the interaction, but for magnets neighbouring along their hard axes (rightmost panel in~\cref{fig:2:MS_distance}) the point dipole model already overestimates the interaction. \par
In conclusion, the dumbbell model is preferred for IP systems, while the second-order dipole correction is most suitable for OOP systems.

\subsection{Effective energy barrier}\label{sec:2:E_B_eff}
To properly simulate ASI dynamics and hysteresis, knowledge of the \idx{effective energy barrier} $\EBeff$, which separates the two magnetisation states of each magnet, is crucial.
Recall that this quantity is distinct from the \xref{shape anisotropy} $\EB$: the effective energy barrier $\EBeff$ is a modification of $\EB$ caused by the interaction with other magnets. \par
This is illustrated in~\crefSubFigRef{fig:2:EB_meanbarrier}{b}, where \indexlabel{energy landscape}energy landscapes are shown for various values of the \xref{switching energy} $\Delta E$. The energy landscape for $\Delta E = 0$ reveals the shape anisotropy $\EB$ while slanted energy landscapes for $\Delta E \neq 0$ show how $\EBeff$ is affected by these interactions.
The figure shows that $\EBeff$ can be calculated at various levels of accuracy, which may yield different switching rates or even a different switching order~\cite{leo2021chiral}. \par
In this section, we will explore some of these approximations for $\EBeff$.
All of them make use of the switching energy $\Delta E$ between the current and opposite state of a magnet, since this is cheap to compute\footnote{
	Recall that the terms in \cref{eq:2:E} simply change sign when the magnetisation $\vc{\mu}$ of a magnet is reversed.
} as $\Delta E = -2 E$.

\xfig[1.0]{2_Hotspice/EB_meanbarrier.pdf}{
	Mean-barrier approximation (\ref{eq:2:EB_meanbarrier_original}, \textcolor{lightblue}{blue}) and its conditional form (\ref{eq:2:EB_meanbarrier_cases}, \textcolor{lightred}{red}) compared to the exact energy barrier (\ref{eq:2:EB_exact}, \textcolor{mplgrey}{grey}).
	Due to the limited information available to a \xref{mean-barrier model} (only $\EB$ and $\Delta E$), the exact solution assumes a sinusoidal shape anisotropy and a uniform \xref{external magnetic field} along the \xref{easy axis}.
	\textbf{(a)} Effective energy barrier $\EBeff$ as function of \xref{switching energy} $\Delta E$ for these approximations.
	\textbf{(b)} Energy landscape between the two stable states (black filled circles) for several values of the switching energy $\Delta E$.
	The energy landscapes without and with shape anisotropy are shown as dotted black and solid grey lines, respectively.
	An open circle indicates the halfway point between both stable states, while a star is put at the top of the exact energy barrier.
	On each landscape, the height of the barrier in the different approximations is indicated by a dotted horizontal line, indicating how they use different values on the energy landscape for their estimate.
	\label{fig:2:EB_meanbarrier}
}
\vspace{-1em}
We will omit the subscript $i$ for the remainder of this section; it is implied that the following discussion and equations apply to each magnet individually.

\subsubsection{Intrinsic barrier due to \xlabel{shape anisotropy}} % TODO END: is any of this more relevant in the introduction?
% TODO: elaborate about the anisotropy constant: demag tensor with form factor $\mathcal{N}$ of a cylinder etc.
\label{sec:2:shape_anisotropy}
An isolated single-domain nanomagnet often exhibits two stable magnetisation states separated by energy barriers.
In IP ASI, these states usually arise from \textit{shape anisotropy}, which originates from the \xlabel{demagnetising field} of the magnet itself.
This imposes an additional energy cost when the magnetisation does not point along the preferential \idx{easy axis}, which is usually the longest axis of the magnet's geometry~\cite{PhD_Leliaert}. \par
Shape anisotropy is quantified by a \xlabel{uniaxial anisotropy} constant $K_\mathrm{u}$.
Assuming that switching occurs by \xlabel{coherent rotation}\footnote{
	Depending on the magnet's properties, \xlabel{non-coherent magnetisation reversal} such as switching by domain wall nucleation may be energetically cheaper.
	An accurate estimate of the energy barrier for such processes can be determined by e.g. \xref{micromagnetic simulations}.
}, this leads to an energy barrier $\EB = K_\mathrm{u} V$, with $V$ the magnet's volume.
Similar to the calculation of the magnetic moment $\mu$ in \cref{fn:2:moment_integral}, relaxation of the magnetisation profile $\vc{M}(\vc{r})$ near the surface of the magnet may cause the actual energy barrier to be slightly smaller.
\hotspice ignores the specifics of the reversal process by allowing the user to set an arbitrary value of $\EB$. \par
Magnets in OOP ASI are usually very flat, so one would expect the shape anisotropy to result in a preferential in-plane magnetisation direction.
This can be avoided by properly choosing materials that exhibit a strong interfacial anisotropy, as occurs at e.g. the Co/Pt interface, resulting in a net preferential out-of-plane magnetisation.

\subsubsection{Mean-barrier model}
The simplest approximation of the effective energy barrier $\EBeff$ is the \idx{mean-barrier model}.
It assumes that the highest-energy state lies halfway between the two stable states.
This means the barrier height changes at half the rate at which the \xref{switching energy} $\Delta E$ changes, leading to the approximation
\begin{equation}
	\label{eq:2:EB_meanbarrier_original}
	\EBeff = \EB + \frac{\Delta E}{2} \mathrm{,}
\end{equation}
as it is often encountered in literature~\cite{MC_TemperatureDesorption,DirectionalEnergyBarrier}.
This method is illustrated in blue in~\cref{fig:2:EB_meanbarrier}. \par
However, this is a very crude approximation which does not account for the extreme cases where the interactions are so strong that the energy barrier effectively disappears.
In the model of \cref{eq:2:EB_meanbarrier_original}, this happens when $\abs{\Delta E}$ exceeds twice the \xref{shape anisotropy} $\EB$, leaving only one global minimum.
This can be seen in \crefSubFigRef{fig:2:EB_meanbarrier}{b} for $\Delta E = \pm 4 \EB$, where \cref{eq:2:EB_meanbarrier_original} (blue) clearly does not make physical sense.
To handle these situations, \hotspice modifies the calculation of the effective energy barrier $\EBeff$ as follows:
\begin{equation}
	\label{eq:2:EB_meanbarrier_cases}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} + \frac{\Delta E}{2} & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B}, \\
		\Delta E & \quad \text{otherwise}.
	\end{cases}
\end{equation}
This way, $\Delta E$ serves as the barrier when the original energy barrier disappears.
This adjustment is shown in red in~\cref{fig:2:EB_meanbarrier}: it is closer to the exact solution except for $-4 \EB < \Delta E < -2 \EB$.
Notwithstanding that the original mean-barrier approximation~\eqref{eq:2:EB_meanbarrier_original} lies closer to the exact solution in this range, we choose not to use it as this lies outside its valid range of $\abs{\Delta E} < 2 \EB$.

\subsubsection{Asymmetric barrier}\indexlabel{asymmetric energy barrier}
The simple \xref{mean-barrier model} is insufficient for many IP ASI.
In real nanomagnets, reversal by \xref{coherent rotation} can occur via two pathways; clockwise ($\clockwise$) or counter-clockwise ($\counterclockwise$) rotation of the magnetisation.
In an asymmetrical environment --- when the \xref{effective field} has a non-zero component perpendicular to the \xref{easy axis} --- one of these two rotation directions will be preferred~\cite{leo2021chiral,DirectionalEnergyBarrier}.
Take for example the pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}): any two neighbouring magnets form a T-shape, so the magnet pointing into the side of the other will greatly influence whether the other magnet prefers $\clockwise$ or $\counterclockwise$ rotation~\cite{DirectionalEnergyBarrier}. % Q: does this need a figure?
% The mean-barrier model would take the energy difference $\Delta e$ between the two states; for two magnets in a T-shape, $\Delta E = 0$ for both, resulting in an unchanged energy barrier $\EB$ which is clearly incorrect in this situation.
Such an asymmetry can not occur in OOP ASI, so the asymmetric barrier is only applicable to IP ASI.
Accounting for the existence of these separate chiral switching channels profoundly affects the switching rates and transition kinetics, since switching will occur predominantly via the more favourable pathway~\cite{leo2021chiral}. \\\par

In the dipole model, this can be accounted for by considering the energy of each magnet in these transitional states along the hard axis.
Therefore, \hotspice tracks yet another quantity for every magnet: $E_\perp$\indexlabel{hard-axis interaction energy}, representing the \xref{interaction energy} of a magnet if it would point along $\vc{e}_z \times \vc{u}$, i.e. \ang{90} counter-clockwise from its normal magnetisation direction $\vc{u}$.
Note that we are not deviating from the two-state Ising model: we are simply putting ``test dipoles'' along the hard axis to get a better estimate of $\EBeff$, but magnets will never end up in these states during a simulation. \par
In \cref{eq:2:EB_meanbarrier_cases}, $\EB$ essentially represented an estimate of the total energy in the transitional state.
With our newfound knowledge of the interaction energy $E_\perp$ in that state, our improved estimate of the total hard-axis energy becomes $\EB + E_\perp$.
Therefore, we get an expression for the effective barrier $\EBeff$ along the two rotation pathways ($\pm$) by simply substituting $\EB \rightarrow \EB + E_\perp$ in \cref{eq:2:EB_meanbarrier_cases}:
\begin{equation}
	\label{eq:2:EB_asymmetric}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} \pm \rho E_\perp + \frac{\Delta E}{2} & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B} \pm \rho E_\perp, \\
		\Delta E & \quad \text{otherwise}, \\
	\end{cases}
\end{equation}
which results in two different barriers if $E_\perp \neq 0$.
The parameter $\rho = \mu_\perp/\mu_\parallel > 0$ was introduced in \cref{eq:2:EB_asymmetric} to account for \xref{non-coherent magnetisation reversal} processes like domain wall nucleation and propagation, which result in an effective reduction of the magnetic moment during reversal~\cite{leo2021chiral,TimeResolvedDynamicsSOT}.
Using a value $\rho < 1$ improves correspondence with experimental observations, as we will observe in~\cref{sec:3:IP_Pinwheel_reversal}. \par
As a bonus, we can use the combined knowledge of $E$ and $E_\perp$ to implicitly define the \idx{effective field} $\vc{B}_\mathrm{eff}$ that any given magnet experiences by
\begin{equation}
	\label{eq:2:B_eff_implicit}
	\begin{cases}
		\vc{\mu} \,\cdot\, \vc{B}_\mathrm{eff} = E \mathrm{,} \\
		\norm{\vc{\mu} \times \vc{B}_\mathrm{eff}} = E_\perp  \mathrm{.}
	\end{cases}
\end{equation}
For IP ASI, this uniquely defines $\vc{B}_\mathrm{eff}$ since all vectors lay in-plane. Solving \cref{eq:2:B_eff_implicit} for $\vc{B}_\mathrm{eff}$ yields an expression as function of $E$, $E_\perp$ and $\vc{\mu}$, valid for both IP and OOP ASI:
\begin{equation}
	\label{eq:2:B_eff_explicit}
	\vc{B}_\mathrm{eff} = - \frac{E \vc{\mu} + E_\perp \vc{e}_z \times \vc{\mu}}{\mu^2} \mathrm{.}
\end{equation}

\subsubsection{Exact solution}
The effective energy barrier $\EBeff$ can be calculated exactly if an analytical expression is known for the energy as a function of magnetisation angle $\theta$ relative to the magnet's \xref{easy axis}.
The \xref{shape anisotropy} creates an \xref{energy landscape} with two minima at $\theta=0$ and $\theta=\pi$, but the exact form of this landscape depends on the magnet's shape.
For ellipsoidal magnets, the \xref{energy landscape} is $-\frac{E_\mathrm{B}}{2} \cos{2\theta}$~\cite{neel1949theorie}.
Assuming a uniform magnetisation in each magnet\footnote{
	Only perfectly ellipsoidal magnets have uniform magnetisation in a uniform \xref{external magnetic field}~\cite{EllipsoidDemag,MaxwellElectricityMagnetism}.
}, the \link{magnetostatic interaction}{magnetostatic} and \link{Zeeman energy}{Zeeman} interactions add a term proportional to $\cos(\theta - \phi)$, with $\phi$ the angle of their combined \xref{effective field} $\vc{B}_\mathrm{eff}$.
Thus, the total landscape is a sum of two sines and can be fully characterised if $E$ and $E_\perp$ are known.
However, in the general case, this results in a transcendental equation
\begin{align*}
	\label{eq:2:EB_exact_transcendental}
	\frac{\partial E(\theta)}{\partial \theta} = 0 \iff & \frac{\partial}{\partial \theta} \ab(-\frac{\EB}{2} \cos{2\theta} - \mu B_\mathrm{eff} \cos(\theta - \phi)) = 0 \\
	\iff & \EB \sin{2\theta} + \mu B_\mathrm{eff} \sin(\theta - \phi) = 0 \mathrm{,} \numberthis
\end{align*}
which would require numerical approximation to solve.
Since this could significantly impact performance, this is not done in \hotspice. \par
In OOP ASI, however, the high degree of symmetry nonetheless allows an explicit expression to be obtained.
All relevant vectors ($\vc{B}_\mathrm{eff}$, $\vc{\mu}$ ...) in such systems point along the z-axis, causing the \link{hard-axis interaction energy}{hard-axis energy} to vanish ($E_\perp=0$) such that the equation is no longer transcendental.
Solving \cref{eq:2:EB_exact_transcendental} and applying the double-angle identities for $\cos(2\theta)$ and $\sin(2\theta)$ instead leads to the quadratic relation % Q: should I derive this from the d/dtheta equations or is this clear enough? (only needs double-angle rules for cos and sin basically)
\begin{equation}
	\label{eq:2:EB_exact}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} \ab(\frac{\Delta E}{4 E_\mathrm{B}} + 1)^2 & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B}, \\
		\Delta E & \quad \text{otherwise,} \\
	\end{cases}
\end{equation}
as has previously been described by \textit{Tannous and Gieraltowski}~\cite{StonerWohlfarth2008}.
This is the exact solution that was shown in grey in~\cref{fig:2:EB_meanbarrier}.
However, since OOP magnets are not ellipsoidal as assumed by the transcendental equation that originally led to \cref{eq:2:EB_exact_transcendental}, the applicability of \cref{eq:2:EB_exact} remains questionable.

\newpage % For better alignment
\section{Dynamics}\label{sec:2:Dynamics}
A Monte Carlo simulator would not be complete without an algorithm to change the state of the system.
Since any magnet in the ASI may spontaneously switch to the opposite magnetisation state due to thermal fluctuations, \hotspice evaluates the time evolution of the ASI in a stepwise manner using an \idx{update algorithm} that determines which magnet should switch next. \par
In particular, \hotspice uses \idx{kinetic Monte Carlo} (KMC) algorithms\footnote{Sometimes also referred to as dynamic Monte Carlo.}.
Generally speaking, KMC algorithms can be divided into two distinct classes: rejection-free and rejection KMC.
\hotspice implements algorithms of both types.
These techniques go by many names --- here, we shall refer to them as the \emph{\link{first-switch method}{``first-switch'' method}} and \emph{\xref{Metropolis-Hastings sampling}}, respectively~\cite{gillespie1976general,PhysicalTimeKMC}.
The former is more suitable for simulating the temporal evolution of the system, while the latter can be used to sample the equilibrium distribution of the state space.

\subsection{First-switch method: temporal evolution}
\idx{N\'eel relaxation theory}~\cite{neel1949theorie} states that, for an isolated nanomagnet, the \xlabel{switching rate} $\nu$ is given by the \xlabel{N\'eel-Arrhenius} equation
\begin{equation}
	\nu = \nu_0 \exp\ab(-\frac{\EB}{\kBT}) \mathrm{,}
	\label{eq:2:Néel}
\end{equation}
with $\kBT$ the thermal energy and $\nu_0$ the so-called \xref{attempt frequency}.
For mutually interacting magnets, an adjusted version of \cref{eq:2:Néel} can be used where $\EB$ is replaced by the \xref{effective energy barrier} $\EBeff$. \par
In the general case where the energy barriers for clockwise and anticlockwise rotation during switching differ, these two switching channels ($\circlearrowright$ and $\circlearrowleft$) will separately follow~\cref{eq:2:Néel}, so their switching frequencies must be combined.
This yields the total switching rate as presented by \textit{Koraltan et al.}~\cite{DirectionalEnergyBarrier},
\begin{equation}
	\nu = \nu_\circlearrowleft + \nu_\circlearrowright = \frac{\nu_0}{2} \ab[\exp\ab(-\frac{\EBeffLeft}{\kBT}) + \exp\ab(-\frac{\EBeffRight}{\kBT})] \mathrm{,}
	\label{eq:2:Néel_2}
\end{equation}
where a halved attempt frequency $\nu_0/2$ was assigned to either switching channel such that~\cref{eq:2:Néel_2} reduces to~\cref{eq:2:Néel} in the case of $\EBeffLeft=\EBeffRight$~\cite{leo2021chiral}.
% Note that it does not matter which barrier corresponds to clockwise or counter-clockwise rotation; only the height of both barriers matters when determining the switching rate of a magnet.

\paragraph{Attempt frequency}\indexlabel{attempt frequency}
An estimate of $\nu_0$ for coherent magnetisation reversal can be obtained from the limit $\EB \rightarrow 0$, where the \xref{switching rate} $\nu$ should approach the \xlabel{gyromagnetic precession frequency} of the magnetisation of a nanomagnet.
This is reported to be on the order of \qtyrange{e9}{e10}{\hertz}~\cite{BrownThermalFluctuations,bean1959superparamagnetism}.
As $\EB \rightarrow 0$, \cref{eq:2:Néel} implies that $\nu \rightarrow \nu_0$, so we use $\nu_0=\qty{e10}{\hertz}$~\cite{JM_Masterproef}. \par % Each oscillation is an attempt to leave this energy minimum.
If necessary, a more precise value for $\nu_0$ can be obtained from experiments.
However, an order-of-magnitude estimate often suffices because any small (i.e., $\sim \kBT$) change of $\EB$ will translate to an exponential change of the switching rate.
As discussed in~\cref{sec:2:E_B_eff}, our approximations of $\EBeff$ are imperfect, thus rendering an accurate value of $\nu_0$ irrelevant.
Furthermore, $\nu_0$ only affects the absolute value of the elapsed time, not the switching order, so its overall importance to the simulation is limited. \par
The values for $\nu_0$ on the order of \SI{e10}{\hertz} are valid for reversal by \xref{coherent rotation}, as was already assumed in \hotspice during the calculation of the \link{effective energy barrier}{effective barriers}.
For other reversal processes, e.g. domain wall-mediated reversal, the attempt frequency could be many orders of magnitude faster because its interpretation as a rotation frequency then no longer holds~\cite{ArrheniusPrefactor}. % Another ref: 45 in leo2021chiral
It must also be noted that the attempt frequency can significantly depend on other system parameters like temperature~\cite{AttemptFreqTemperature}. % Ref says: ``When this energy barrier is the result of a collective statistical behavior of many constituents, it may contain an intrinsic temperature dependence which has to be carefully taken into account, in particular when the prefactor is interpreted in terms of an attempt frequency.''

\paragraph{Algorithm}
Knowledge of the \xref{switching rate} $\nu$ can be used to construct a rejection-free kinetic Monte Carlo method, first presented by \textit{Gillespie}~\cite{gillespie1976general}, referred to as the ``first-switch'' method.
Each iteration, as presented in~\cref{alg:2:FirstSwitchSingle}, will increment the elapsed time by a certain duration $t \leq t_\mathrm{max}$, where the maximum time $t_\mathrm{max}$ can be set by the user to prevent excessively long switching times.
When using the first-switch method, \cref{alg:2:FirstSwitchSingle} is simply performed as many times as needed to reach the desired elapsed time. \par
\begin{algorithm}[Single iteration of the ``first-switch'' method]
	\label{alg:2:FirstSwitchSingle}
	\indexlabel{first-switch method}
	\begin{enumerate}[rightmargin=15pt]
		\item Calculate the \xref{effective energy barriers} of all magnets (i.e., $\EBeff{}_{,i}$ for OOP ASI, $\EBeffLeft{}_{,i}$ and $\EBeffRight{}_{,i}$ for IP ASI, $\forall i$) based on their \link{interaction energy}{interaction energies} $E_i$, as determined by the current magnetisation state.
		\item Calculate the \xref{switching rate} $\nu_i$ of each magnet using \cref{eq:2:Néel_2}.
		\item Generate a random switching time interval $\Delta t_i$ for each magnet $i$, sampled from an exponential distribution with mean value $1/\nu_i$.
		\item Determine which magnet $j$ has the smallest such time $\Delta t_j = \min_i \Delta t_i$.
		\item Finally, $t_\mathrm{max}$ determines whether a switch occurs.
		\begin{itemize}
			\item \textit{If $t + \Delta t_j \leq t_\mathrm{max}$}: increment the elapsed time $t$ by $\Delta t_j$ and switch magnet $j$.
			\item \textit{If $t + \Delta t_j > t_\mathrm{max}$}: increment the elapsed time $t$ by $t_\mathrm{max}$ without switching a magnet.\footnote{The maximum time $t_\mathrm{max}$ means that, strictly speaking, this is no longer a rejection-free algorithm.}
		\end{itemize}
	\end{enumerate} % NOTE: this is different from the BKL algorithm, yet still gives the exact same result.
\end{algorithm}
% Note that this algorithm still gives the correct switching rate $\nu$, even though the switching time $t_i$ is sampled randomly multiple times before magnet $i$ typically switches. This is because, the more magnets there are, the harder it gets for an arbitrary magnet $i$ to have the shortest randomly sampled time (and thus to switch), but the elapsed time increment after each switch will be smaller as well, and these two effects cancel perfectly to end up with the same switching rate as if the magnet would be the only one switching.
The maximum time $t_\mathrm{max}$ (default value of one second) prevents the simulation from advancing too far into the future.
For example, it is needed when a time-dependent \xref{external magnetic field} is applied to the lattice: for a sinusoidal signal of frequency $f$, using $t_{\mathrm{max}}=20/f$ will ensure that the waveform is captured in sufficient detail.
Furthermore, it prevents unrealistic timescales, as the exponential character of the \xref{N\'eel-Arrhenius} law can cause switching times to become much longer than what could ever be observed experimentally. \par
In the realm of rejection-free KMC, two commonly used algorithms are the very similar \idx{Bortz-Kalos-Lebowitz}~\cite{nfoldMCalgorithm} and \idx{Gillespie}~\cite{gillespie1976general} algorithms.
Although Gillespie's paper also discusses the first-switch method, these two algorithms differ slightly from~\cref{alg:2:FirstSwitchSingle}, yet all yield identical results.
Their different strategies for calculating random switching times and selecting the next switch are discussed by \textit{Gibson and Bruck}~\cite{GibsonBruck}, alongside an optimised algorithm.

\subsection{Metropolis-Hastings: sampling equilibrium states} \label{sec:2:Dynamics_MH}
\idx{Metropolis-Hastings sampling} is a rejection-based KMC method designed to sample the state space at thermal equilibrium, where the probability of each state appearing is proportional to their \xlabel{Boltzmann factor} $e^{-E/\kBT}$~\cite{IntroductionMC,kyimba2006comparisonIsingAlgorithms}.
Hence, in contrast to the \xref{first-switch method}, Metropolis-Hastings sampling is not intended to accurately model the system's transient dynamics.
Instead, it is more suitable for examining equilibrium statistical properties of ASI, e.g. the average magnetisation, heat capacity, correlations...~\cite{ApparentFMpinwheel} \par

\paragraph{Algorithm}
A rejection-based KMC method works by selecting magnets at random and subsequently deciding, with a certain probability $P$, whether or not to switch them.
In particular, the Metropolis-Hastings algorithm repeats the steps outlined in~\cref{alg:2:MetropolisHastingsSingle}.
\begin{algorithm}[Single iteration of Metropolis-Hastings sampling]
	\label{alg:2:MetropolisHastingsSingle}
	\begin{enumerate}
		\item Select a magnet $i$ at random (with all magnets equally likely to be chosen).
		\item Calculate the energy change $\Delta E_i$ if this magnet were to switch.
		\item Switch the magnet with an \idx{acceptance probability}
		\begin{equation}
			\label{eq:2:MH_acceptance}
			P_{\mathrm{MH},i} = \begin{cases}
				\exp(-\Delta E_i/\kBT), & \text{if } \Delta E_i > 0 \mathrm{,} \\
				1 & \text{otherwise} \mathrm{.}
			\end{cases}
		\end{equation}
		\item \textit{Optional}:
		Increment the elapsed time $t$ by
		\begin{equation}
			\Delta t = -\frac{\exp\ab(\EBeff\big/\kBT\ab) \ln{\chi}}{N \nu} \mathrm{,}
			\label{eq:Metropolis_time}
		\end{equation}
		with $N$ the number of magnets in the system and $\chi$ a uniformly distributed random variable in $(0,1]$.~\cite{PhysicalTimeKMC} % Q: which \EBeff to use in the asymmetric barrier case? Hotspice uses the smallest barrier. Can also use an exponentially weighted average like in the first-switch method (\exp(x) = \frac{1}{\exp(-a) + exp(-b)}).
	\end{enumerate}
\end{algorithm}
For enhanced performance, multiple sufficiently distant magnets can be selected simultaneously, as will be explored in~\cref{sec:2:MultiSwitch}.

\paragraph{Elapsed time in rejection KMC}
While the \xref{first-switch method} relies on an explicit calculation of the elapsed time, Metropolis-Hastings sampling does not strictly require this knowledge.
Therefore, the last step of~\cref{alg:2:MetropolisHastingsSingle} --- the calculation of the elapsed time --- is optional. \par
For a long time, the notion of a well-defined measure for the elapsed time in rejection KMC was controversial~\cite{nfoldMCalgorithm,GlauberTimescale_sadiq1984,MCSim_StatPhys}. % Refs: `nfoldMCalgorithm` mentions both GD & MH acceptance prob., but says that GD can have a timescale (their equation looks a lot like in PhysicalTimeKMC) yet Metropolis cannot (due to the max()?), which is no longer true. `GlauberTimescale_sadiq1984' uses a less rigorously defined yet similar expression as in `PhysicalTimeKMC`. Don't use this reference when referring to the lack of a timescale, because this one seems to be one of the better solutions presented in literature, yet not very rigorous. Also `Lattice Kinetic Descriptions for Bulk Reaction-Diffusion Processes: Application to Alloys under Irradiation' p.215 says that ``The relationship between the simulation time and the physical time has been widely debated [MCSim_StatPhys], and it is sometimes assumed that the two time scales are proportional.''
Often, the number of performed \xref{Monte Carlo sweeps}\footnote{
	A Monte Carlo \textit{step} refers to attempting to switch a single magnet. By a \idx{Monte Carlo sweep} (MCS), we refer to $N$ attempted switches when the simulation contains $N$ magnets~\cite{NumericalDynamicalNiedermayer}. Note that no distinction between attempted switches and actual switches exists in the \xref{first-switch method}.
} was used as a crude measure, but eventually a formal derivation for the physical time scale in rejection KMC was presented by \textit{Serebrinsky}~\cite{PhysicalTimeKMC}, who derived~\cref{eq:Metropolis_time}. \par
Note that the \xref{effective energy barrier} $\EBeff$ only appears in the optional calculation of the elapsed time.
Energy barriers do not affect the \xref{acceptance probability} in the Metropolis-Hastings algorithm because they have no effect on the equilibrium state~\cite{DynamicalGlassyBehaviour}.

\paragraph{Disambiguation: Glauber dynamics}
The Metropolis-Hastings \xref{acceptance probability} in~\cref{eq:2:MH_acceptance} is the main reason why this algorithm samples the equilibrium state space.
However, it is not unique: there exist other forms of the acceptance probability which also correctly sample this equilibrium.
This is the reason why there often exists confusion between the \link{alg:2:MetropolisHastingsSingle}{Metropolis-Hastings algorithm} and the so-called \idx{Glauber dynamics}\footnote{
	The term ``Glauber dynamics'' is a misnomer~\cite{bit-player_MCvsGlauber}; the acceptance probability in~\cref{eq:2:GD_acceptance} was first derived by \textit{Flinn and McManus}~\cite{flinn1961TransitionProbability}, and later used by \textit{Glauber}~\cite{glauber1963time}.
}~\cite{flinn1961TransitionProbability,glauber1963time}, both examples of \xlabel{Markov chain Monte Carlo} (MCMC) algorithms.
These two methods differ only in their choice of acceptance probability: Glauber dynamics instead uses
\begin{equation}
	\label{eq:2:GD_acceptance}
	P_\mathrm{GD} = \frac{\exp(-\Delta E / \kBT)}{1 + \exp(-\Delta E / \kBT)} \mathrm{.}
\end{equation}

For a MCMC algorithm to sample the equilibrium state space, it must satisfy two conditions: \idx{detailed balance} and \idx{ergodicity}.
\begin{itemize}
	\item The principle of \textit{detailed balance} states that, in a system at equilibrium, each elementary process must also be in equilibrium with its reverse process.
	This can be expressed mathematically as $\pi_i P_{ij} = \pi_j P_{ji}, \forall i,j$, with $P_{ij}$ the Markov transition probability from state $i$ to $j$, and with $\pi_i$ and $\pi_j$ the equilibrium probabilities of these states appearing.
	Since all magnets are equally likely to be chosen in step 1 of~\cref{alg:2:MetropolisHastingsSingle}, it can be shown that both $P_\mathrm{MH}$ and $P_\mathrm{GD}$ result in an algorithm that satisfies detailed balance~\cite{kyimba2006comparisonIsingAlgorithms}.
	\item To guarantee that a Monte Carlo Markov chain algorithm will eventually reach this equilibrium, it must be \textit{ergodic}, i.e. from a given state it must be possible to reach any other state via some route~\cite{kyimba2006comparisonIsingAlgorithms}.
	This is clearly satisfied by~\cref{alg:2:MetropolisHastingsSingle}, for both $P_\mathrm{MH}$ and $P_\mathrm{GD}$, as the acceptance probability is strictly positive at non-zero temperatures.
\end{itemize}
As such, \cref{alg:2:MetropolisHastingsSingle} is guaranteed to reach equilibrium eventually, though the rate of convergence may vary, as noted in~\cref{sec:2:Verification_OOP_Exchange}~\cite{jang2004stochastic}. % MH seems to be the most physically accurate from a theoretical point of view, though a point can be made for either of them depending on the physics of the problem~\cite{jang2004stochastic}.

\sidefig{2_Hotspice/RejectionKMC.pdf}{
	\label{fig:2:RejectionKMC}
	Acceptance probability used by Metropolis-Hastings and Glauber dynamics, as a function of the \xref{switching energy} $\Delta E$.
	\newline\newline\newline % Newlines center the caption w.r.t. plot
}

These two common choices for the acceptance probability, $P_\mathrm{MH}$ and $P_\mathrm{GD}$, are compared in~\cref{fig:2:RejectionKMC}.
Both $P_\mathrm{MH}$ and $P_\mathrm{GD}$ result in an algorithm that satisfies ergodicity and detailed balance, so they both yield the same statistical values for quantities like the average magnetisation.
However, Glauber dynamics explores the state space more slowly because it is always more likely to reject a switch --- especially for small $\Delta E / \kBT$ --- since $P_\mathrm{GD}(\Delta E) < P_\mathrm{MH}(\Delta E), \forall \Delta E$.
Therefore, \hotspice uses the Metropolis-Hastings acceptance probability.
For a broader overview of Monte Carlo methods, we refer to Ref.~\cite{IntroductionMC}, which may further clarify the at times confusing naming present throughout literature.

\section{Implementation}\label{sec:2:Implementation}
Now that the physics underlying the simulator have been extensively discussed, we turn our attention to the software implementation. \par
First, we discuss why we chose to implement the ASI on an underlying rectilinear grid, and explain how the \textit{`kernel'} --- the lookup table for the magnetostatic interaction --- was implemented.
We then take a look at the performance of \hotspice and the various improvements enabled by the implementation of these kernels.
One particular performance-enhancing feature is examined in more detail: the possibility to select multiple magnets at once when using \xref{Metropolis-Hastings sampling}.
Finally, we briefly discuss the structure of the software package and the functionality included in the various submodules, before finishing with a short retrospective.

\subsection{Grid}\indexlabel{unit cell}
\hotspice{} represents an ASI as a non-uniform \xlabel{rectilinear grid} of unit cells, with magnets positioned at selected grid points.
This choice was made based on the trade-off between calculation efficiency and the freedom to place magnets anywhere.
We opted to prioritise efficiency and accept the geometrical restriction, as most ASI research focuses on periodic lattices.
All quantities are therefore stored in 2D matrices of size $L_x \times L_y$, upon which operations can be performed efficiently.
We will denote these matrices with large bold upright symbols: e.g., $\vc{S}$ contains the states $s_i$, $\vc{E_\mathrm{MS}}$ the magnetostatic energies $E_{\mathrm{MS},i}$, $\vc{\bigmu}$ the magnitudes $\mu_i$ of magnetic moments... \par % Q: other suggestions? I can not capitalise the magnitudes of magnetic moments, because that would just be capital M which is too reminiscent of the magnetisation (in H = B/mu0 - M). So I just put it upright. Is that sufficient?
Despite the seemingly restrictive nature of the rectilinear grid,~\cref{fig:2:ASIs} illustrates its versatility in forming various periodic lattices, with only the Cairo lattices requiring grid non-uniformity.
As a bonus, \link{graphical user interface}{real-time visualisation} is simple and efficient using this approach, as the underlying matrix can directly be cast to a pixel image. \\\par
By leveraging the unit cell concept in periodic lattices and the efficient indexation of a rectilinear grid in computer memory, several aspects of the calculation can be performed more efficiently than for free-form ASI.
%The unit cell of each lattice in~\cref{fig:2:ASIs} is depicted as a grey rectangle.
Although non-rectilinear unit cells with fewer magnets can be identified for some lattices, such unit cells would increase complexity without significant benefit: the amount of unoccupied sites in a unit cell only increases the required memory, with little impact on the performance of the simulation.

\subsection{Kernels for magnetostatic interaction}\label{sec:2:Kernels}
Pre-calculated ``kernels'' are used to efficiently update the \xref{magnetostatic interaction} after each switch. 
While these kernels have to be calculated before the simulation starts, and therefore increase the initialisation time, the reduced runtime when simulating \link{sec:2:Dynamics}{dynamics} more than makes up for this.

\subsubsection{Magnetostatic interaction kernel}
For each magnet $i$, a kernel $\vc{k}^{(i)}$ stores the magnitude of the magnetostatic interaction between itself and all other magnets.
By calculating these values beforehand, when the ASI is created, the magnetostatic interaction energy between magnets $i$ and $j$ can readily be calculated as
\begin{equation}
	E_{\mathrm{MS},i,j}= s_i s_j \vc{k}^{(i)}_j \mathrm{,}
\end{equation}
which can only change sign, since the states $s_i = \pm 1$ and $s_j = \pm 1$ are the only variables in the system. \par
Due to the fact that magnets are placed on a \xref{rectilinear grid}, the kernel corresponding to a magnet $i$ can also be written as an $L_x \times L_y$ matrix $\vc{K}^{(i)}_{ab}$ where $ab$ denotes a position on the ASI grid.
For example, $\vc{K}^{(i)}_{2,3}$ stores the magnetostatic interaction strength between magnet $i$ and the magnet at index\footnote{
	The indexation used throughout this section starts counting at 1, as is typical for matrix notation.
	This is not to be confused with indexation in source code, which starts at 0.
} $(2,3)$ on the grid.
If no magnet was placed at index $(a,b)$ on the grid, then $\vc{K}^{(i)}_{ab} = 0, \forall i$. \\\par

However, storing such a kernel for all magnets $i$ would require storing $\order{N^2}$ values in memory. % Furthermmore, many values would be zero, though the same (but less extreme) can be said of the unit cell kernel. Note: small \vc{k} has no zeros if a lookup table is used to assign an index to each magnet.
By leveraging \link{unit cell}{unit cells}, this storage requirement can be reduced to $\order{N}$.
Each occupied grid point in the unit cell is assigned a unique index $q = 1,\dots,\widetilde{N}$, with $\widetilde{N}$ the number of magnets in a single unit cell.
Hence, each magnet in the ASI is associated with a specific value of $q$. \par
This way, all magnets with the same value of $q$ share the same layout of surrounding magnets\footnote{
	In some ASI, it is possible that different sites inside a single unit cell also experience the same surrounding layout.
	This can be used to further improve memory usage (but not performance) by only storing unique kernels.
	This was not implemented as this constitutes only a minor improvement; for the lattices in~\cref{fig:2:ASIs}, this would at best half the number of kernels.
}, apart from a different cut-off at the border in case of open boundary conditions.
Therefore, if two magnets occupy equivalent positions $q$ in the unit cell --- say, $i$ at index $(x,y)$ on the grid and $j$ at $(x+\Delta x, y+\Delta y)$ --- then
\begin{equation}
	\label{eq:2:Kernel_equivalence}
	\vc{K}^{(i)}_{ab} = \vc{K}^{(j)}_{a+\Delta x,b+\Delta y} \quad \mathrm{,} \quad \forall a,b:
	\begin{cases}
		-\Delta x < a \leq L_x - \Delta x \\
		-\Delta y < b \leq L_y - \Delta y \\
		% Or more rigorously, but even more overcomplicated:
		% \max(-\Delta x, 0) \leq a < L_x - \min(\Delta x, 0) \\
		% \max(-\Delta y, 0) \leq b < L_y - \min(\Delta y, 0) \\
	\end{cases} \quad\mathrm{.}
\end{equation}
In other words, their kernels are identical apart from an offset by $(\Delta x, \Delta y)$, at least in the area that remains inside the kernel after this offset.
Due to this equivalence, and the fact that $- L_x < \Delta x < L_x$ and $- L_y < \Delta y < L_y$, all possible interactions that a magnet at site $q$ can experience can be stored in a single $(2L_x-1) \times (2L_y-1)$ matrix $\vc{\mathcal{K}}^{(q)}_{ab}$, a \idx{unit cell kernel}.

\xfig[1.0]{2_Hotspice/Kernel_IP_Pinwheel.pdf}{
	\label{fig:2:Kernel_IP_Pinwheel}
	Two examples showing the kernels $\vc{\mathcal{K}}^{(q)}$ shifting with respect to the ASI grid for elementwise multiplication when a magnet switches.
	\textbf{Left}: the ASI itself, with thin dotted lines showing individual cells in the \link{rectilinear grid}{simulation grid} while thick solid lines delineate unit cell boundaries.
	Letters indicate the index of a magnet within its unit cell.
	\textbf{Right}: each site in the unit cell corresponds to a kernel.
	The values stored in the kernel are designed such that shifting the switching magnet to the centre of the kernel and multiplying overlapping cells ($s_{ab}$ of the ASI with $\vc{\mathcal{K}}_{ab}$ of the kernel) yields the \xref{magnetostatic interaction} energy between the switching magnet and all other magnets at positions $ab$.
	The figure shows this for two example magnets indicated in \textcolor{lightblue}{blue} and \textcolor{lightred}{red}.
}

\cref{fig:2:Kernel_IP_Pinwheel} shows the structure of such unit cell kernels $\vc{\mathcal{K}}$ to clarify the interpretation of how they store the magnetostatic interaction strength between a particular magnet and any other magnet in the system.
This is most easily illustrated by an example.
Consider the blue magnet in the $5 \times 5$ pinwheel lattice on the left side of the figure, which occupies position $q=\mathrm{A}$ in the unit cell.
The corresponding $9 \times 9$ kernel $\vc{\mathcal{K}}^\mathrm{(A)}$ is shown schematically, also in blue.
It stores the magnetostatic interactions in such a way that, when the ASI grid is shifted onto the kernel to put the blue magnet at the centre of the kernel, the interaction strength of the blue magnet with any other magnet in the ASI is stored in the kernel elements where those other magnets end up.
So, in the figure, these are the elements in kernel A shown to contain a grey magnet\footnote{
	The exact values stored at these elements are omitted as they are irrelevant to illustrate the concept. See \cref{fig:2:Kernel_PBC} for an example of the particular values stored in kernel A for a larger $16 \times 16$ lattice.
}. \par
The concept is illustrated a second time, now for the red magnet in the pinwheel ASI, which instead occupies position $q=\mathrm{B}$ in the \xref{unit cell} and therefore uses a different kernel; the red kernel $\vc{\mathcal{K}}^\mathrm{(B)}$. \\\par

This construction can be used to efficiently update the \link{magnetostatic interaction}{magnetostatic energy} $E_\mathrm{MS}$ of all magnets whenever a magnet switches.
\link{rectilinear grid}{Recall} that the grid stores the state $s_i$ of all magnets in a matrix $\vc{S}$ and the size of their magnetic moment $\mu_i$ in $\vc{\bigmu}$.
When a magnet at unit cell index $q$ switches, the change of magnetostatic energy for all other magnets can be calculated by shifting $\vc{S}$ in the same way as the ASI was shifted in \cref{fig:2:Kernel_IP_Pinwheel}, and performing a pointwise multiplication --- the \idx{Hadamard product}, denoted by $\odot$ --- with the respective elements of the kernel $\vc{\mathcal{K}}^{(q)}$.
Denoting the grid index of the switching $q$-site magnet as $(x,y)$ and using the states $\vc{S}$ after the switch, this yields a new matrix
\begin{equation}
	\label{eq:2:Kernel_update}
	\Delta \vc{E_\mathrm{MS}} = 2\mu_{xy} s_{xy}\vc{\bigmu} \odot \vc{S} \odot \ab(\vc{\mathcal{K}}^{(q)}_{ab})_{\substack{L_x < a + x \leq 2L_x \\ L_y < a + y \leq 2L_y}} \quad\mathrm{,}
\end{equation}
containing the change in magnetostatic energy $\Delta E_\mathrm{MS}$ for each grid point, which can then simply be added to the current values of $E_\mathrm{MS}$. \par
The kernel is also used to initialise the magnetostatic energy $\left. E_\mathrm{MS} \right|_{t=0}$ of each magnet at the start of the simulation.
This is done by performing a convolution ($*$) rather than a pointwise multiplication:
\begin{equation}
	\label{eq:2:Kernel_init}
	\left. \vc{E_\mathrm{MS}} \right|_{t=0} = \sum_q \vc{Q_q} \odot \vc{\bigmu} \odot \vc{S} \odot \ab((\vc{\bigmu} \odot \vc{S}) * \vc{\mathcal{K}}^{(q)}) \mathrm{,}
\end{equation}
where only the central $L_x \times L_y$ area of the convolution is calculated and the matrix $\vc{Q_q}$ contains 1 at sites with unit cell index $q$, otherwise 0.
Such a convolution can be calculated very efficiently by using the \xlabel{fast Fourier transform} (FFT). \\\par

The exact values stored in the kernel depend on whether any \link{sec:2:finite}{finite-size corrections} to the magnetostatic interaction were used and whether the ASI is of the IP or OOP type. For example, an IP kernel using the \link{point dipole model}{point dipole approximation} contains elements
\begin{equation}
	\label{eq:2:Kernel_detailed}
	\vc{\mathcal{K}}^{(q)}_{ab} = \frac{\mu_0}{4 \pi} \frac{
		u_x^{(q)} u_x^{(ab)} \ab(1 - 3 \ab(\Delta x)^2) + u_y^{(q)} u_y^{(ab)} \ab(1 - 3 \ab(\Delta y)^2) - 3\Delta x \Delta y \ab(u_x^{(q)} u_y^{(ab)} + u_x^{(ab)} u_y^{(q)})
	}{
		\sqrt{\ab(\ab(\Delta x)^2 + \ab(\Delta y)^2)^5}
	} \mathrm{,}
\end{equation}
with $\Delta x$ and $\Delta y$ the x- and y-distance between the central magnet $(q)$ and a magnet at position $(ab)$ in the kernel (if a magnet exists at the corresponding point in the ASI grid) and $\vc{u}^{(i)}$ a unit vector along the \xref{easy axis} of magnet $i$. \par
With this form for the kernel, the \xref{magnetostatic interaction} between a magnet $i$ at grid-index $(v,w)$ and another magnet $j$ at index $(v+x, w+y)$ can be calculated as
\begin{equation}
	\label{eq:2:Kernel_unitcell}
	E_{\mathrm{MS},i,j} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(q_i)}_{L_x+x, L_y+y} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(q_j)}_{L_x-x, L_y-y} \quad \mathrm{,}
\end{equation}
if the indexation of $\vc{\mathcal{K}}^{(q)}$ starts at $(1,1)$.
Note that $\mu_i$ and $\mu_j$ were not included in the \xref{unit cell kernel} to allow magnets in different unit cells but with the same unit cell index $q$ to have a different magnetic moment $\mu$.
This versatility comes at the cost of two additional elementwise multiplications per interaction, which has a non-negligible performance impact as \cref{eq:2:Kernel_unitcell} is by far the most common operation performed during a meaningful simulation. \\\par

\subsubsection{Periodic boundary conditions}
The \xref{rectilinear grid} enables the straightforward implementation of first-order \idx{periodic boundary conditions} (PBC), which account for the eight nearest replicas of the ASI --- two horizontally, two vertically and four diagonally.
An open-boundary kernel $\vc{\mathcal{K}}^{(q)}$ can be transformed into a PBC kernel by just adding eight offset copies of the kernel to itself.
Specifically, four copies are shifted along the axes by $\pm L_x$ or $\pm L_y$, while the remaining four are shifted diagonally by $(\pm L_x, \pm L_y)$.
This is illustrated in~\cref{fig:2:Kernel_PBC} for an example of pinwheel ASI. \par

\xfig[1.0]{2_Hotspice/Kernel_PBC.pdf}{
	\label{fig:2:Kernel_PBC}
	\indexlabel{open boundary conditions}
	Comparison of the \xref{magnetostatic interaction} \link{unit cell kernel}{kernel} $\vc{\mathcal{K}}^\mathrm{(A)}$ for open (left) and periodic (right) boundary conditions, for a $16 \times 16$ pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}) using the \link{point dipole model}{point dipole approximation}.
	The magnet of the \xref{unit cell} site associated to this particular kernel is shown in the centre with a black outline.
	The colour of other magnets indicates the magnitude of the magnetostatic interaction between the central and the coloured magnets if they both point `up'. This alignment is preferable for \textcolor{blue}{blue} magnets (low energy) and unstable for \textcolor{red}{red} magnets (high energy).
}

This construction works because elements near the edge of a kernel represent interactions between distant magnets on opposite sides of the ASI, which are the interactions most affected by PBC.
Since the middle region of the open-boundary \link{unit cell kernel}{kernel} already stores the interaction between nearby magnets, PBC can be applied by re-using these central values and moving the kernel by $\pm L_x$ and/or $\pm L_y$. \\\par

This approach has the advantage that PBC do not impact performance, as they are baked into the kernel, contrary to the alternative of using a circular convolution.
However, this method is limited to ASI consisting of an integer number of \xref{unit cells} as systems with truncated unit cells along an edge can not properly be tiled.
Higher-order PBC --- beyond only the 8 nearest copies --- can not be calculated based on an open-boundary kernel since it does not include interactions with such distant magnets.
Due to the rapid $1/r^3$ decline of the \xref{magnetostatic interaction} over distance, the use for such higher-order PBC would be very limited, and they were therefore not implemented.
% For very small systems, higher-order PBC can be relevant, but it is recommended to instead increase the system size to increase the fidelity of the Monte Carlo simulation.
% To explain the usage of the magnetostatic kernel, we can possibly refer to ``Real-space observation of emergent magnetic monopoles and associated Dirac strings in artificial kagome spin ice'', which notes that the energy is just a convolution of the interaction potential with the lattice sites at which there are magnets (which, for the Ewald summation, means that in Fourier space it is just a multiplication of the Fourier-transformed versions of those functions), which is pretty much what we are doing during multi-switching.

\subsubsection{Hard-axis magnetostatic kernel}
If the simulation accounts for the \xref{asymmetric energy barrier}, as detailed in \cref{sec:2:E_B_eff}, then two additional kernels are needed for each site in the unit cell. \par
The first is calculated for a situation where the central magnet maintains its usual orientation while all other magnets are rotated counter-clockwise by \ang{90}.
This kernel will be used whenever a magnet switches, to update the \link{hard-axis interaction energy}{hard-axis magnetostatic energy} $\left. E_{\mathrm{MS}} \right|_{\perp}$ of all other magnets.
The underlying equation is very similar to \cref{eq:2:Kernel_detailed}, but with $u_x^{(q)} \rightarrow -u_y^{(q)}$ and $u_y^{(q)} \rightarrow u_x^{(q)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(q)}$ in \cref{eq:2:Kernel_update}. \par
The second additional kernel is the opposite of the first one; the central magnet is rotated \ang{90} counter-clockwise while all other magnets maintain their usual orientation.
This kernel is needed only once, to calculate the initial value of $\left. E_{\mathrm{MS}} \right|_{\perp}$ for all magnets.
The underlying equation for this one is also very similar to \cref{eq:2:Kernel_detailed}, but now with $u_x^{(ab)} \rightarrow -u_y^{(ab)}$ and $u_y^{(ab)} \rightarrow u_x^{(ab)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(q)}$ in \cref{eq:2:Kernel_init}.

\subsubsection{Numerical error with cut-off kernel}
Since the kernel is nearly 4 times larger than the simulation domain, a straightforward performance improvement presents itself: truncating the \xref{magnetostatic interaction} at a certain distance.
Due to the underlying grid, the most natural way of achieving this is to reduce the size of the kernel from $2L-1 \times 2L-1$ to a smaller $2K-1 \times 2K-1$ central region, $1 < K < L$.
Note that the kernel size has to remain odd for the convolution to still place the switching magnet in the centre. \par
However, such truncation will result in inaccurate calculation of the interaction energies: whenever a magnet switches, $E_\mathrm{MS}$ of distant magnets will not be updated.
As an increasing number of magnets switch, this error will accumulate, as shown in \cref{fig:2:Kernel_cutoff} for the particular case of a pinwheel ASI with a $41 \times 41$ \xlabel{truncated kernel}.
After a certain amount of switches, however, the error is observed to stagnate.
The reason for this is that, whenever a magnet switches back, the error introduced by its original switch gets cancelled, leaving only some residual \xlabel{floating-point error} in its wake.
This also explains why the plateau starts after $\approx N$ switches ($N=5000$ magnets in the figure): at that point, each magnet will have switched once on average and will begin randomly switching back.
The error is highly dependent on the situation and size of the severity of the truncation.
The closer to equilibrium the system is initialised, the smaller the error will be while the simulation explores this equilibrium.
For example, the figure shows that the absolute error $E_\mathrm{err}$ for this particular situation with a $41 \times 41$ truncated kernel is on the order of \SIrange{5}{10}{\percent} of $E_\mathrm{MS}$.

\xfig[1.0]{2_Hotspice/Kernel_cutoff.pdf}{
	\label{fig:2:Kernel_cutoff}
	Accumulation of error in the\link{magnetostatic interaction}{magnetostatic energy} $E_\mathrm{MS}$ when using a truncated $41 \times 41$ kernel for a simulation of $100 \times 100$ pinwheel ASI at high temperature, starting from a uniform state.
	\textbf{(a)} Mean and maximum error of the magnetostatic energy $E_\mathrm{MS}$ throughout the ASI, as a function of the number of switches after initialising the system in the uniform state.
	\textbf{(b)} Final situation at the right edge of panel (a), i.e. after $\approx \SI{5e6}{}$ random switches.
	The absolute error $E_\mathrm{err}$ is the difference between the simulations with (``Approximation'') and without (``Exact $E_\mathrm{MS}$'') a truncated kernel.
}

Note that PBC require extra attention when the magnetostatic kernel is truncated, as this procedure cuts off the eight offset kernel images.
Previously, \cref{eq:2:Kernel_update} was applied when a magnet switches, where the kernel was cut-out appropriately to the size of the ASI to allow a \link{Hadamard product}{pointwise multiplication}.
With open boundaries, the truncated kernel presents no issue, but with PBC the cut-out should be applied with care, such that it properly wraps around the edges.
This wrapping can instead also be achieved by performing a \xlabel{circular convolution} of the truncated kernel with an $L_x \times L_y$ array whose only non-zero element is at the position of the switching magnet. \par
For a single switch, convolution with a truncated kernel is far slower than the original \link{Hadamard product}{pointwise multiplication} of \eqref{eq:2:Kernel_update}.
In fact, with the hardware used for this thesis, only for $\gtrapprox 20$ simultaneous switches does a convolution of truncated kernels become more performant than a sequential sum of offset non-truncated kernels.
Therefore, \hotspice allows the user to fine-tune both the size of the truncated kernel as well as this threshold between sum and convolution to optimise performance on a particular system.

\subsection{Multi-switching in Metropolis-Hastings} \label{sec:2:MultiSwitch}
\indexlabel{multi-switching}
The standard \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm} selects a magnet at random and subsequently decides whether or not to switch it.
Since this algorithm is designed primarily for sampling equilibrium states rather than capturing the temporal evolution of the system, a straightforward performance improvement can be achieved by selecting multiple magnets simultaneously rather than sequentially.
This allows for better usage of the parallel processing capabilities of the GPU, as the \link{magnetostatic interaction}{magnetostatic energy} can then be updated using a convolution. \par
Previously, when only a single magnet switched at any instant, the magnetostatic energy was updated by \cref{eq:2:Kernel_update}, which appropriately cut out a piece of the kernel for pointwise multiplication.
This process is no longer usable if we intend to use multi-switching as a method to efficiently parallelise Metropolis-Hastings iterations: not much performance can be gained if every switching magnet requires a different cut-out.
Instead, a far more efficient method to calculate the change in magnetostatic energy whenever multiple magnets switch is to perform a convolution of the kernel with an $L_x \times L_y$ array whose only non-zero elements are the values $s_i$ at the positions of the switching magnets.

\subsubsection{Minimal distance between sampled magnets} % Derive equation
Since all magnets affect each other through the magnetostatic interaction, simultaneously switching nearby magnets that significantly affect each other's \xref{switching energy} may result in unphysical behaviour.
Take for example the extreme case where it would be allowed to switch all magnets in a system at once: the total energy would not change after such a simultaneous step and an infinite loop results where the energy of the system remains high.
In a less extreme case, convergence to a low-energy or equilibrium state may be slowed down significantly. \par
Therefore, to avoid such issues, we enforce a minimum distance $\rmin$ between selected magnets.
The particular criterion we will use is as follows: two simultaneously sampled magnets should never be able to affect each other's \xref{acceptance probability} by more than some user-adjustable factor $0 < Q \leq 1$, where we commonly use $Q=0.01$.
This principle is illustrated schematically in~\cref{fig:2:MultiSwitch_proof}.
We will now derive an inequality for the minimal distance $\rmin$ as a function of $Q$.
\newtheorem{inequality}{Inequality}
\begin{inequality}
	The minimal distance $\rmin$ between magnets 1 and 2, such that the \xref{acceptance probability} $P(\Delta E)$ of one of them can not change by more than $0 < Q \leq 1$ when the other switches, is upper bounded by
	\begin{equation}
		r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \, \max(\mu^2)}{\pi Q} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}}} \triangleq \rmin \mathrm{.}
		\label{eq:2:MultiSwitch_inequality}
	\end{equation}
\end{inequality}

\xfig[1.0]{2_Hotspice/MultiSwitch_proof.pdf}{
	\label{fig:2:MultiSwitch_proof}
	Consider two states: the initial state `i' (top), and the final state `f' where magnet 1 has switched (bottom).
	The surrounding magnets stay unchanged.
	In both states, magnet 2 has a certain \xref{acceptance probability} for switching $P_2$ if it is selected by the \link{alg:2:MetropolisHastingsSingle}{Metropolis-Hastings algorithm}, determined by the associated energy change $\Delta E$.
	This switch of magnet 1 induces a change in acceptance probability $\Delta P_2$ of magnet 2.
	The limitation that we impose, is that the Metropolis-Hastings algorithm is only allowed to simultaneously sample magnet 1 and 2 if $\abs{\Delta P_2} = \abs{P_2^{(f)} - P_2^{(i)}} \leq Q$.
}

\begin{proof}
	Consider~\cref{fig:2:MultiSwitch_proof}.
	In the initial situation (top left panel), magnet 2 has an acceptance probability $P_2^{(i)}$ due to its possible change in energy $\Delta E^{(i)}$.
	Once magnet 1 switches (bottom left panel), magnet 2 has an acceptance probability $P_2^{(f)} = P_2^{(i)} + \Delta P_2$ where a switch would incur a change of $\Delta E^{(f)}$ in energy.
	Our requirement is that $\abs{\Delta P_2} \leq Q$, with $0 < Q \leq 1$.
	We therefore want to determine the minimal distance between magnets 1 and 2 such that $\abs{\Delta P_2} \leq Q$. \par
	For any function $P(\Delta E)$, we can move this constraint from $\Delta P_2$ to the more manageable quantity $\Delta E^{(f)} - \Delta E^{(i)}$ by writing the upper bound
	\begin{equation}
		\abs{\Delta P} \leq \abs{\Delta E^{(f)} - \Delta E^{(i)}} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}} \leq Q \mathrm{.}
		\label{eq:2:MultiSwitch_proof_leq_Q}
	\end{equation}
	The only long-range interaction present in the systems under consideration here is the \xref{magnetostatic interaction} given by \eqref{eq:2:E_MS}, repeated here for convenience,
	\begin{equation*}
		E_{\mathrm{MS},1,2} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_1 \bcdot \vc{\mu}_2}{\abs{\vc{r}_{12}}^3} - \frac{3(\vc{\mu}_1 \bcdot \vc{r}_{12}) (\vc{\mu}_2 \bcdot \vc{r}_{12})}{\abs{\vc{r}_{12}}^5}) \mathrm{.}  \tag{\ref*{eq:2:E_MS}}
	\end{equation*}
	We do not consider any \link{sec:2:finite}{finite-size corrections}, since their long-range effects are negligible and any reasonable value of $Q$ will result in a value $\rmin$ far larger than the magnet size.
	The magnetostatic energy in the \link{point dipole model}{point-dipole approximation} is bounded by
	\begin{equation}
		\abs{E_{\mathrm{MS},1,2}} \leq \frac{\mu_0 \abs{\vc{\mu}_1} \abs{\vc{\mu}_2}}{2 \pi r_{12}^3} \mathrm{,}
		\label{eq:2:MultiSwitch_proof_EMS_leq}
	\end{equation}
	with this extremum being achieved for two dipoles $\vc{\mu}_1$ and $\vc{\mu}_2$ aligned in the same direction along their common axis $\vc{r}_{12}$.
	From the symmetry of the magnetostatic interaction, and considering the states in~\cref{fig:2:MultiSwitch_proof}, the unknown term in~\cref{eq:2:MultiSwitch_proof_leq_Q} can be expanded as 
	\begin{align*}
		\abs{\Delta E^{(f)} - \Delta E^{(i)}} &= \abs{-\cancel{2 \sum_{k=3}^N E_{\mathrm{MS},2,k}^{(f)}} -2 E_{\mathrm{MS},2,1}^{(f)} +\cancel{2 \sum_{k=3}^N E_{\mathrm{MS},2,k}^{(i)}} +2 E_{\mathrm{MS},2,1}^{(i)}} \\
		&= 2\abs{E_{\mathrm{MS},2,1}^{(i)} - E_{\mathrm{MS},2,1}^{(f)}} = 4 \abs{E_{\mathrm{MS},2,1}^{(i)}} \numeq{\ref{eq:2:MultiSwitch_proof_EMS_leq}}{\leq} \frac{2 \mu_0 \abs{\vc{\mu}_1} \abs{\vc{\mu}_2}}{\pi r_{12}^3} \mathrm{,}
	\end{align*}
	since only magnet 1 is different between the initial and final states, hence $E_{\mathrm{MS},2,1}^{(f)} = - E_{\mathrm{MS},2,1}^{(i)}$.
	Combining this with \eqref{eq:2:MultiSwitch_proof_leq_Q} gives the sufficient condition
	\begin{equation}
		\frac{2 \mu_0 \abs{\vc{\mu}_1} \abs{\vc{\mu}_2}}{\pi r_{12}^3} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}} \leq Q \mathrm{.}
	\end{equation}
	To get a single value for $\rmin$ that is valid throughout the ASI, replacing $\abs{\vc{\mu}_1} \abs{\vc{\mu}_2}$ by $\max(\mu^2)$ finally yields the expected expression for $\rmin$:
	\begin{equation*}
		r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \, \max(\mu^2)}{\pi Q} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}}} \triangleq \rmin \mathrm{.} \tag{\ref*{eq:2:MultiSwitch_inequality}}
	\end{equation*}
\end{proof}
This equation is an upper bound for $\rmin$ throughout the entire ASI.
To apply this inequality in \hotspice, all that remains is to plug in the derivative of the \link{Metropolis-Hastings sampling}{Metropolis-Hastings} \xref{acceptance probability} function $P_\mathrm{MH}(\Delta E, T) = \min(1, \exp(-\Delta E/k_B T))$, into~\cref{eq:2:MultiSwitch_inequality}.
Since
\begin{equation}
	\abs{\frac{\partial P_\mathrm{MH}}{\partial \Delta E}} = \left.\begin{cases}
		\frac{1}{k_B T} \exp(\frac{-\Delta E}{k_B T}) \quad &\mathrm{if} \quad \Delta E > 0 \\
		0 \quad &\mathrm{if} \quad \Delta E < 0
	\end{cases} \right\}
	\leq \frac{1}{k_B T} \mathrm{,}
\end{equation}
this finally gives
\begin{equation}
	r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \max(\mu^2)}{\pi Q k_B T}} = \rmin \mathrm{.}
\end{equation}
Recall that we are using the Metropolis-Hastings acceptance probability in \hotspice's rejection-based \xref{kinetic Monte Carlo} algorithm (\cref{sec:2:Dynamics_MH}) rather than the \xref{Glauber dynamics} acceptance probability $P_\mathrm{GD}(\Delta E, T) = \frac{\exp(-\Delta E/k_B T)}{1+\exp(-\Delta E/k_B T)}$, because they both satisfy \xref{ergodicity} and \xref{detailed balance} but the Glauber acceptance probability is always more likely to reject a switch~\cite{bit-player_MCvsGlauber}.
However, we can now see that the Glauber acceptance probability is not all bad when \xref{multi-switching} is used, as the extremum of its derivative is 4 times smaller than for the Metropolis-Hastings acceptance probability.
Hence, \cref{eq:2:MultiSwitch_inequality} tells us that the minimal distance between samples with Glauber dynamics can be a factor $\sqrt[3]{1/4} \approx 0.63$ smaller than with Metropolis-Hastings.
Consequently, $\sqrt[3]{16} \approx 2.52$ times more magnets can switch at once, so it is not as clear-cut which acceptance probability is more efficient in a given situation.

\subsubsection{Selection algorithms}\indexlabel{selection algorithm}
Now that we know how far apart simultaneously sampled magnets must be spaced, we can turn our attention to the question of how to sample them.
Several methods are available to achieve this, whose respective benefits and drawbacks will be explored in the following paragraphs. \par
An open question is whether \xref{multi-switching} could induce non-physical correlations into the system, as there will always be preferential directions or distances between samples.
Since this can be hard to assess in the general case, the spatial distribution of each sampling method will be compared visually in~\crefrange{fig:2:MultiSwitch_select_Poisson}{fig:2:MultiSwitch_select_Hybrid} to gain insight into the basic properties of their spatial distribution.
At present, we have encountered no sign of non-physical phenomena due to multi-switching.
The comparison with analytical solutions in~\cref{sec:2:Verification} while using a high $Q$-value (low $\rmin$) instils confidence that this distribution does not significantly affect the behaviour of the ASI in most cases.

\paragraph{Poisson disc}
The problem of distributing points in an area is a topic of interest in computer graphics, where \idx{blue noise sampling} is frequently used to generate randomised uniform distributions~\cite{BlueNoiseSurvey}. % REF: elucidates meaning of 'blue noise' and has several nice methods to compare such point sets (i.e., just showing them, voronoi tesselation with coloring of valence (number of neighbors) of each cell, Delaunay triangulation, power spectrum, radial means & anisotropy, Zone plate pattern (also used in~\cite{PoissonDiskParallel}), also see~\href{https://observablehq.com/@fil/poisson-distribution-generators}{this Observable post} for a nice comparison attempt).
While not all blue noise point sets satisfy a minimal distance requirement, \idx{Poisson disc sampling} is a particular class of blue noise sampling where a minimal distance requirement has to be met.
Various algorithms for Poisson disc sampling exist, all yielding slightly different spatial distributions~\cite{EfficientBlueNoisePointSets,PoissonDiskComparison,SamplingPolyominoes}.
Typically, these algorithms sequentially add points to an area, rejecting new points if they are too close to an existing point. \par
An efficient implementation is \idx{Bridson's algorithm}~\cite{FastPoissonDiskSampling}.
In essence, this algorithm tracks a set of ``seed'' points (starting with a randomly placed seed), attempts to place $k$ random points in an annulus at a distance $\rmin \leq r \leq 2 \rmin$ from each seed point, and accepts whichever points are sufficiently far from all other points.
To efficiently check for nearby points, an underlying grid of cell size $r/\sqrt{2}$ is used.
The new points then become the seed points, while the previous seed points become inactive, and the process repeats until no seed points are left.
Several variants have been proposed over the years to further improve performance, such as Roberts' adjustment to the distribution within the annulus~\cite{PoissonRoberts}.
Using Bridson's algorithm, $k=4$ yields on average half as many samples as an optimal close packing would achieve, and in \hotspice this results in the highest number of samples per second when including the time required to perform all other ASI calculations during a \link{alg:2:MetropolisHastingsSingle}{Monte Carlo iteration}. \par
Insight into the spatial distribution of this algorithm can be gained from~\cref{fig:2:MultiSwitch_select_Poisson}, where Bridson's algorithm was used to sample magnets in an OOP square-lattice ASI.
Using the algorithm on sparser ASI is non-trivial because they contain unoccupied cells.
These cells should not be sampled, as this would significantly reduce the number of simultaneously sampled magnets.
Therefore, rather than randomly placing a point inside the annulus at an integer coordinate, a random magnet should be selected within this annulus instead, which is more computationally demanding.

\xfig[1.0]{2_Hotspice/MultiSwitch_select_Poisson.pdf}{
	\label{fig:2:MultiSwitch_select_Poisson}
	Characteristics of \link{Bridson's algorithm}{Bridson's} \xref{Poisson disc sampling} algorithm with PBC, based on $\approx \SI{e6}{}$ samples.
	\textbf{(a)} Probability density of \link{multi-switching}{simultaneously sampling} the magnet at the centre of the figure (white circle) and at any other point in the figure.
	The imposed minimal distance $\rmin=16$ cells is indicated by the dashed white circle.
	\textbf{(b)} Binned (cumulative) probability density of nearest-neighbour distance for simultaneously sampled points.
	\textbf{(c)} Sample distribution over the simulation domain. This must not display any obvious pattern.
	\textbf{(d)} \link{periodogram}{Periodogram}, i.e. spatial Fourier transform of simultaneously sampled magnets, averaged over all iterations of the selection algorithm.
}

\crefSubFigRef{fig:2:MultiSwitch_select_Poisson}{a} shows the probability density of sampling a point in the plane, if a point is also sampled at the centre in the small white circle.
The imposed minimal distance $\rmin$, indicated by the white dashed circle, and is clearly respected.
The distribution is highly isotropic, apart from some discretisation effects due to the \link{rectilinear grid}{discrete nature} of the ASI.
In the same vein, \crefSubFigRef{fig:2:MultiSwitch_select_Poisson}{b} shows the (cumulative) probability density of having a nearest neighbour at a certain distance.
Nearly all nearest neighbours are at a distance $\rmin \leq r \leq 2\rmin$, as expected from Bridson's algorithm.
The probability density is rather spiky, due to the sampling occurring on a rectilinear grid in \hotspice.
\crefSubFigRef{fig:2:MultiSwitch_select_Poisson}{c} checks that the sampling method does not sample any part of the ASI at a higher rate than another part, which result in a violation of detailed balance.
Luckily, this panel is very boring, so detailed balance is still satisfied.
Finally, \crefSubFigRef{fig:2:MultiSwitch_select_Poisson}{d} shows the average spatial Fourier spectrum of a set of sampled points, called a \idx{periodogram}.
This metric is often used to compare the various methods for generating Poisson disc distributions~\cite{PoissonDiskComparison}.
An ideal blue noise point set exhibits weak low-frequency energy~\cite{EfficientBlueNoisePointSets}, as is the case here in the centre of the periodogram.
An cross-shaped artefact is visible in the centre, once again due to the \xref{rectilinear grid} upon which \hotspice samples, which is not expected for continuous Poisson disc distributions. \par
However, while its distribution shown in~\cref{fig:2:MultiSwitch_select_Poisson} is very smooth, Poisson disk sampling is rather slow due to its sequential nature, which defeats the purpose of this whole \xref{multi-switching} endeavour.
While a parallel version of this algorithm exists~\cite{PoissonDiskParallel}, it is nontrivial to apply to our situation --- points on a rectilinear grid, possibly with PBC --- without violating the $r \geq \rmin$ constraint. % It would be useful to consider an $L \cross L$ grid, and then do the multiscale technique the paper describes.
If this can be implemented, the number of sequential operations could be reduced from $\order{N}$ to $\order{\log(N)}$, with $N$ the number of samples, making Poisson disk sampling a viable strategy.

\paragraph{Restricted stratified jittered grid}
To improve performance, we can use a restricted ``\xlabel{stratified jittered grid}'' (SJG) selection algorithm.
Normal SJG sampling divides the domain into strata --- typically squares --- and selects a random point in each \xlabel{stratum}~\cite{ProgressiveMultiJittered}.
Since this procedure does not guarantee a minimal distance between samples, a restricted SJG is used instead, where samples are only taken from from the centre area of each square-shaped stratum, as discussed and illustrated in~\ccite{SamplingPolyominoes}. \par % Fig. 3b in SamplingPolyominoes is restricted SJG
To apply this in \hotspice, the simulation is divided into square subregions of $R_x \times R_y$ grid cells, with $R_x$ and $R_y$ chosen such that each subregion has a physical size of at least $\rmin \times \rmin$.
These subregions form a ``super-grid'' throughout the simulation domain.
A quarter of the subregions are then chosen such that they are non-adjacent --- sharing no edges nor corners --- and a single magnet is sampled from each of them. % Q: do we need a simple figure for this? And for the hybrid method?
This ensures that these sampled magnets are at least a distance $\rmin$ apart, though they will have an average spacing of $2\rmin$.
By choosing a random quarter of subregions each time, all magnets are equally likely to be chosen.
\vspace{-3em}
\xfig[1.0]{2_Hotspice/MultiSwitch_select_Grid.pdf}{
	\label{fig:2:MultiSwitch_select_Grid}
	Characteristics of the restricted \xref{stratified jittered grid} selection algorithm with PBC, based on $\approx \SI{e6}{}$ samples.
	\textbf{(a)} Probability density of \link{multi-switching}{simultaneously sampling} the magnet at the centre of the figure (white circle) and at any other point in the figure.
	The imposed minimal distance $\rmin=16$ cells is indicated by the dashed white circle.
	\textbf{(b)} Binned (cumulative) probability density of nearest-neighbour distance for simultaneously sampled points.
	\textbf{(c)} Sample distribution over the simulation domain. This must not display any obvious pattern.
	\textbf{(d)} \link{periodogram}{Periodogram}, i.e. spatial Fourier transform of simultaneously sampled magnets, averaged over all iterations of the selection algorithm.
}

Similar to the figure for Poisson disc sampling,~\cref{fig:2:MultiSwitch_select_Grid} shows an analysis of the spatial distribution of the restricted SJG method.
The contrast with \xref{Poisson disc sampling} is striking, as was to be expected.
The spatial distribution and \xref{periodogram} are highly anisotropic, with magnets being most likely to be sampled $(2iR_x, 2jR_y), \forall i,j$ relative to each other.
This may lead to undesirable correlations in the system, though such effects have not been observed at present, even for very low $\rmin$.
The distance to the nearest neighbours is on average slightly further than for Poisson disc sampling, but is still mostly concentrated between $\rmin \leq r \leq 2\rmin$.
\crefSubFigRef{fig:2:MultiSwitch_select_Grid}{c} shows that all cells in the simulation are equally likely to be sampled with this method, once again ensuring detailed balance is satisfied.
However, achieving uniform sampling required additional attention in the case of PBC.
This, and several other edge cases or considerations that had to be addressed to implement this algorithm in \hotspice, are listed below.
\begin{itemize}
	\item Special attention must be given to PBC with this method.
	Consider for example a situation where an odd number of subregions exist along one axis, resulting in simultaneous sampling in the first and last subregions along this axis.
	With PBC, this may result in \link{multi-switching}{simultaneous sampling} of magnets which are physically closer than $\rmin$, but computationally separated by the edge.
	To prevent this, the super-grid of subregions is taken to be slightly smaller than the ASI.
	In the example, this would mean that the last subregions along the odd axis are not used.
	To ensure uniform coverage and proper sampling near the edges, the super-grid then has to be randomly shifted over the simulation domain and wrap around the edges.
	Note that PBC can make it impossible to maintain a spacing $>r$ in very small systems, or with small $Q$ values; in such cases, we resort to selecting a single magnet.
	\item The size of the subregions must be lower bounded by $R_x \geq 2$ and $R_y \geq 2$ to avoid forming artefacts in highly ordered systems.
	In the extreme case where $R_x = R_y = 1$, nearly all randomness would be removed, and only 4 possible multi-samples would remain (one for each quarter of subregions).
	Think for example of an \link{exchange coupling}{exchange-coupled} OOP square ASI in the uniform state: a regularly-spaced quarter of all magnets would switch simultaneously, without introducing additional spatial variation, which would be non-physical.
	Avoiding this situation retains sufficient randomness for small $\rmin$.
	\item The size of a subregion is constrained to an integer amount of \xref{unit cells}, such that all subregions have an identical pattern of (un)occupied grid cells within them.
	This way, the sampling of magnets can be done in parallel for all subregions without sampling any unoccupied grid points.
	\item Whereas \xref{Poisson disc sampling} can in theory account for a spatially dependent $\rmin$ (e.g., due to a local variation of $\mu$), this is not possible with the restricted SJG, so a worst-case $\rmin$ across the lattice has to be assumed (as in the derivation of~\cref{eq:2:MultiSwitch_inequality}).
\end{itemize}

In some sense, this method is the opposite of Poisson disc sampling: it is less versatile, sparser and highly anisotropic, but is efficient to compute in parallel --- the number of sequential operations is independent of the number of simultaneous samples --- and synergises well with our \link{rectilinear grid}{grid-based} ASI implementation.
While not entirely random, this restricted SJG sampling method has shown satisfactory results in maintaining physically correct ASI behaviour, with minimal artefacts in systems with complex long-range correlations.
This strategy effectively balances efficiency and randomness, reducing complexity and maintaining performance, especially in large simulations.

\paragraph{Hybrid grid-Poisson}
The \xref{Poisson disc sampling} may have the most natural distribution, but is at least an order of magnitude slower than the \link{stratified jittered grid}{restricted stratified jittered grid}.
A hybrid approach between these two methods may yield a relatively smooth distribution while still being quite fast.
This was implemented by selecting subregions using the Poisson disc algorithm, rather than simply selecting a regularly spaced quarter of subregions.
An adapted version of the Poisson disc sampling was used for this purpose, tailored specifically for selecting non-adjacent subregions.
Once the subregions have been selected, everything proceeds as in the restricted SJG method: magnets are sampled in parallel, one in each subregion, making use of the fact that the occupation of the \link{rectilinear grid}{underlying grid} is identical for each subregion. \par
Similar to the previous figures,~\cref{fig:2:MultiSwitch_select_Hybrid} shows the spatial distribution of this hybrid ``grid-Poisson'' method.

\xfig[1.0]{2_Hotspice/MultiSwitch_select_Hybrid.pdf}{
	\label{fig:2:MultiSwitch_select_Hybrid}
	Characteristics of the hybrid grid-Poisson selection algorithm with PBC, based on $\approx \SI{e6}{}$ samples.
	\textbf{(a)} Probability density of \link{multi-switching}{simultaneously sampling} the magnet at the centre of the figure (white circle) and at any other point in the figure.
	The imposed minimal distance $\rmin=16$ cells is indicated by the dashed white circle.
	\textbf{(b)} Binned (cumulative) probability density of nearest-neighbour distance for simultaneously sampled points.
	\textbf{(c)} Sample distribution over the simulation domain. This must not display any obvious pattern.
	\textbf{(d)} \link{periodogram}{Periodogram}, i.e. spatial Fourier transform of simultaneously sampled magnets, averaged over all iterations of the selection algorithm.
}

The sample distribution visible in~\crefSubFigRef{fig:2:MultiSwitch_select_Hybrid}{a} is in every way a hybrid of the spatial distribution of both earlier methods.
The average distance to a nearest neighbour is, however, larger than both methods, and now extends up to $r \leq 3 \rmin$, resulting in a lower amount of magnets being sampled simultaneously. \par
This hybrid grid-Poisson method is half as fast as the restricted SJG method, which is still a substantial improvement over Poisson disc sampling --- due to the optimised Poisson sampler for non-adjacent subregions --- and has a more natural-looking distribution of neighbouring magnets than the grid-select method.
As such, it is up to user preference which method to use, with modified SJG being the default in \hotspice to prioritise performance.

\subsection{Performance}
\subsubsection{History}
The performance of \hotspice has been improved throughout development in various ways. \par
At an early stage of development, the implementation of \hotspice was adjusted to allow calculations to be performed on either a \xlabel{central processing unit} (CPU) or a \xlabel{graphics processing unit} (GPU).
Both types of processor are typically present in modern computers and are designed for different tasks.
The CPU can execute diverse, sequential tasks in rapid succession, while the GPU is optimised for parallel computing and therefore excels at performing the same calculation across a large amount of data simultaneously~\cite{owens2008gpu}.
In our simulations, the number of magnets in the ASI has a major impact on performance, and often forms the determining factor as to whether calculation on the GPU rather than CPU will result in a faster simulation~\cite{lee2010debunking}. \\\par
In the following paragraphs, we present a chronological overview of various improvements to the calculation of the magnetostatic interaction, with a performance comparison between CPU and GPU for the first and final methods.
Two tables are shown:~\cref{tab:2:perf_init} for the initialisation time, and~\cref{tab:2:perf_switch} for the time it takes to perform 5000 switches using the \xref{first-switch method}, taking into account the \xref{magnetostatic interaction} between all magnets.
During initialisation, the \link{unit cell kernel}{magnetostatic kernel} is constructed, after which the starting energy $E_i$ of all magnets is calculated.
In the tables, only the last 3 rows use the kernel as it was explained in~\cref{sec:2:Kernels}; the other rows use less efficient kernels as explained below. \\\par % Can't really call it a "lookup table" if it is being convolved etc., right?

\paragraph{Hardware}\label{sec:2:hardware}\label{hardware}
This benchmark, as well as all other simulations throughout this thesis, were performed on an NVIDIA GeForce RTX 3080 Mobile GPU and/or an 11th Gen Intel\textregistered{} Core\texttrademark{} i7-11800H @ 2.30GHz.
Simulation times displayed serve an illustrative purpose: absolute values on other hardware may vary significantly, though the general trends within the tables should remain similar.

% Q: should we use L or N=L²? I would prefer L because at some point we talk about 2D indexation, so L makes it clearer that we have a 2D system.
\xtable[tab:2:perf_init]{\textbf{Initialisation time} for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 0.5s & 6.8s & 34.0s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 2.2s & 5.4s & 11.9s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 1.0s & 3.4s & 7.6s & 14.5s & 91.6s & ? \\
		\hline
		GPU | \makecell{Unit cell kernel (see \S\ref{sec:2:Kernels})} & 2.4s & 4.6s & 9.7s & 16.3s & 66.2s & 730.4s \\
		\hline \hline
		GPU | \makecell{Final version\\(convolution initialisation)} & 1.0s & 1.0s & 1.0s & 1.0s & 1.6s & 23.9s \\
		\hline
		CPU | \makecell{Final version\\(convolution initialisation)} & 0.014s & 0.2s & 1.0s & 3.2s & 53.1s & ? \\
		\hline
	\end{tabular}
}

\xtable[tab:2:perf_switch]{\textbf{Time for 5000 switches} using the \xref{first-switch method}, for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 9.4s & 217.6s & ? & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 5.0s & 13.6s & 51.8s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 7.1s & 7.2s & 8.2s & 9.0s & 13.0s & ? \\
		\hline
		GPU | \makecell{Unit cell kernel (see \S\ref{sec:2:Kernels})} & 7.8s & 7.9s & 8.3s & 8.9s & 11.8s & 36.9s \\
		\hline \hline
		GPU | \makecell{Final version\\(2D indexation)} & 6.8s & 7.0s & 7.2s & 7.6s & 10.2s & 35.9s \\
		\hline
		CPU | \makecell{Final version\\(2D indexation)} & 0.56s & 1.3s & 2.5s & 4.3s & 41s & ? \\
		\hline
	\end{tabular}
}

\paragraph{From CPU to GPU}
Originally, \hotspice performed all calculations on the CPU. % Ing the beninging
This caused the simulation time to rise dramatically as the system size grew (e.g. first row in \cref{tab:2:perf_switch}).
For early versions of \hotspice with an unoptimised kernel, this made it impractical to simulate systems larger than $50 \times 50$ magnets.
By simply switching to GPU-based calculations using the \python{CuPy} library (second row), this upper limit was increased significantly.
The limiting factor instead became memory consumption of the unoptimised kernel. \par
Do note, however, that for small systems the CPU outperforms the GPU.
This remains true independent of the kernel implementation.
GPUs are optimised for parallel processing, and therefore reach a bottleneck when the simulation takes on a more sequential nature~\cite{owens2008gpu}.
Simulating a single switch requires several distinct operations that do not lend themselves to parallelisation.
When operating on small arrays, this means relatively fewer parallel calculations can be performed, thereby increasing the importance of being able to perform distinct mathematical operations in quick succession.
The CPU excels at the latter.

\paragraph{Kernel improvements}
The first two rows \textit{``Pairwise kernel''} in the tables use the na\"ive kernels $\vc{k}$ (or, equivalently, $\vc{K}$) described at the start of \cref{sec:2:Kernels}, arranged into a big $N \times N$ matrix $\vc{D}$ storing the \xref{magnetostatic interaction} between each pair of magnets.
This allowed easy calculation of $E_\mathrm{MC}$ for each magnet by a single matrix product $\vc{D}\vc{s}$ with the column vector $\vc{s}=\{s_i\}$.
However, this was very inefficient: the initialisation time on CPU (first row in~\cref{tab:2:perf_init}) bears witness to the size of this matrix, as $t_\mathrm{init} \propto L^4 = N^2$.
On GPU, parallelisation of the calculation of this matrix $\vc{D}$ approximately reduces this to $\propto L^2 = N$. \par
Clearly, for large systems, the need to store $\order{N^2}$ elements quickly becomes prohibitive.
For a system as small as $170 \times 170$, this type of kernel already exceeded the \SI{8}{\giga\byte} of available memory, as indicated in the table by ``Mem''.
For periodic lattices, the matrix will contain many identical values, wasting a lot of memory.
While it was possible to alleviate this by instead using a sparse matrix and limiting the interaction distance, memory usage was not the only issue: also the performance per switch (\cref{tab:2:perf_switch}) was rapidly declining for increasing $N$, both on CPU and GPU. % Also: sparse matrices probably have more indexation overhead
Clearly, a more efficient approach was required. \\\par

An extreme solution is to use no kernel at all, as done in the third row \textit{``No kernel''}.
Instead, for every switching magnet, its interaction energy with all other magnets was calculated from scratch.
The tables show that this clearly gets rid of the memory issue and yields a surprisingly fast simulation.
However, the initialisation for larger systems now takes a long time, because without a kernel this requires $\order{N^2}$ operations resulting in $t_\mathrm{init} \propto N=L^2$ on GPU for small systems.
Still, in all situations it is beneficial not to use the pairwise kernel. \\\par

To reduce the initialisation time, two improvements had to be combined. \par
First, a \xref{unit cell kernel} as described earlier in~\cref{sec:2:Kernels} had to be implemented, which overcomes the memory issue by using the periodic nature of most ASI to only store interactions for a single \xref{unit cell}.
This is used in the fourth row \textit{``Unit cell kernel''}, but does not yield a significant improvement on its own aside from enabling $1000 \times 1000$ systems for the first time. \par
Secondly, however, this new kernel enables the usage of a convolution to initialise the magnetostatic interaction energy $E_\mathrm{MS}$, rather than the sequential sum used before.
This is what the final two rows \textit{``Final version''} use, where we once again compare CPU and GPU as this is the final version.
Another minor improvement in the final version was to use 2D indexation (rather than a flat index $i = L_x y + x$ which did not synergise well with the convolution), yielding another \SI{15}{\percent} performance increase.

\paragraph{Final performance}
In the end, the upper limit for feasible calculation time has become $L \approx 1000$ on GPU and $L \approx 400$ on CPU.
Comparing the last two rows of \cref{tab:2:perf_switch} reveals that GPU outperforms CPU for $L \gtrapprox 300$.
However, this is for the \xref{first-switch method} which only performs a single switch at a time.
Performing multiple switches simultaneously in Metropolis-Hastings benefits greatly from the \xref{unit cell kernel}, as it can use the same principle of convolution as was already used in the initialisation.
With this algorithm, the GPU can maintain the advantage for systems as small as $L \gtrapprox 60$.

\subsubsection{Multi-switching}
The \xref{first-switch method}, used in the preceding discussion about performance, does not implement a \xref{multi-switching} procedure.
This makes it very inefficient for large systems: for a system of $N$ magnets to change its macrostate significantly, on the order of $N$ switches must occur.
Even though the time required to perform a single switch with the first-switch method stays roughly constant up to $L<400$, the time per \xref{Monte Carlo sweep} will scale linearly with the number of magnets if each iteration only switches a single magnet.
The ability to perform \link{multi-switching}{multiple switches} simultaneously during \xref{Metropolis-Hastings sampling} alleviates this problem, allowing for a significantly higher amount of Monte Carlo sweeps per second. \\\par

\cref{fig:2:Performance} shows the performance of the multi-switching algorithm as a function of system size, both for calculation on the CPU (a) and GPU (b).

\vspace{-1em}
\xfigsnocap[0.47]{2_Hotspice/Performance_CPU.pdf}{2_Hotspice/Performance_GPU.pdf}{
	Performance of OOP square ASI as a function of system size, on \textbf{(a)} CPU and \textbf{(b)} GPU. The Metropolis-Hastings algorithm was used with \xref{multi-switching} $Q=0.05$, lattice spacing $a = \SI{1}{\micro\metre}$, temperature $T = \SI{100}{\kelvin}$ and energy barrier $\EB = 0$.
	\label{fig:2:Performance}
} % Q: Performance for IP ASI might be worse. Add a Pinwheel figure as well?

The multi-switching algorithm selects a certain number of magnets per second, as depicted by the blue curve: these are candidates for switching.
Dividing this value by $N$, the number of magnets in the system, gives the number of \xref{Monte Carlo sweeps} per second (MCS/s), depicted by the black curve.
This is the main performance metric in Monte Carlo simulations. \par
Only a subset of the selected magnets will switch, as depicted by the red curve.
This switching rate is highly dependent on the specific conditions of the simulation --- particularly the ratio $\EBeff/T$ --- making general statements infeasible.
There exist situations where no magnets switch or where any selected magnet switches.
In the figure, values of $a$, $T$, and $\EB$ were chosen which result in a reasonable \xref{switching rate}. \\\par

There is once again a stark contrast between CPU and GPU.
On CPU, the sampling rate starts off rather constant for small systems, as overhead dominates.
For $N>100$ magnets, the sampling rate increases proportional to $N$, leading to a stable performance independent of $N$.
Beyond $\approx 80 \times 80$ magnets, however, performance drops dramatically. % While this is reminiscent of a lack of garbage collection while building the figure, this is not the case here as the drop remains even when starting the figure at $L=80$.
We hypothesise that this is due to exceeding a CPU cache size. \par
%On the CPU used for~\cref{fig:2:Performance} (the aforementioned i7-11800H), the L1 and L2 cache hold \SI{48}{\kilo\byte} and \SI{1.25}{\mega\byte}, respectively.
% Q: Are any of the below hypotheses reasonable? Which is most reasonable?
% Hypothesis 1: The size occupied by a 72x72 ASI is 1.25MB. It is not unreasonable that, soon after reaching this point, some arrays can no longer be stored on this cache level, impacting performance.
% Hypothesis 2: The size of mm.m at 78x78 is 48kB, so we might be exceeding the L1 cache beyond this point.
% Hypothesis 3: The size of the dipole kernel at this point is around 220kB. Doesn't seem to match up with anything.
% TODO: ? revisit this at some point, it is odd that on another CPU (Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz) the cut-off happens at the same system size, indicating that it is not cache-size-related because that CPU has 256kB L2 and 32kB L1, both different from the UGent laptop.
The GPU, on the other hand, hits its stride for exactly those systems that are too large for the CPU to handle.
Around $70 \times 70$, the sampling rate increases rapidly, resulting in an increased number of MCS per second with optimal performance achieved for $200 \times 200$ magnets.
Beyond this, the sampling rate stagnates, eventually settling at around \SI{150e3}{samples\per\second}, corresponding to an ever decreasing MCS/s as the parallelism of the GPU gets exhausted.

\subsubsection{RNG}
Another factor which could affect performance is the choice of random number generator (RNG), since both update algorithms rely on generating a random numbers to select the next magnet(s) to switch.
For the \xref{first-switch method}, each switch requires $\order{N}$ random numbers --- one random switching time for each magnet.
\xref{Metropolis-Hastings sampling}, on the other hand, only requires $\order{1}$ random numbers per switch --- randomly selecting one or multiple magnets and randomly switching them according to their \xref{acceptance probability}.
Both of these algorithms use exponentially distributed random numbers, which can easily be generated from a uniformly distributed random number $\chi$ by the transformation $-\ln{\chi}$. \par
On CPU, \hotspice uses\footnote{
	On CPU, \hotspice uses \python{numpy.random.default_rng}, which uses the PCG64 generator for \python{numpy}~\cite{NumPy} v1.17 and up.
} the PCG64~\cite{PCG64} generator.
For GPU calculations, NVIDIA's \code{cuRAND} library\footnote{
	On GPU, \hotspice uses \python{cupy.random.default_rng}, which itself uses the \code{cuRAND} library.
} provides several random number generators, of which the XORWOW~\cite{XORWOW} generator is used.
The performance of these various types of GPU RNG depends on the amount of random values being generated in parallel (i.e., the size of the ASI) and the details of the GPU architecture used~\cite{RNG_GPU_evaluation}.
In \hotspice, no significant effect on performance has been noticed by changing the RNG method.

\subsection{Package structure}\label{sec:2:API} % Q: should I put all the information of this section earlier already, at points where these subjects are discussed? I could e.g. create a custom "API" environment to explain different parts of the package in to show code outside the main text.
Hotspice\footnote{
	\hotspice is an abbreviation of ``hot spin ice'', for temperature plays a central role in the \xref{kinetic Monte Carlo} algorithms used for simulating artificial spin ice.
} is written as a Python 3.10 package and can perform simulations on either the CPU or GPU.
The optimal \xref{hardware} choice depends on the size of the ASI and the \xref{update algorithm} used.
By default, \hotspice runs on the CPU using the popular NumPy~\cite{NumPy} and SciPy~\cite{SciPy} libraries.
For GPU-accelerated array manipulation, the CuPy v11.4~\cite{CuPy} library is used, but this is an optional dependency. \par
To run on the GPU, the environment variable \python{HOTSPICE_USE_GPU} must be set to \python{"true"} before the the \hotspice package is loaded in a Python script with \python{import hotspice}.
It is not possible to switch between GPU/CPU within a script.
This behaviour is controlled by the \textbf{\python{hotspice.config}} module.

\paragraph{ASI and energies}
The \textbf{\python{hotspice.Magnets}} class implements all the core functionality of a simulation.
It controls the various technical aspects of the topics discussed earlier, like the \xref{multi-switching} methods, choice of \link{update algorithm}{KMC algorithm} and the calculation of the \xref{effective energy barrier} $\EBeff$, as well as more technical parameters such as the size of the \xref{truncated kernel}. \par
To create an ASI, several attributes of the \python{Magnets} class have to be set appropriately.
To this end, the \textbf{\python{hotspice.ASI}} module provides two abstract classes, \python{IP_ASI} and \python{OOP_ASI}, which can be used to define a particular ASI lattice through inheritance, as is done for the built-in lattices shown in~\cref{fig:2:ASIs}.
Two parameters suffice to make a rudimentary ASI: the lattice parameter $a$ (the red indicator in~\cref{fig:2:ASIs}) and the size of the underlying grid $L_x \times L_y$. \par
The three \xref{energy contributions} in \eqref{eq:2:E} are provided in the \textbf{\python{hotspice.energies}} module and can be accessed from the main \python{hotspice} namespace.
Choosing between the \link{finite dipole model}{dipole} and \link{dumbbell model}{monopole} representations for the \link{magnetostatic interaction}{magnetostatic energy} is currently done by respectivley using either the \python{DipolarEnergy} or \python{DiMonopolarEnergy} classes.
By default, an ASI object uses only a \python{DipolarEnergy}.
When relevant, the user has to explicitly add a \link{Zeeman energy}{\python{ZeemanEnergy}} or \link{exchange coupling}{\python{ExchangeEnergy}} using the \python{add_energy()} method of an ASI object.
By manually setting the \python{local_interaction} field of an \python{ExchangeEnergy}, longer-range exchange interactions can be applied.

\paragraph{Reservoir computing} % io and experiments modules
The \python{hotspice.io} module provides input and readout routines for ASI, but these were implemented in the early stages of development and therefore have limited scope.
The same is true for the \python{hotspice.experiments} module, which provides routines to determine kernel-rank~\cite{RC_ASI} and task-agnostic~\cite{RC_TaskAgnosticMetrics_v2} RC metrics.
Since then, the \texttt{PRCpy} package~\cite{PRCpy} has become available, which is more broadly applicable and therefore preferable for calculating RC metrics. \par
Besides this, the \python{experiments} module provides a \python{Sweep} class, which will be used in \cref{ch:Applications} in conjunction with the \python{utils.ParallelJobs} method to determine RC metrics during parameter sweeps on multiple GPUs or CPUs.

\paragraph{Graphical interface}
A \xlabel{graphical user interface} (GUI) is available for \hotspice, which allows the user to directly interact with the ASI and observe changes in real-time.
Calling \python{hotspice.gui.show(ASI)} displays the window shown in~\cref{fig:2:GUI}.
This supersedes the deprecated \python{hotspice.plottools} module.

\xfig[1.0]{2_Hotspice/GUI.png}{
	The \hotspice graphical user interface.
	This example shows a spatially averaged view of an $80 \times 50$ pinwheel ASI, \SI{2}{\micro\second} after it was initialised in a random state.
	\label{fig:2:GUI}
}

The state of the ASI is prominently displayed, with a few useful statistics shown in the bottom left panel.
The \xref{update algorithm} is controlled by the red box in the bottom right.
The central panel changes the ASI display between four display modes.
By default, the magnetisation is shown using an averaging method appropriate for the ASI lattice.
The second mode displays individual magnets' magnetisation direction $s_i \vc{u}_i$ or the \xref{effective field} $\vc{B}_{\mathrm{eff},i}$ they experience.
In the third mode, all \xref{energy contributions} ($E_\mathrm{MS}$, $E_\mathrm{Z}$, $E_\mathrm{Exch}$, their sum $E_i$ and resulting $\EBeff$) are shown for each magnet, along both the \link{easy axis}{easy} and hard axis.
The last mode shows the spatial distribution of some parameters like the temperature $T$, \xref{shape anisotropy} $\EB$ and size of the magnetic moment $\mu$. \par
The user can interact with the ASI via buttons in the rightmost panels, e.g. progressing through time, setting an initial state, or applying custom functions for more complex simulations.
Specific magnets can be switched by clicking on the ASI plot with the mouse.

%\paragraph{Useful functions}
%While most functions in the \textbf{\python{hotspice.utils}} module are meant for internal use, some can be useful in scripts.
%The \python{asnumpy} method is particularly useful for GPU/CPU-agnostic code, as it converts an array to CPU if necessary.
%For long simulations, the \python{free_gpu_memory} method may be needed as Python's garbage collector might not collect all GPU objects.

\subsection{Retrospective on the \hotspice implementation}
\subsubsection{Grid}
Throughout this section, we have seen that the use of a \xref{rectilinear grid} has significant implications on the efficiency, applicability and implementation of \hotspice.
On the positive side, the grid enables fast and memory‐efficient calculation of the \xref{magnetostatic interaction} because all \xref{unit cells} can use \link{unit cell kernel}{common kernels}.
With this approach, the magnetostatic interaction need not be \link{truncated kernel}{truncated} at a certain distance to prevent excessive memory usage, though the benefit of considering interactions across the entire system may be negligible.
For instance, the \texttt{flatspin} ASI simulator~\cite{flatspin} has no underlying grid, so it only considers interactions within a certain radius, which does not significantly affect dynamics.
Still, when using the GPU, the performance with a full kernel only starts to decline for systems on a grid larger than $800 \times 800$. \par
On the other hand, the grid restricts the usage of \hotspice to \link{fig:2:ASIs}{periodic ASI} configurations which can be put on a regular lattice.
In theory, simulations of up to a few hundred randomly placed magnets should nonetheless be possible, though very inefficiently.
This precludes the simulation of irregular ASI such as the evolutionary ASI developed by \textit{Penty and Tufte}~\cite{ASI_Evolutionary_ALife}. \par
\link{multi-switching}{Multi-switching} in the \link{Metropolis-Hastings sampling}{Metropolis-Hastings scheme} benefits greatly from the grid, as it enables efficient \xref{selection algorithms} that maintain a minimal distance between samples.
In irregular ASI, selecting sufficiently distant magnets would be far more difficult and likely necessitate the use of an underlying `virtual' grid regardless.

\subsubsection{GPU/CPU}
In retrospect, porting \hotspice to the GPU was not necessary for this thesis, as the ASI used later in \cref{ch:Applications} are relatively small and are most often simulated using the \xref{first-switch method}.
Since rejection-free KMC does not support \xref{multi-switching}, the majority of computations in that chapter are performed on the CPU. \par
Nonetheless, GPU support has broadened the range of systems that \hotspice can efficiently simulate, in particular for the \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm}.
One remaining limitation is that the current implementation requires the user to choose between GPU and CPU before the \python{import hotspice} statement.
This is a remnant from early development since GPU support was not planned from the start.
Even though this rarely poses an issue, it is an unnecessary limitation that could be addressed in the future.

\subsubsection{Ensembles}
In many cases, simulations are repeated for statistical averaging or parameter variations.
With the current \hotspice implementation, these runs must be simulated separately.
By ``stacking'' simulations in memory --- extending the underlying 2D arrays to 3D --- such ensembles could be simulated far more efficiently.
This could particularly enhance GPU performance for small systems using the first-switch method~\cite{GillespieParallel}.
Although this would require extensive changes to the core of \hotspice, it can be a useful consideration when developing future software.

\newpage % For consistent formatting
\section{Verification} \label{sec:2:Verification}
Now that we know the ins and outs of the model used by \hotspice, all that remains is to verify its correct implementation.
To this end, we simulate several systems of increasing complexity for which analytical solutions are available.
The examples discussed here are all equilibrium problems, so we will use \xref{Metropolis-Hastings sampling} to verify that it indeed samples the equilibrium state space.
Special attention will be given to the issue of ``\xref{critical slowing down}'' which plagues this algorithm near phase transitions.
\subsection{Hexagon}
Before progressing to equilibrium problems, we briefly check that the \xref{magnetostatic interaction} and its underlying \link{unit cell kernel}{kernels} all work as expected.
The magnetostatic interaction energy of any given arrangement of nanomagnets can be calculated analytically and compared to \hotspice. \par
As an example that is neither too simple nor complex, we consider a regular hexagon in a vortex state. The energy of all magnets should equal the following analytical solution, which is the sum of the magnetostatic interaction energy between nearest neighbours $\circled{1}$, next-nearest neighbours $\circled{2}$ and magnets on opposite sides of the hexagon $\circled{3}$:
\begin{align*}
	E_{\mathrm{MS},i} =&\, \circled{1} + 2 \times \circled{2} + 2 \times \circled{3} \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} -2\frac{\mu_0 m^2}{4\pi \frac{3 \sqrt{3} a^3}{8}} \Big[3\cos^2(\pi/3) - \cos(2\pi/3)\Big] -2\frac{\mu_0 m^2}{4\pi \frac{a^3}{8}} \Big[3\cos^2(\pi/6) - \cos(\pi/3)\Big] \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} \bigg[29+\frac{20}{3\sqrt{3}}\bigg] \mathrm{.}
\end{align*}
The following Python function verifies that, indeed, \hotspice gives the expected total magnetostatic energy $E_{\mathrm{MS},i}$ for all 6 magnets $i$ in the system.
\begin{lstlisting}
import numpy as np
import hotspice

def test_hexagon(l=470e-9, s=10e-9, m=1.1278401e-15):
	mm = hotspice.ASI.IP_Kagome(a := (l+2*s)/np.tan(30*np.pi/180), nx=5, ny=3, moment=m, pattern="vortex")
	E_sim = mm.get_energy('dipolar').E[mm.m.astype(bool)]
	E_exact = -1e-7*m*m/a**3*(29+20*np.sqrt(3)/9)
	print("OK" if np.allclose(E_sim, E_exact, atol=0) else "Fail")

test_hexagon() # OK
\end{lstlisting}

\subsection{Non-interacting spin ensemble}

When an \xref{external magnetic field} of magnitude $B$ is applied to a non-interacting ensemble of Ising spins, it follows directly from the partition function that the average magnetisation follows the relation
\begin{equation}
	\frac{\langle M \rangle}{M_0} = \tanh\ab(\frac{\mu B}{\kBT}) \mathrm{,}
\end{equation}
with $M = \sum_i \mu_i s_i$ the total magnetic moment of the system and $M_0 = \sum_i \mu_i$.
\cref{fig:2:Noninteracting} demonstrates that \hotspice{} correctly reproduces the expected result.

\sidefig{2_Hotspice/Verification/Noninteracting_IP.pdf}{
	\label{fig:2:Noninteracting}
	Average magnetisation of a non-interacting ensemble of spins, as a function of the \link{external magnetic field}{applied magnetic field} magnitude $B$.
	\newline\newline\newline % Newlines center the caption w.r.t. plot
}

\subsection{Exchange-coupled Ising system}
\label{sec:2:Verification_OOP_Exchange}
The 2D square-lattice \link{exchange coupling}{exchange-coupled} Ising model is one of the few exactly solvable systems in statistical physics~\cite{ExactlySolvedModelsStatMech}.
An analytical solution is known for the temperature-dependence of its average magnetisation
\begin{equation}
	\label{eq:2:Verification_exchange_M}
	\frac{\langle M \rangle}{M_0} = \sqrt[8]{1 - \sinh^{-4}(2J/\kBT)} \mathrm{,}
\end{equation}
with $J$ the \xref{exchange coupling} constant~\cite{Correlations2DIsing,IsingSpontaneousMagnetization,coey2010magnetism}.
This implies that the system exhibits a second-order \xlabel{phase transition} at the critical temperature~\cite{ExactlySolvedModelsStatMech}
\begin{equation}
	T_\mathrm{c} = \frac{2J}{\kB \ln(1 + \sqrt{2})} \mathrm{.}
\end{equation}
Furthermore, an analytical solution is available for the nearest-neighbour correlation
\begin{equation}
	\label{eq:2:Verification_exchange_corr}
	\langle s_i s_{i+1} \rangle = 
	\begin{cases}
		\sqrt{1+k} \ab[\frac{1-k}{\pi}K(k) + \frac{1}{2} \ab] &\text{for } T < T_\mathrm{c} \mathrm{,} \\ 
		\sqrt{1+k} \ab[\frac{1-k}{\pi k}K(1/k) + \frac{1}{2} \ab] &\text{for } T > T_\mathrm{c} \mathrm{.} \\ 
	\end{cases}
\end{equation}
with $K$ the complete elliptic integral of the first kind and $k=\sinh^{-2}(2J/\kBT)$~\cite{Correlations2DIsing}. \\\par

The result of a \hotspice simulation of this Ising system is shown in \cref{fig:2:Verification_exchange}, as calculated for an $800 \times 800$ lattice at various temperatures $T$.
Maximal \xref{multi-switching} ($Q=+\infty$) during \xref{Metropolis-Hastings sampling} was used to verify that --- at least for systems with such short-range coupling --- this sort of extreme multi-switching still converges to the correct solution.
Since the theoretical curves for $\langle M \rangle / M_0$ and $\langle s_i s_{i+1} \rangle$ are monotonically decreasing, the final state from the previous temperature step was retained as the starting point for the next step. This preserves progress already made towards equilibrium and avoids the disruption that resetting to the uniform state would cause~\cite{MCinStatPhys}. \par

\xfig{2_Hotspice/Verification/OOP_Exchange.pdf}{
	\label{fig:2:Verification_exchange}
	\textbf{(a)} Average magnetisation and \textbf{(b)} nearest-neighbour (NN) correlation as a function of temperature.
	Markers show the \hotspice result for an $800 \times 800$ OOP square-lattice \link{exchange coupling}{exchange-coupled} Ising system using \xref{Metropolis-Hastings sampling} with maximal \xref{multi-switching}.
	The simulation was performed 3 times --- for different amounts of \link{Monte Carlo sweep}{Monte Carlo steps per site} (MCS) for each temperature step.
	Discrepancies due to \xref{critical slowing down} above $T_c$ improve with increasing MCS.
}
% TODO: ? why the discrepancy in the correlation?

The \hotspice result corresponds well to the theoretical predictions of~\cref{eq:2:Verification_exchange_M} and~\cref{eq:2:Verification_exchange_corr}, both below $T_\mathrm{c}$ and in the high-temperature limit.
The average magnetisation $\langle M \rangle / M_0$ follows the theoretical curve more closely than the NN correlation $\langle s_i s_{i+1} \rangle$.
However, just above $T_\mathrm{c}$, the average magnetisation $\langle M \rangle / M_0$ evolves only slowly towards the expected value.
Increasing the number of MCS per temperature step brings the system closer to the theoretical equilibrium and reduces the temperature range above $T_\mathrm{c}$ where the simulated system is not at equilibrium.
However, this yields diminishing returns, as every additional Monte Carlo step converges more slowly towards the equilibrium.
This slow convergence near the critical point is due to a well-known phenomenon known as ``\xref{critical slowing down}''.

\paragraph{Critical slowing down}
As the correlation length $\xi$ diverges at a second-order \xref{phase transition}, the autocorrelation time $\tau$ will increase as $L^z$ in finite systems of linear size $L$~\cite{NumericalDynamicalNiedermayer}.
The \xlabel{dynamical critical exponent} $z$ is typically $\gtrsim 2$ for local dynamics --- \textit{Niedermayer}~\cite{niedermayer1988general} notes that this is intuitively analogous to the problem of a random walk, where $\order{R^2}$ steps are required to traverse a distance $R$.
The significant rise in the autocorrelation time causes successive Monte Carlo configurations to be highly correlated, resulting in a very slow exploration of the phase space that impedes convergence towards equilibrium~\cite{NumericalDynamicalNiedermayer,CompStatPhys,StatisticalMechanicsAlgorithmsComputations}. \par
This phenomenon is termed \idx{critical slowing down} in second-order phase transitions --- supercritical slowing down is an analogous phenomenon for discontinuous transitions~\cite{PhD_Reynal}.
It is particularly problematic for single-spin flip algorithms like \xref{Metropolis-Hastings sampling}, as single-spin flips are no longer physically relevant in this region~\cite{PhD_Reynal}.
Such local-update Monte Carlo algorithms only change a small portion of the system in each step, resulting in minimal differences between successive states.
Consequently, even after many MCS, the number of statistically independent configurations remains low, as the large autocorrelation time yields a very low effective number of independent samples $N_\mathrm{eff} = N_\mathrm{MCS}/\tau$~\cite{niedermayer1988general,BeatCriticalSlowingDown1990}. \par

While CSD is to some extent an inherent property of the physical system --- physical quantities like $\xi$ and $\tau$ diverge near the \xref{phase transition} --- the choice of simulation algorithm also has a significant impact~\cite{PhD_Reynal}.
Because CSD originates from long-range correlations, the \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm} exacerbates the issue due to its single-spin flip nature.
Other algorithms have been devised that alleviate CSD by non-local spin updates, thereby acting on collective modes~\cite{BeatCriticalSlowingDown1990}.
Among these are the group of \idx{cluster algorithms} --- first the \idx{Swendsen-Wang}~\cite{SwendsenWang} and later the \idx{Niedermayer}~\cite{niedermayer1988general} and \idx{Wolff}~\cite{Wolff} algorithms --- which construct clusters of magnets and flip all magnets in the cluster simultaneously~\cite{CompStatPhys}. % Also multi-grid Monte Carlo~\cite{edwards1991multi}
This generates more distinct states and, if the clusters are constructed correctly, reduces the autocorrelation time.
The clusters are constructed by starting from a `seed' magnet and adding its neighbours to the cluster with a certain probability.
For these cluster algorithms, the dynamical critical exponent $z \approx 0$ in the 2D \link{exchange coupling}{exchange-coupled} Ising system~\cite{NumericalDynamicalNiedermayer}, as compared to $z\approx2.17$ with the Metropolis-Hastings algorithm~\cite{DynamicExponentMetropolis}.
However, in a system with long-range interactions, like the \xref{magnetostatic interaction}, all magnets are ``neighbours'', and generating an appropriate cluster becomes nontrivial.
While methods exist to generate clusters in systems with long-range interactions~\cite{MC_spinLongRange}, they are not easily applied to a general system.
Since CSD will not present significant issues in this thesis, the Wolff algorithm has therefore only been implemented in \hotspice for \link{exchange coupling}{exchange-coupled} systems.

\subsection{Exchange- and magnetostatically-coupled Ising system}
Including long-range \xref{magnetostatic interactions} into an exchange-coupled square-lattice Ising system significantly alters its behaviour.
While the temperature $T$ remains an important system parameter, now another determining factor is the ratio $\delta = E_{\mathrm{exch},i,j}/E_{\mathrm{MS},i,j}$ ($j \in \mathcal{N}_i$), which represents the balance between the \xref{exchange coupling} and magnetostatic interaction for nearest neighbours.
In terms of \hotspice parameters, we can write
\begin{equation}
	\delta = \frac{8 \pi J a^3}{\mu_0 \mu^2} \mathrm{,}
\end{equation}
with the magnetic moment $\mu$ constant for all magnets, and $a$ the NN distance in the square lattice.
Analytical predictions remain possible in this system at zero temperature: for $\delta < 0.85$, the magnetostatic coupling dominates, leading to a \idx{checkerboard state}~\cite{AgingIsingDipolar}.
As $\delta$ increases, the ever-stronger exchange coupling leads to the formation of ferromagnetic domains, which organise into stripes due to the magnetostatic interaction, with the stripe width determined by $\delta$~\cite{StripedDipolarIsing,FMmonolayer1993}. \par
The average stripe width is reflected in the \idx{nearest-neighbour correlation} $\langle s_i s_{i+1} \rangle$, as shown in~\cref{fig:2:Verification_dipolar} for a \hotspice simulation.
The analytical theory is valid at zero temperature, but we are using the \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm} which requires non-zero temperature, so we will be using a sufficiently low temperature (\SI{50}{\kelvin}) for this simulation.

\xfig{2_Hotspice/Verification/OOP_Dipolar.pdf}{
	\label{fig:2:Verification_dipolar}
	\link{nearest-neighbour correlation}{Nearest-neighbour (NN) correlation} in an \link{exchange coupling}{exchange-coupled} system with long-range \xref{magnetostatic interactions} included, as a function of relative NN magnetostatic/exchange coupling $\delta$. Transitions occur at $\delta=0.85$ and 2.65, indicated by dotted lines. Insets show the magnetisation state with growing \xlabel{stripe domains}: white corresponds to spin `up', black to `down'.
}

Consistent with the theoretical predictions of~\ccite{StripedDipolarIsing}, for $\delta < 0.85$ a \xref{checkerboard state} exists with $\langle s_i s_{i+1} \rangle = -1$.
In the range $0.85 < \delta < 2.65$, \xref{stripe domains} with a width of 1 row are preferred, leading to $\langle s_i s_{i+1} \rangle = 0$.
Beyond $\delta=2.65$, a 2-row width becomes preferable with $\langle s_i s_{i+1} \rangle = 0.5$.
Increasing $\delta$ further leads to ever wider stripe domains, and in the limit $\delta \rightarrow +\infty$ the correlation approaches $\langle s_i s_{i+1} \rangle \rightarrow 1$.
The phenomenon of CSD is also present here, albeit less severely.

\subsection{Square-to-pinwheel transition angle}\label{sec:2:Verification_IP_SquarePinwheel}
The in-plane square and pinwheel ASI lattices can be continuously transformed into each other by rotating each individual magnet by \ang{45}.
Despite being this closely related, their ground state magnetic ordering differs significantly.
Square ASI has an antiferromagnetic (AFM) ground state, where all vertices have a net zero magnetisation.
Meanwhile, pinwheel ASI exhibits \idx{superferromagnetism}, where all magnets with similarly-oriented \link{easy axis}{easy axes} are magnetised in the same direction~\cite{ApparentFMpinwheel}.
Therefore, a critical angle $\ang{0} < \alpha_c < \ang{45}$ must exist where the ground state transitions between these two extremes. \par
For the dipole model, theoretical calculations predict this transition at $\alpha_c = \arcsin(\sqrt{3}/3) = \ang{35.3}$~\cite{AFM-FM-transition-Pinwheel,MagicAngle}.
For the \xref{dumbbell model}, the transition angle depends on the distance $d$ between \xref{magnetic monopoles}, but is always larger than for the dipole model~\cite{AFM-FM-transition-Pinwheel}. \\\par
The result of a \hotspice simulation using both models is shown in \cref{fig:2:Pinwheel_angle}.
To quantify this transition, we measure the fraction of vertices with net zero magnetisation --- this value is 0 for superferromagnetic order while it is 1 for AFM order.
We used the same lattice compression as described in~\ccite{AFM-FM-transition-Pinwheel}, making our lattice spacing $a=\SI{240}{\nano\metre} / \sin(\ang{45}+\alpha)$ angle-dependent: it varies from \SI{340}{\nano\metre} at $\alpha=\ang{0}$ to \SI{240}{\nano\metre} at $\alpha=\ang{45}$. 
For the dipole model, the transition occurs at $\approx \ang{35}$ as expected, while the dumbbell model (here with $d = \SI{220}{\nano\metre}$) indeed transitions at a larger rotation angle.
% TODO REF: should or should we not look for some experimental references to include the following sentences, or do we keep this text hidden?
% While this transition was experimentally observed to be gradual from \ang{35} to nearly \ang{45}, these experiments likely could not reach an equilibrium state due to defects, rapid quenching, or finite correlation times, all of which freeze domain walls in the system. Metropolis-Hastings samples the equilibrium state space, resulting in the sharp transition seen in the figure.

\xfig[0.6]{2_Hotspice/Verification/Pinwheel_angle.pdf}{
	Fraction of vertices with net zero magnetisation at equilibrium, as square ASI (left) transitions to pinwheel ASI (right) by rotating individual magnets.
	The theoretical transition angle $\alpha_c \approx \ang{35.3}$ for the dipole model is indicated by the vertical dotted line.
	The dumbbell model uses a monopole-monopole distance $d = \SI{220}{\nano\metre}$.
	\label{fig:2:Pinwheel_angle}
}

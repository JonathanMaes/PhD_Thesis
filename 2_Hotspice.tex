\chapter{Methods: ``\hotspice'' simulator for ASI}\label{ch:Hotspice}
% \glijbaantje{It's not a bug, it's a feature.}{Someone}

\begin{adjustwidth}{2em}{2em} % TODO END: update once published.
    \begin{center}
        \textbf{Material from this chapter has also been published in:} \\
    \end{center}
    \vspace{1em}
    \begin{adjustwidth}{0em}{1.5em}
	    \begin{itemize}
	    	\item[\cite{MAES-24}] J.~Maes, D.~De~Gusem, I.~Lateur, J.~Leliaert, A.~Kurenkov, and B.~Van Waeyenberge.
	    	\newblock The design, verification, and applications of Hotspice: a Monte Carlo simulator for artificial spin ice.
	    	\newblock \emph{ArXiV}, arXiv:\penalty0 2409.05580, 2024.
	    \end{itemize}
    \end{adjustwidth}
    \vspace{0.5em}
    \begin{center}
        \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{.5em}
    \begin{center}
    	The \hotspice simulator discussed in this chapter\\
    	is open-source and available on \href{https://github.com/bvwaeyen/Hotspice}{GitHub}. \\
    \end{center}
    %\vspace{0.5em}
    \begin{center}
    \centering\rule{0.7\linewidth}{0.4pt}
    \end{center}
    \vspace{1em}
\end{adjustwidth}

To assess the potential of \xref{Reservoir Computing} in (perpendicular-anisotropy) \xref{Artificial Spin Ice}, as is the main topic of this thesis, a simulation framework is needed that allows efficient exploration of the impact that various system parameters and input methods have on the reservoir performance. \\\par

Nanomagnetic systems are often simulated using micromagnetic codes, such as the finite-difference-based \mumax~\cite{mumax3} and \oommf~\cite{OOMMF} or the finite-element-based \nmag~\cite{Nmag}, which capture the magnetization dynamics of individual nanomagnets in great detail. \par
However, the time between successive switches of a nanomagnet is not necessarily similar to the timescale of micromagnetics.
Furthermore, determining RC metrics requires applying many input cycles --- on the order of 100 for the task-agnostic metrics --- to get a statistically valid result.
In this timeframe, many switches occur.
When the simulated time extends beyond several microseconds, as is typically the case, simulating even a modest number of magnets --- on the order of several dozen --- becomes computationally unfeasible~\cite{leo2021chiral}. \\\par

To address these limitations, specialized ASI simulation tools have been developed.
An example of this is the flatspin simulator~\cite{flatspin}, which implements deterministic spin flipping via a Stoner-Wohlfarth model~\cite{StonerWohlfarth2008}.
Using such higher-level approximations enables the study of collective behaviour in much larger systems and over far longer timescales than is feasible with micromagnetic codes, though at the cost of no longer simulating the internal magnetization structure of individual nanomagnets in detail.
Additionally, Monte Carlo methods are often used to simulate spin ices, including ASI, but these are typically specialized to a select few lattice geometries and often only account for nearest-neighbour interactions, whose strength is often arbitrarily set or calculated separately using micromagnetic codes.~\cite{MeltingASI,sklenar2019field,gilbert2014emergent,zhang2013crystallites} \\\par % REFS: 'gilbert2014emergent': MC sims based on a vertex model and interacting magnetic charges. 'zhang2013crystallites': uses monopoles and NN couplings to model kagome ASI with Metropolis and loop update. 'moller2006artificial': bit broader using dipolar interaction, though seemingly still only for nearest neighbors. 'mengotti2011kagome' use a full dipole model with Ewald summation for PBC. 'lou2023competing': dipole model for half-occupation IP Ising. 'sendetskyi2019continuous': dipole model for square ASI. 'EngineeringRelaxationComputation': KMC on dipole model, seemingly for square arrays. 'sklenar2019field': NN interactions calculated by mumax on quadrupole lattice. 'MeltingASI': 16-vertex ice model

Our goal was to blend these two approaches, resulting in \hotspice: a versatile Monte Carlo simulator meant to capture ASI physics with minimal arbitrary parameters, allowing various lattice configurations to be evaluated. \par
This software approximates each single-domain nanomagnet as a single Ising spin, associating energies with the various ASI states by accounting for the magnetostatic interaction between all magnets.
\hotspice supports both in-plane (IP) and out-of-plane (OOP) ASI, which may contain thousands of magnets. Simulations can span arbitrary timescales, as determined by the switching time of magnets in the system. \\\par

In this chapter, we discuss several model variants that have been implemented, and assess their accuracy in simulating the behaviour of ASI.
These variants differ in their choice of Monte Carlo spin-flip algorithm and their calculation of the magnetostatic interactions and energy barriers. \\\par

\section{Model}
In single-domain IP nanomagnets, the magnetisation prefers to align along the fixed easy axis of the geometry, while for OOP magnets a strong interfacial anisotropy causes a preferential orientation along the $z$-axis.
Either way, it is natural to use an Ising-like approximation for simulating such single-domain nanomagnets.
The position $\vc{r}_i$, axis $\vc{u}_i$ and size of the magnetic moment\footnote{
	\label{fn:2:moment_integral}
	The size of the magnetic moment $\mu_i$ corresponds to the total ground state magnetic moment $\abs{\int_{\Omega_i} \vc{M}(\vc{r})d\vc{r}}$, with $\Omega_i$ the shape of magnet $i$ and $\vc{M}(\vc{r})$ its magnetisation in the twofold degenerate ground state. Due to edge relaxation effects, this value is slightly smaller than $M_\mathrm{sat} V_i$.
} $\mu_i$ of each magnet $i$ are fixed and they are only allowed to switch between the `up' ($\uparrow$) and `down' ($\downarrow$) magnetisation states.
Thus, the total magnetic moment vector of magnet $i$ can be expressed as
\begin{equation}
	\vc{\mu}_i = s_i \mu_i \vc{u}_i \mathrm{,}
\end{equation}
where $s_i = \pm 1$ and $\abs{\vc{u}_i} = 1$. \par
The switching rate between these two states is determined by the effective energy barrier $\EBeff$ that separates them, as well as the temperature $T$.
For an isolated nanomagnet, the energy barrier $\EB$ originates from its shape anisotropy.
Interactions with other magnets or external fields modify the energy landscape, leading to an effective barrier which we denote as $\EBeff$~\cite{leo2021chiral}.
Each magnet can have a unique magnetic moment size $\mu_i$, temperature $T_i$ and energy barrier $E_{\mathrm{B},i}$.
This enables, for instance, modelling some of the disorder due to lithographic variations by assigning a different shape anisotropy to each magnet, typically sampled from a Gaussian distribution with mean $\EB$ and standard deviation $\sigma(\EB)$. \\\par

% We are most interested in simulating ASI, which are often well-ordered and spatially periodic. Therefore, \hotspice chooses to...
Due to the periodic nature of many ASI lattices, \hotspice chooses to perform the simulation on a rectilinear grid as this allows quantities to be stored as matrices.
Each grid point may or may not contain a magnet, and the magnets must either all be of the IP type, or all OOP.
The benefits and details of this approach will be discussed further in~\cref{sec:2:Implementation}.
Even though this implementation does not allow complete freedom in the placement of magnets, many popular ASI lattices can be constructed in this manner. \par
\vspace{-1em}
\xfig[1.0]{2_Hotspice/ASIs.pdf}{
	Predefined artificial spin ice (ASI) lattices available in \hotspice.
	The unit cell of each lattice is delineated by a central dark grey rectangle.
	The red indicator defines the lattice parameter $a$.
	In the Ising approximation, the magnetization of in-plane magnets (top) aligns along the major axis of the depicted ellipses.
	Out-of-plane magnets (bottom) are illustrated as circles.
	\label{fig:2:ASIs}
}

\cref{fig:2:ASIs} showcases the 12 lattices that \hotspice provides out-of-the-box. \par
The pinwheel and square lattices come in two variants, related by a global \ang{45} rotation of the entire lattice.
This gives rise to different boundaries due to the Cartesian character of the underlying grid, which may alter the dynamics of the ASI.
Furthermore, the unit cell for lucky-knot pinwheel (b) and open square (d) is more compact than for the more popular diamond pinwheel (a) and closed square (c), resulting in faster simulation. \par % TODO REF: more popular in literature? Examples where these are used?
Magnets in the pinwheel lattices (a) and (b) are placed at the same location as in the square lattices (c) and (d), respectively, but with each magnet rotated by \ang{45}.
The same can be said of the triangle (f) and kagome (g) lattices where individual magnets are rotated by \ang{90}.
The Cairo lattice (h) can be continuously deformed into the Shakti lattice~\cite{ShaktiCairo}, but note that the point dipole model is no longer appropriate for the latter; instead, a dumbbell model (see~\cref{sec:2:Dumbbell}) would be more accurate. \par
The remaining four IP and four OOP lattices are also related: the magnets in the OOP lattices (i)-(l) are positioned at the vertices where magnets meet in their respective IP counterparts (e)-(h).

\section{Energy calculation}
The energy of the system is the driving force behind these Monte Carlo simulations.
In this section, we will list the various energy contributions used in \hotspice, explain how we may account for the finite size of real nanomagnets in this Ising-like model, and show how the effective energy barrier $\EBeff$ is calculated.

\subsection{Energy contributions}
Three energy contributions have been implemented in \hotspice, supporting both open and periodic boundary conditions (PBC).\footnote{
	Users can implement more energy contributions by inheriting from the \python{hotspice.Energy} class and implementing the \python{abstractmethod}s, taking care to correctly account for open or periodic BC when necessary.
}
\begin{enumerate}
	\item The \textit{magnetostatic interaction energy} between magnets $i$ and $j$
	\begin{equation}
		\label{eq:2:E_MS}
		E_{\mathrm{MS},i,j} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\abs{\vc{r}_{ij}}^3} - \frac{3(\vc{\mu}_i \bcdot \vc{r}_{ij}) (\vc{\mu}_j \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^5}) \mathrm{,}
	\end{equation}
	with $\mu_0$ the vacuum permeability and $\vc{r}_{ij} = \vc{r}_i - \vc{r}_j$ the vector connecting the two magnetic dipoles $\vc{\mu}_i$ and $\vc{\mu}_j$. \par
	This is the main interaction dictating how nanomagnets influence each other, causing the typical properties of the various ASI lattices, e.g. superferromagnetism in the pinwheel lattice~\cite{li2018pinwheel}.
	Because of its importance, this is the only interaction \hotspice considers by default when an ASI is created.
	Any other energy contributions must explicitly be added to an ASI; see \cref{sec:2:API_energies}.
	This avoids wasting calculations on energies not relevant to the simulation.
	
	\item The \textit{Zeeman energy} of an external field $\vc{B}_\mathrm{ext}$ interacting with magnet $i$
	\begin{equation}
		\label{eq:2:E_Z}
		E_{\mathrm{Z},i} = -\vc{\mu}_i \bcdot \vc{B}_\mathrm{ext} \mathrm{,}
	\end{equation}
	where $\vc{B}_\mathrm{ext}$ can be set for each magnet individually. \par
	This energy contribution provides a means for the outside world to interact with the system, and is therefore indispensable when we will be investigating reservoir computing later on.
	Even if input is provided through other means than an external field, this energy contribution can often still be used by considering an effective field instead.
	
	\item The \textit{exchange coupling energy} between nearest neighbours (NN) $i$ and $j$
	\begin{equation}
		\label{eq:2:E_exch}
		E_{\mathrm{exch},i,j} = J \frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\mu_i \mu_j} \mathrm{,}
	\end{equation}
	with $J$ the exchange coupling constant, which is constant throughout the ASI. \par
	This interaction is rarely present in ASI, but can for example be relevant in interconnected ASI --- whether by design or due to limited lithographic accuracy.
	We will encounter an example of the latter in \cref{sec:3:OOP:MFM}.
\end{enumerate}

The combined \textit{interaction energy} $E_i$ of a single magnet $i$ with its environment is then given by
\begin{equation}
	\label{eq:2:E}
	E_i = E_{\mathrm{Z},i} + \sum_j E_{\mathrm{MS},i,j} + \sum_{j \in \mathcal{N}_i} E_{\mathrm{exch},i,j} \mathrm{,}
\end{equation}
where $\mathcal{N}_i$ is the collection of nearest neighbours of magnet $i$.
Which magnets are included in this collection depends on the ASI lattice and which site of the unit cell magnet $i$ is in, and can be defined separately for each ASI lattice. \\\par
Note that all terms in \cref{eq:2:E} simply change sign\footnote{
	Energy contributions which are independent of the state $s_i$, such as the magnetostatic self-energy (i.e., the energy due to the magnetisation profile $\vc{M}(\vc{r})$ throughout the magnet), are not included in \cref{eq:2:E} because they are constant and therefore do not affect the ASI dynamics.
} when magnet $i$ switches ($\vc{\mu}_i \rightarrow -\vc{\mu}_i$): if the switch occurs at time $t$, then $E_i(t^+) = -E_i(t^-)$.
%As such, $E_i$ represents the total interaction energy of a magnet with its `neighbours',\footnote{
%	In this context, a `neighbour' of a magnet can be interpreted more broadly as all magnets it interacts with through a particular energy contribution. For example, the magnetostatic interaction considers all magnets to be `neighbours', unless the user has explicitly set a maximum interaction distance.
%}.
Therefore, the change in energy of the ASI when that magnet switches is simply $\Delta E_{i,1\rightarrow2} = -2 E_i$.
This is called the \textit{switching energy}, and we will see in the next few sections that it plays a central role in both algorithms used for simulating system dynamics as well as the calculation of the effective energy barrier $\EBeff$.
It is therefore very advantageous that the switching energy is so cheap to compute. \par
When the simulation is initialised, the energy contributions are calculated for all magnets.
Each magnet therefore stores a value in memory for each of the terms in \cref{eq:2:E}.
Whenever a magnet switches, the energies of its `neighbours' are updated appropriately, which constitutes a major part of the calculation effort required for every step in the simulation.

\subsection{Finite-size corrections to the magnetostatic energy}\label{sec:2:finite}
\cref{eq:2:E_MS,eq:2:E_Z,eq:2:E_exch} approximate each nanomagnet as a point dipole, but real nanomagnets have a finite spatial extent.
If one assumes a uniform magnetisation throughout each single-domain nanomagnet, then this finite size does not affect the Zeeman energy, nor the exchange energy which can capture any shape-related effects by an appropriate choice of the exchange coupling $J$.
The Zeeman energy could even account for a deviation from uniform magnetisation near the edge of a magnet, which occurs in reality but which we will neglect in the following, by an appropriate choice of $\vc{B}_\mathrm{ext}$. \par
The magnetostatic interaction, however, depends on the relative position, orientation, and shape of all magnets.
This may result in inadequate simulation of closely spaced ASI where the true magnetostatic coupling can be significantly stronger than predicted by a point dipole approximation.
Therefore, two (mutually exclusive) improvements have been implemented in \hotspice, which rescale the magnetostatic interaction energy between magnets.

\subsubsection{Second-order correction for dipoles}
\textit{Politi and Pini}~\cite{Dipolar2Dparticles} have presented a multipole expansion of the magnetostatic interaction, to account for the finite size of 2D nanomagnets (i.e., lateral dimensions $\gg$ thickness), assuming a uniform magnetization.
This results in a second-order correction
\begin{equation}
	E_{\mathrm{MS},i,j} = E_{\mathrm{MS},i,j}^\mathrm{(0)} + E_{\mathrm{MS},i,j}^\mathrm{(2)} \mathrm{,}
\end{equation}
where $E_{\mathrm{MS},i,j}^\mathrm{(0)}$ is the original point dipole magnetostatic interaction given by~\cref{eq:2:E_MS}. \par
The second-order correction can be written as
\begin{equation}
	E_{\mathrm{MS},i,j}^\mathrm{(2)} = \frac{\mu_0}{4\pi} \frac{3\mathcal{I}_{ij}}{2} \Bigg[3\frac{\vc{\mu}_i^\mathrm{OOP} \bcdot \vc{\mu}_j^\mathrm{OOP}}{\abs{\vc{r}_{ij}}^5} + \frac{\vc{\mu}_i^\mathrm{IP} \bcdot \vc{\mu}_j^\mathrm{IP}}{\abs{\vc{r}_{ij}}^5} -5\frac{(\vc{\mu}_i^\mathrm{IP} \bcdot \vc{r}_{ij}) (\vc{\mu}_j^\mathrm{IP} \bcdot \vc{r}_{ij})}{\abs{\vc{r}_{ij}}^7} \Bigg] \mathrm{,}
\end{equation}
where $\vc{\mu}_i$ was split into its IP and OOP components, conveniently leading to separate IP and OOP terms as implemented in the two types of ASI in \hotspice. \par
The particular shape of the nanomagnets is encapsulated in the single scalar $\mathcal{I}_{ij} = (\mathcal{I}_i + \mathcal{I}_j)/2$.
These $\mathcal{I}$ are calculated similar to a moment of inertia:
\begin{equation}
	\mathcal{I}_i = \int_{\Omega_i} \abs{\vc{r} - \ab(\int_{\Omega_i} \vc{r} d\vc{r})}^2 d\vc{r} \mathrm{,}
\end{equation}
with $\Omega_i$ the volume of magnet $i$.
We assume all magnets have the same shape, such that $\mathcal{I}_{ij} = \mathcal{I}_i = \mathcal{I}_j$.
Since nanomagnets in ASI are typically rather elliptical (IP ASI) or round (OOP ASI), \hotspice allows the user to set the semi-major axis $a$ and semi-minor axis $b$ of the magnets comprising an ASI.
For such a geometry --- elliptic cylinders --- these moments of inertia reduce to the simple expression $\mathcal{I}_{ij} = \frac{1}{4}(a^2 + b^2)$. % TODO REF: add examples of papers using round or elliptical magnets?
%While this correction can be applied to both IP and OOP magnetic dipoles, it is most effective for OOP systems, as can be seen in~\cref{fig:2:MS_distance}.

\subsubsection{Dumbbell model}\label{sec:2:Dumbbell}
Instead of representing a magnet as a point dipole, one may instead choose to represent it as a pair of magnetic monopoles~\cite{MagneticMonopoles2008,MagneticMonopoleDynamics}.
This introduces a new parameter $d$: the effective distance between the north and south poles of a magnet, with respective positions $\vc{r_N}_i = \vc{r}_i + s_i\frac{d_i}{2}\vc{u}_i$ and $\vc{r_S}_i = \vc{r}_i - s_i\frac{d_i}{2}\vc{u}_i$.
An appropriate choice of $d_i$ (slightly smaller than the physical length $l$ of the nanomagnet~\cite{DDG_Masterproef}) allows this dumbbell model to emulate the spatial extent of a real nanomagnet. \par
The north and south monopoles are assigned magnetic charges $+q_i$ and $-q_i$, respectively, with $q_i=\mu_i/d_i$~\cite{MagneticMonopoles2008}.
This choice yields the same effective dipole moment at long distance.
The interaction energy between two magnetic charges $q$ and $q'$ can be derived from the magnetic version of Coulomb's law~\cite{ForceMagneticDipole} as
\begin{equation}
	E = -\int_\infty^{\vc{r}} \frac{\mu_0}{\num{4}\pi}\frac{qq'}{\abs{\vc{r}}^3} \hat{\vc{r}} \cdot d\vc{r} = \frac{\mu_0}{\num{4}\pi} \frac{qq'}{\abs{\vc{r}}} \mathrm{.}
\end{equation}
The magnetostatic interaction energy between two nanomagnets is then the sum of their four mutual monopole interactions, finally resulting in
\begin{equation}
	E_{\mathrm{MS},i,j} = \frac{\mu_0 \mu_i \mu_j}{4\pi d_i d_j} \Bigg(\frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_N}_j}} + \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_S}_j}}\\ - \frac{1}{\abs{\mathbf{r_N}_i - \mathbf{r_S}_j}} - \frac{1}{\abs{\mathbf{r_S}_i - \mathbf{r_N}_j}}\Bigg) \mathrm{.} \label{eq:2:E_MS_mono}
\end{equation}
The minus sign in the equation appears because north and south poles have opposite charge.
\hotspice is limited to a single value of $d$ for all magnets, due to the structure of the kernels used to calculate the magnetostatic interaction, which will be discussed in~\cref{sec:2:Kernels}.

\subsubsection{Comparison}
To assess whether these two corrections constitute an improvement to the resulting magnetostatic energy, we must quantify their impact by comparing against a known solution.
For this, we use the micromagnetic simulation package \mumax~\cite{mumax3}, which can determine the interaction energy for a given arrangement of ferromagnetic material.
While this solution is not exact, as \mumax uses a finite difference (FD) discretisation, the result will approach the true value for sufficiently small FD cell sizes. \\\par

\cref{fig:2:MS_distance} compares the original point dipole approximation and the two corrections (``finite dipole'' and ``dumbbell'') against the solution obtained with \mumax.
The figure shows 3 typical arrangements of neighbouring magnets in an ASI: two circular OOP magnets and two elliptical IP neighbours aligned along their easy and hard axes.
The magnetostatic interaction energy between the pair of magnets, divided by $\mu^2$ to be independent of magnet volume, is shown as a function of their normalized centre-to-centre distance.
A uniform magnetisation was used in the \mumax simulation, as this is the assumption under which the corrections were derived and because the lowest energy magnetisation state is size-dependent while the figure uses dimensionless units. \\\par

\xfig[1.0]{2_Hotspice/MS_distance.pdf}{
	Magnitude of the magnetostatic interaction between two magnets as a function of their normalized center-to-center distance, for the three \hotspice{} calculation methods (point dipole, second-order correction for dipoles, and dumbbell) compared to a micromagnetic \mumax{} calculation.
	OOP magnets are assumed to be circular with diameter $2r$, IP magnets are ellipses with length $l$ and width $w=4l/11$.
	Positions of north and south monopoles used in the dumbbell model are shown as red {\color{red}$\bullet$} and blue {\color{blue}$\bullet$} dots and are a distance $d=0.9l$ apart within a magnet.
	\label{fig:2:MS_distance}
}

For out-of-plane (OOP) systems, the dumbbell model is inadequate due to the small fringe fields and the limited thickness of the magnets.
Instead, the second-order dipole correction is more appropriate, yielding a significant improvement towards the ideal \mumax curve.
Still, a discrepancy remains for separations below $r_{ij}/2r \lessapprox 1.5$, which could be reduced by even higher-order corrections. \\\par

For IP systems, the dumbbell model constitutes a vast improvement over the standard point dipole treatment.
The dumbbell model does, however, require an additional parameter $d$, which affects the interaction energy.
For the best correspondence with \mumax, the monopole-monopole distance $d$ should be set slightly shorter than the length $l$ of a magnet, typically around $d/l\approx0.9$.
This adjustment accounts for the curvature and corresponding non-uniform magnetisation at the ends of real nanomagnets.
Similar values for $d/l$ were previously found in~\ccite{DDG_Masterproef} for typical nanomagnet shapes like ellipses and stadiums. \par
In contrast, the second-order dipole correction has little effect in IP systems and can even increase the discrepancy with \mumax. It emulates increased spatial extent and therefore always increases the interaction, but for magnets neighbouring along their hard axes (rightmost panel in~\cref{fig:2:MS_distance}) the point dipole model already overestimates the interaction. \par
In conclusion, the dumbbell model is preferred for IP systems, while the second-order dipole correction is most suitable for OOP systems.

\subsection{Effective energy barrier}\label{sec:2:E_B_eff}
To properly simulate ASI dynamics and hysteresis, knowledge of the effective energy barrier $\EBeff$, which separates the two magnetisation states of each magnet, is crucial.
Recall that this quantity is distinct from the shape anisotropy $\EB$: the effective energy barrier $\EBeff$ is a modification of $\EB$ caused by the interaction with other magnets. \par
This is illustrated in~\crefSubFigRef{fig:2:EB_meanbarrier}{b}, where the energy landscape for $\Delta E = 0$ reveals the shape anisotropy $\EB$ while slanted energy landscapes for $\Delta E \neq 0$ show how $\EBeff$ is changed by these interactions.
The figure shows that $\EBeff$ can be calculated at various levels of accuracy, which may yield different switching rates or even a different switching order~\cite{leo2021chiral}. \par
In this section, we will explore some of these approximations.
All of them make use of the switching energy $\Delta E$ between the current and opposite state of a magnet, since this is cheap to compute\footnote{
	Recall that the terms in \cref{eq:2:E} simply change sign when the magnetisation $\vc{\mu}$ of a magnet is reversed.
} as $\Delta E = -2 E$.

\xfig[1.0]{2_Hotspice/EB_meanbarrier.pdf}{
	Mean-barrier approximation (\ref{eq:2:EB_meanbarrier_original}, \textcolor{lightblue}{blue}) and its conditional form (\ref{eq:2:EB_meanbarrier_cases}, \textcolor{lightred}{red}) compared to the exact energy barrier (\ref{eq:2:EB_exact}, \textcolor{mplgrey}{grey}).
	Due to the limited information available to a mean-barrier model (only $\EB$ and $\Delta E$), the exact solution assumes a sinusoidal shape anisotropy and a uniform effective external field along the easy axis.
	\textbf{(a)} Effective energy barrier $\EBeff$ as function of switching energy $\Delta E$ for these approximations.
	\textbf{(b)} Energy landscape between the two stable states (black filled circles) for several values of the switching energy $\Delta E$.
	The energy landscapes without and with shape anisotropy are shown as dotted black and solid grey lines, respectively.
	An open circle indicates the halfway point, while a star is put at the top of the exact energy barrier.
	On each landscape, the height of the barrier in the different approximations is indicated by a dotted horizontal line, indicating how they use different values on the energy landscape for their estimate.
	\label{fig:2:EB_meanbarrier}
}

We will omit the subscript $i$ for the remainder of this section; it is implied that the following discussion and equations apply to each magnet individually.

\subsubsection{Intrinsic barrier due to shape anisotropy} % TODO END: is any of this more relevant in the introduction?
An isolated single-domain nanomagnet often exhibits two stable magnetisation states separated by energy barriers.
In IP ASI, these states usually arise from shape anisotropy, which originates from the demagnetising field of the magnet itself.
This imposes an additional energy cost when the magnetisation does not point along the preferential ``easy axis'', which is usually the longest axis of the magnet's geometry~\cite{PhD_Leliaert}. \par
Shape anisotropy is quantified by a uniaxial anisotropy constant $K_\mathrm{u}$.
Assuming that switching occurs by coherent rotation\footnote{
	Depending on the magnet's properties, non-coherent reversal such as switching by domain wall nucleation may be energetically cheaper.
	An accurate estimate of the energy barrier for such processes can be determined by e.g. micromagnetic simulations.
}, this leads to an energy barrier $\EB = K_\mathrm{u} V$, with $V$ the magnet's volume.
Similar to the calculation of the magnetic moment $\mu$ in \cref{fn:2:moment_integral}, relaxation of the magnetisation profile $\vc{M}(\vc{r})$ near the surface of the magnet may cause the actual energy barrier to be slightly smaller.
\hotspice ignores the specifics of the reversal process by allowing the user to set an arbitrary value of $\EB$. \par
Magnets in OOP ASI are usually very flat, so one would expect the shape anisotropy to result in a preferential in-plane magnetisation direction.
This can be avoided by properly choosing materials that exhibit a strong interfacial anisotropy, as occurs at e.g. the Co/Pt interface, resulting in a net preferential out-of-plane magnetisation.

\subsubsection{Mean-barrier approximation} % TODO: mention relevant choices with discontinuities, expand this explanation
The simplest approximation of the effective energy barrier $\EBeff$ is the \textit{mean-barrier approximation}.
It assumes that the highest-energy state lies halfway between the two stable states.
This means the barrier height changes at half the rate at which the switching energy $\Delta E$ changes, leading to the approximation % TODO: illustrate with a simple figure
\begin{equation}
	\label{eq:2:EB_meanbarrier_original}
	\EBeff = \EB + \frac{\Delta E}{2} \mathrm{,}
\end{equation}
as it is often encountered in literature~\cite{MC_TemperatureDesorption,DirectionalEnergyBarrier}.
This method is illustrated in blue in~\cref{fig:2:EB_meanbarrier}. \par
However, this is a very crude approximation which does not account for the extreme cases where the interactions are so strong that the energy barrier effectively disappears ($\abs{E} \gg \EB$).
In the model of \cref{eq:2:EB_meanbarrier_original}, this happens when $\abs{\Delta E}$ exceeds twice the shape anisotropy $\EB$, leaving only one global minimum.
This can be seen in \crefSubFigRef{fig:2:EB_meanbarrier}{b} for $\Delta E = \pm 4 \EB$, where \cref{eq:2:EB_meanbarrier_original} clearly does not make physical sense.
To handle these situations, \hotspice modifies the calculation of the effective energy barrier $\EBeff$ as follows:
\begin{equation}
	\label{eq:2:EB_meanbarrier_cases}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} + \frac{\Delta E}{2} & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B}, \\
		\Delta E & \quad \text{otherwise}.
	\end{cases}
\end{equation}
This way, $\Delta E$ serves as the barrier when the original energy barrier disappears.
This adjustment is shown in red in~\cref{fig:2:EB_meanbarrier}: it is closer to the exact solution except for $-4 \EB < \Delta E < -2 \EB$.
Notwithstanding that the original mean-barrier approximation~\eqref{eq:2:EB_meanbarrier_original} lies closer to the exact solution in this range, we choose not to use it as this lies outside its valid range of $\abs{\Delta E} < 2 \EB$.

\subsubsection{Asymmetric barrier}
The simple mean-barrier approximation is insufficient for many IP ASI.
In real nanomagnets, reversal by coherent rotation can occur via two pathways; clockwise ($\clockwise$) or counter-clockwise ($\counterclockwise$) rotation of the magnetisation.
In an asymmetrical environment --- when the effective field has a non-zero component perpendicular to the easy axis --- one of these two rotation directions will be preferred~\cite{leo2021chiral,DirectionalEnergyBarrier}.
Take for example the pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}): any two neighbouring magnets form a T-shape, so the magnet pointing into the side of the other will greatly influence whether the other magnet prefers $\clockwise$ or $\counterclockwise$ rotation~\cite{DirectionalEnergyBarrier}. % TODO: also make a figure for this. Possibly combine with the mean-barrier figure.
Such an asymmetry can not occur in OOP ASI, so the asymmetric barrier is only applicable to IP ASI.
Accounting for the existence of these separate chiral switching channels profoundly affects the switching rates and transition kinetics, since switching will occur predominantly via the more favourable pathway~\cite{leo2021chiral}. \\\par

In the dipole model, this can be accounted for by considering the energy of each magnet in these transitional ``perpendicular'' states.
Therefore, \hotspice tracks yet another quantity for every magnet: $E_\perp$, representing the interaction energy of a magnet if it would point \ang{90} counter-clockwise from its normal magnetisation direction $\vc{u}$.
Note that we are not deviating from the two-state Ising model: we are simply putting ``test dipoles'' in the perpendicular orientation to get a better estimate of $\EBeff$, but magnets will never end up in these states during a simulation. \par
In \cref{eq:2:EB_meanbarrier_cases}, $\EB$ essentially represented an estimate of the total energy in the perpendicular state.
With our newfound knowledge of the interaction energy $E_\perp$ in that state, our improved estimate of the total perpendicular energy becomes $\EB + E_\perp$.
Therefore, we get an expression for the effective barrier $\EBeff$ along the two rotation pathways ($\pm$) by simply substituting $\EB \rightarrow \EB + E_\perp$ in \cref{eq:2:EB_meanbarrier_cases}:
\begin{equation}
	\label{eq:2:EB_asymmetric}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} \pm \rho E_\perp + \frac{\Delta E}{2} & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B} \pm \rho E_\perp, \\
		\Delta E & \quad \text{otherwise}, \\
	\end{cases}
\end{equation}
which results in two different barriers if $E_\perp \neq 0$.
The parameter $\rho = \mu_\perp/\mu_\parallel > 0$ was introduced in \cref{eq:2:EB_asymmetric} to account for non-coherent magnetisation reversal processes like domain wall nucleation and propagation, which result in an effective reduction of the magnetic moment during reversal.~\cite{leo2021chiral,TimeResolvedDynamicsSOT}
Using a value $\rho < 1$ improves correspondence with experimental observations, as we will observe in~\cref{sec:3:IP_Pinwheel_reversal}. \par
As a bonus, we can use the combined knowledge of $E$ and $E_\perp$ to implicitly define the effective field $\vc{B}_\mathrm{eff}$ that any given magnet experiences by
\begin{equation}
	\label{eq:2:B_eff_implicit}
	\begin{cases}
		\vc{\mu} \,\cdot\, \vc{B}_\mathrm{eff} = E \mathrm{,} \\
		\norm{\vc{\mu} \times \vc{B}_\mathrm{eff}} = E_\perp  \mathrm{.}
	\end{cases}
\end{equation}
For IP ASI, this uniquely defines $\vc{B}_\mathrm{eff}$ since all vectors lay in-plane. Solving \cref{eq:2:B_eff_implicit} for $\vc{B}_\mathrm{eff}$ yields an expression as function of $E$, $E_\perp$ and $\vc{\mu}$, valid for both IP and OOP ASI:
\begin{equation}
	\label{eq:2:B_eff_explicit}
	\vc{B}_\mathrm{eff} = - \frac{E \vc{\mu} + E_\perp \vc{e}_z \times \vc{\mu}}{\mu^2} \mathrm{.}
\end{equation}

\subsubsection{Exact solution}
The effective energy barrier $\EBeff$ can be calculated exactly if an analytical expression is known for the energy as a function of magnetisation angle $\theta$ relative to the magnet's easy axis.
The shape anisotropy creates a basic energy profile with two minima at $\theta=0$ and $\theta=\pi$, but the exact form of this profile depends on the magnet's shape.
For ellipsoidal magnets, the energy landscape is $-\frac{E_\mathrm{B}}{2} \cos{2\theta}$~\cite{neel1949theorie}.
Assuming a uniform magnetisation in each magnet\footnote{
	Only perfectly ellipsoidal magnets have uniform magnetisation in a uniform external field~\cite{EllipsoidDemag,MaxwellElectricityMagnetism}.
}, the magnetostatic and Zeeman interactions add a term proportional to $\cos(\theta - \phi)$, with $\phi$ the angle of their combined effective field $\vc{B}_\mathrm{eff}$.
Thus, the total profile is a sum of two sines and can be fully characterised if $E$ and $E_\perp$ are known.
However, in the general case, this results in a transcendental equation
\begin{align*}
	\label{eq:2:EB_exact_transcendental}
	\frac{\partial E(\theta)}{\partial \theta} = 0 \iff & \frac{\partial}{\partial \theta} \ab(-\frac{\EB}{2} \cos{2\theta} - \mu B_\mathrm{eff} \cos(\theta - \phi)) = 0 \\
	\iff & \EB \sin{2\theta} + \mu B_\mathrm{eff} \sin(\theta - \phi) = 0 \mathrm{,} \numberthis
\end{align*}
which would require numerical approximation to solve.
Since this could significantly impact performance, this is not done in \hotspice. \par
In OOP ASI, however, the high degree of symmetry nonetheless allows an explicit expression to be obtained.
All relevant vectors ($\vc{B}_\mathrm{eff}$, $\vc{\mu}$ ...) in such systems point along the z-axis, causing the perpendicular energy to vanish ($E_\perp=0$) and the equation to no longer be transcendental.
Solving \cref{eq:2:EB_exact_transcendental} and applying the double-angle identities for $\cos$ and $\sin$ instead leads to the quadratic relation % Q: should I derive this from the d/dtheta equations or is this clear enough? (only needs double-angle rules for cos and sin basically)
\begin{equation}
	\label{eq:2:EB_exact}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} \ab(\frac{\Delta E}{4 E_\mathrm{B}} + 1)^2 & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B}, \\
		\Delta E & \quad \text{otherwise,} \\
	\end{cases}
\end{equation}
as has previously been described by \textit{Tannous and Gieraltowski}~\cite{StonerWohlfarth2008}.
This is the exact solution that was shown in grey in~\cref{fig:2:EB_meanbarrier}.
However, since OOP magnets are not ellipsoidal as assumed by the transcendental equation that originally led to \cref{eq:2:EB_exact_transcendental}, the relevance of \cref{eq:2:EB_exact} remains questionable.

\section{Dynamics}\label{sec:2:Dynamics}
A Monte Carlo simulator would not be complete without an algorithm to change the state of the system.
Since any magnet in the ASI may spontaneously switch to the opposite magnetisation state due to thermal fluctuations, \hotspice evaluates the time evolution of the ASI in a stepwise manner using an update algorithm that determines which magnet should switch next. \par
To achieve this, \hotspice uses kinetic Monte Carlo (KMC) algorithms, sometimes also referred to as Dynamic Monte Carlo.
KMC algorithms can typically be divided into two distinct classes: rejection-free and rejection KMC.
\hotspice implements algorithms of both types.
These techniques go by many names, but in the context of ASI we shall refer to them as \textit{N\'eel-Arrhenius switching} and \textit{Metropolis-Hastings}, respectively~\cite{PhysicalTimeKMC}.
The former is more suitable for simulating the temporal evolution of the system, while the latter can be used to sample the equilibrium distribution of the state space.

\subsection{N\'eel relaxation: temporal evolution}
N\'eel relaxation theory~\cite{neel1949theorie} states that, for an isolated nanomagnet, the switching rate $\nu$ is given by the N\'eel-Arrhenius equation
\begin{equation}
	\nu = \nu_0 \exp\ab(-\frac{\EB}{\kBT}) \mathrm{,}
	\label{eq:2:Néel}
\end{equation}
with $\kBT$ the thermal energy and $\nu_0$ the so-called \textit{attempt frequency}.
For mutually interacting magnets, an adjusted version of \cref{eq:2:Néel} can be used where $\EB$ is replaced by the effective energy barrier $\EBeff$. \par
In the general case where the energy barriers for clockwise and anticlockwise rotation during switching differ, these two switching channels ($\circlearrowright$ and $\circlearrowleft$) will separately follow~\cref{eq:2:Néel}, so their switching frequencies must be combined.
This yields the total switching rate as presented by \textit{Koraltan et al.}~\cite{DirectionalEnergyBarrier},
\begin{equation}
	\nu = \nu_\circlearrowleft + \nu_\circlearrowright = \frac{\nu_0}{2} \ab[\exp\ab(-\frac{\EBeffLeft}{\kBT}) + \exp\ab(-\frac{\EBeffRight}{\kBT})] \mathrm{,}
	\label{eq:2:Néel_2}
\end{equation}
where we assigned a halved attempt frequency $\nu_0/2$ to either switching channel such that~\cref{eq:2:Néel_2} reduces to~\cref{eq:2:Néel} in the case of $\EBeffLeft=\EBeffRight$~\cite{leo2021chiral}.
Note that it does not matter which barrier corresponds to clockwise or counter-clockwise rotation; only the height of both barriers matters when determining the switching rate of a magnet. \par

\paragraph{Attempt frequency}
An estimate of $\nu_0$ for coherent magnetisation reversal can be obtained from the limit $\EB \rightarrow 0$, where the switching rate $\nu$ should approach the gyromagnetic precession frequency of the magnetisation of a nanomagnet.
This is reported to be on the order of \qtyrange{e9}{e10}{\hertz}~\cite{BrownThermalFluctuations,bean1959superparamagnetism}.
As $\EB \rightarrow 0$, \cref{eq:2:Néel} implies that $\nu \rightarrow \nu_0$, so we use $\nu_0=\qty{e10}{\hertz}$~\cite{JM_Masterproef}. \par % Each oscillation is an attempt to leave this energy minimum.
A more precise value for $\nu_0$ for a particular simulation could be achieved by comparing with a similar experiment.
However, an order-of-magnitude estimate often suffices because any small (i.e., $\sim \kBT$) change of $\EB$ will translate to an exponential change of the switching rate.
As discussed in~\cref{sec:2:E_B_eff}, our approximations of $\EBeff$ are imperfect, thus rendering an accurate value of $\nu_0$ irrelevant.
Furthermore, $\nu_0$ only affects the absolute value of the elapsed time, not the switching order, so its overall importance to the simulation is limited. \par
The values for $\nu_0$ on the order of \SI{e10}{\hertz} are valid for reversal by coherent rotation, as was already assumed in \hotspice during the calculation of the effective barriers.
For other reversal processes, e.g. domain wall-mediated reversal, the attempt frequency could be many orders of magnitude faster because its interpretation as a rotation frequency then no longer holds~\cite{ArrheniusPrefactor}. % Another ref: 45 in leo2021chiral
It must also be noted that the attempt frequency can significantly depend on other system parameters.
For example, the energy barrier $\EB$ itself may be temperature-dependent, which can ultimately be captured by a temperature-dependent attempt frequency~\cite{AttemptFreqTemperature}. % Ref says: ``When this energy barrier is the result of a collective statistical behavior of many constituents, it may contain an intrinsic temperature dependence which has to be carefully taken into account, in particular when the prefactor is interpreted in terms of an attempt frequency.''

\paragraph{Algorithm}
Knowledge of the switching rate $\nu$ can be used to construct a rejection-free kinetic Monte Carlo method, which we refer to as the N\'eel-Arrhenius update algorithm.
Each iteration, as presented in~\cref{alg:NeelArrheniusSingle}, will increment the elapsed time by a certain duration $t \leq t_\mathrm{max}$, where the maximum time $t_\mathrm{max}$ can be set by the user to prevent excessively long switching times.
When using the N\'eel-Arrhenius update scheme, \cref{alg:NeelArrheniusSingle} is simply performed as many times as needed to reach the desired elapsed time. \par
\begin{algorithm}[Single N\'eel-Arrhenius iteration]
	\label{alg:NeelArrheniusSingle}
	\begin{enumerate}[rightmargin=15pt]
		\item Calculate the effective energy barriers of all magnets (i.e., $\EBeff{}_{,i}$ for OOP ASI, $\EBeffLeft{}_{,i}$ and $\EBeffRight{}_{,i}$ for IP ASI, $\forall i$) based on their interaction energies $E_i$, as determined by the current magnetisation state.
		\item Calculate the switching rate $\nu_i$ of each magnet using \cref{eq:2:Néel_2}.
		\item Generate a random switching time interval $\Delta t_i$ for each magnet $i$, sampled from an exponential distribution with mean value $1/\nu_i$.
		\item Determine which magnet $j$ has the smallest such time $\Delta t_j = \min_i \Delta t_i$.
		\item Finally, $t_\mathrm{max}$ determines whether a switch occurs.
		\begin{itemize}
			\item \textit{If $t + \Delta t_j \leq t_\mathrm{max}$}: increment the elapsed time $t$ by $\Delta t_j$ and switch magnet $j$.
			\item \textit{If $t + \Delta t_j > t_\mathrm{max}$}: increment the elapsed time $t$ by $t_\mathrm{max}$ without switching a magnet.
		\end{itemize}
	\end{enumerate} % NOTE: this is different from the BKL algorithm, yet still gives the exact same result.
\end{algorithm}
% Note that this algorithm still gives the correct switching rate $\nu$, even though the switching time $t_i$ is sampled randomly multiple times before magnet $i$ typically switches. This is because, the more magnets there are, the harder it gets for an arbitrary magnet $i$ to have the shortest randomly sampled time (and thus to switch), but the elapsed time increment after each switch will be smaller as well, and these two effects cancel perfectly to end up with the same switching rate as if the magnet would be the only one switching.
The maximum time $t_\mathrm{max}$ (default value of one second) prevents the simulation from advancing too far into the future, as the exponential character of the N\'eel-Arrhenius law can cause switching times to become much longer than what could ever be observed experimentally.
It can also be used to apply time-dependent external fields.
For example, when a sinusoidal signal of frequency $f$ is applied to the lattice, using $t_{\mathrm{max}}=20/f$ will ensure that the waveform is captured in sufficient detail. \par
In the realm of rejection-free KMC, a commonly used algorithm is the Bortz-Kalos-Lebowitz (BKL) algorithm.
While the procedure detailed in~\cref{alg:NeelArrheniusSingle} is not the same, its yields an identical result as the BKL algorithm. % TODO: add reference to this

\subsection{Metropolis-Hastings: sampling equilibrium states} \label{sec:2:Dynamics_MH}
% TODO: decide whether to talk about nomenclature details etc. here, or in the introduction.
Metropolis-Hastings is a rejection-based kinetic Monte Carlo (KMC) method designed to sample the state space at thermal equilibrium, where the probability of each state appearing is proportional to their Boltzmann factor $e^{-E/\kBT}$~\cite{IntroductionMC,kyimba2006comparisonIsingAlgorithms}.
Hence, in contrast to N\'eel-Arrhenius switching, Metropolis-Hastings is not intended to accurately model the system's transient dynamics and is instead more suitable for examining equilibrium statistical quantities in ASI, like the average magnetization, heat capacity, correlation...~\cite{ApparentFMpinwheel} \par

% TODO: talk about acceptance probability here and detailed balance and ergodicity etc. etc.

\paragraph{Algorithm}
A rejection-based KMC method works by selecting magnets at random, and deciding whether or not to switch them with a certain probability.
More rigorously, the algorithm repeats the following steps:
\begin{algorithm}[Single Metropolis-Hastings iteration]
	\begin{enumerate}
		\item Select a magnet $i$ at random (with all magnets equally likely to be chosen).
		\item Calculate the energy change $\Delta E_i$ if this magnet were to switch.
		\item Switch the magnet with probability
		\begin{equation}
			P_i = \begin{cases}
				\exp(-\Delta E_i/\kBT), & \text{if } \Delta E_i > 0 \mathrm{,} \\
				1 & \text{otherwise} \mathrm{.}
			\end{cases}
		\end{equation} % This MH formulation acceptance probability will converge faster than the GD acceptance probability for equilibrium studies since any attempted switches without barrier are guaranteed to switch, whereas the GD acceptance probability may reject such switches.
		\item \textit{Optional}:
		Increment the elapsed time $t$ by
		\begin{equation}
			\Delta t = -\frac{\exp\ab(\EBeff\big/\kBT\ab) \ln{\chi}}{N \nu} \mathrm{,}
			\label{eq:Metropolis_time}
		\end{equation}
		with $N$ the number of magnets in the system and $\chi$ a uniformly distributed random variable in $(0,1]$.~\cite{PhysicalTimeKMC}
	\end{enumerate}
\end{algorithm}
This algorithm satisfies detailed balance and ergodicity, thus ensuring equilibrium is eventually reached, though the rate of convergence may vary as noted in~\cref{sec:2:Verification_OOP_Exchange}~\cite{jang2004stochastic}.
For enhanced performance, multiple sufficiently distant magnets can be selected simultaneously, see~\cref{sec:2:MultiSwitch}. \par
While N\'eel relaxation relies on an explicit calculation of the elapsed time, Metropolis-Hastings does not strictly require this knowledge.
Therefore, the last step of the algorithm --- the calculation of the elapsed time --- is optional.
For a long time, the notion of a well-defined elapsed time in rejection KMC was controversial~\cite{nfoldMCalgorithm,GlauberTimescale_sadiq1984,MCSim_StatPhys}. % Refs: `nfoldMCalgorithm` mentions both Glauber & MH acceptance prob., but says that Glauber can have a timescale (their equation looks a lot like in PhysicalTimeKMC) yet Metropolis cannot (due to the max()?), which is no longer true. `GlauberTimescale_sadiq1984' uses a less rigorously defined yet similar expression as in `PhysicalTimeKMC`. Don't use this reference when referring to the lack of a timescale, because this one seems to be one of the better solutions presented in literature, yet not very rigorous. Also `Lattice Kinetic Descriptions for Bulk Reaction-Diffusion Processes: Application to Alloys under Irradiation' p.215 says that ``The relationship between the simulation time and the physical time has been widely debated [MCSim_StatPhys], and it is sometimes assumed that the two time scales are proportional.''
Often, the number of performed Monte Carlo sweeps\footnote{
	A Monte Carlo ``step'' refers to attempting to switch a single magnet. By a ``Monte Carlo sweep'' (MCS), we refer to $N$ attempted switches when the simulation contains $N$ magnets~\cite{NumericalDynamicalNiedermayer}. Note that the distinction between attempted switches and actual switches only exists in the Metropolis-Hastings algorithm.
} was used as a crude measure of ``elapsed time'', but eventually a formal derivation for the physical time scale in rejection KMC was presented by \textit{Serebrinsky}~\ccite{PhysicalTimeKMC}, where~\cref{eq:Metropolis_time} was derived.
Note that the effective energy barrier $\EBeff$ only appears in the calculation of the elapsed time: it does not influence the switching probability in the Metropolis-Hastings algorithm because energy barriers do not affect the equilibrium state.

\paragraph{Nomenclature disambiguation}
There often exists confusion between the Metropolis-Hastings algorithm and so-called ``Glauber dynamics''. % TODO: discuss acceptance probabilities and why they are correct (ergodicity, detailed balance). Can extend the small mention of detailed balance and ergodicity below the algorithm environment as well.

\subsection{Other Monte Carlo algorithms} % Not implemented, explain why Wolff could be useful but why it's not in Hotspice
For a broader overview of Monte Carlo methods, we refer to Ref.~\cite{IntroductionMC}, which may also clarify the at times confusing naming present throughout literature.

\section{Implementation}\label{sec:2:Implementation}
Now that the physics underlying the simulator have been extensively discussed, we turn our attention to the software implementation. \par
First, we discuss why we chose to implement the ASI on an underlying rectilinear grid, and explain how the \textit{`kernel'} --- the lookup table for the magnetostatic interaction --- was implemented.
We then take a look at the performance of \hotspice and how it has improved since its initial version due to improvements to the kernel.
One particular performance-enhancing feature is examined in more detail: the possibility to select multiple magnets at once in the Metropolis-Hastings algorithm.
Finally, we briefly discuss the structure of the software package and the functionality included in the various submodules, and finish with a short discussion of things that could be improved.

\subsection{Grid}
\hotspice{} represents an ASI as a rectilinear grid of non-uniform unit cells, with magnets positioned at selected grid points.
This choice was made based on the trade-off between calculation efficiency and the freedom to place magnets arbitrarily.
We opted to prioritise efficiency and accept the geometrical restriction, as most ASI research focuses on periodic lattices.
All quantities are therefore stored in 2D matrices of size $L_x \times L_y$, upon which operations can be performed efficiently.
We will denote these matrices with bold upright capitalised symbols: e.g., $\vc{S}$ contains the states $s_i$, $\vc{E_\mathrm{MC}}$ the magnetostatic energies, $\vc{\upmu}$ the magnitudes of magnetic moments... \par % Q: other suggestions? I can not capitalise the magnitudes of magnetic moments, because that would just be capital M which is too reminiscent of the magnetisation (in H = B/mu0 - M). So I just put it upright. Is that sufficient?
Despite the seemingly restrictive nature of the rectilinear grid,~\cref{fig:2:ASIs} illustrates its versatility in forming various periodic lattices, with only the Cairo lattices requiring grid non-uniformity.
As a bonus, real-time visualisation is simple and efficient using this approach, as the underlying matrix can directly be cast to a pixel image. \par
By leveraging the unit cell concept in periodic lattices and the efficient indexation of a rectilinear grid in computer memory, several aspects of the calculation can be performed more efficiently than for free-form ASI.
%The unit cell of each lattice in~\cref{fig:2:ASIs} is depicted as a grey rectangle.
Although non-rectilinear unit cells with fewer magnets can be identified for some lattices, such unit cells would increase complexity without significant benefit: the amount of sites in a unit cell only determines the required memory, not the performance of the simulation.

\subsection{Kernels for magnetostatic interaction}\label{sec:2:Kernels}
Pre-calculated ``kernels'' are used to efficiently update the magnetostatic interaction after each switch. 
The need to calculate these kernels beforehand increases the initialisation time, but the reduced runtime when simulating dynamics more than makes up for this.

\subsubsection{Magnetostatic interaction kernel}
For each magnet $i$, a kernel $\vc{k}^{(i)}$ stores the magnitude of the magnetostatic interaction between itself and all other magnets.
By calculating these values beforehand, when the ASI is created, the magnetostatic interaction energy between magnets $i$ and $j$ can readily be calculated as
\begin{equation}
	E_{\mathrm{MS},i,j}= s_i s_j \vc{k}^{(i)}_j \mathrm{,}
\end{equation}
which can only change sign, since the states $s_i = \pm 1$ and $s_j = \pm 1$ are the only variables in the system. \par
Due to the fact that magnets are placed on a rectilinear grid, the kernel of magnet $i$ can also be written as an $L_x \times L_y$ matrix $\vc{K}^{(i)}_{ab}$ where $ab$ denotes a position on the ASI grid.
For example, $\vc{K}^{(i)}_{2,3}$ stores the interaction strength between magnet $i$ and the magnet at index\footnote{The indexation used throughout this section starts counting at 1, as is typical for matrix notation. This is not to be confused with indexation in source code, which starts at 0.} $(2,3)$ on the grid.
If no magnet was placed at index $(a,b)$ on the grid, then $\vc{K}^{(i)}_{ab} = 0, \forall i$. \\\par

However, storing such a kernel for all magnets $i$ would require storing $\order{N^2}$ values in memory.
By leveraging unit cells, this storage requirement can be reduced to $\order{N}$.
Each occupied grid point in the unit cell is assigned a unique index $q = 1,\dots,\widetilde{N}$, with $\widetilde{N}$ the number of magnets in a single unit cell.
Hence, each magnet in the ASI is associated with a specific value of $q$. \par
This way, all magnets with the same value of $q$ share the same layout of surrounding magnets\footnote{
	In some ASI, it is possible that different sites inside a single unit cell also experience the same surrounding layout.
	This can be used to further improve memory usage (but not performance) by only storing unique kernels.
	This was not implemented as this constitutes only a minor improvement; for the lattices in~\cref{fig:2:ASIs}, this would at best half the number of kernels.
}, apart from a different cut-off at the border in case of open boundary conditions.
Therefore, if two magnets occupy equivalent positions $q$ in the unit cell --- say, $i$ at index $(x,y)$ on the grid and $j$ at $(x+\Delta x, y+\Delta y)$ --- then
\begin{equation}
	\label{eq:2:Kernel_equivalence}
	\vc{K}^{(i)}_{ab} = \vc{K}^{(j)}_{a+\Delta x,b+\Delta y} \quad \mathrm{,} \quad \forall a,b:
	\begin{cases}
		-\Delta x < a \leq L_x - \Delta x \\
		-\Delta y < b \leq L_y - \Delta y \\
		% Or more rigorously, but even more overcomplicated:
		% \max(-\Delta x, 0) \leq a < L_x - \min(\Delta x, 0) \\
		% \max(-\Delta y, 0) \leq b < L_y - \min(\Delta y, 0) \\
	\end{cases} \quad\mathrm{.}
\end{equation}
In other words, their kernels are identical apart from an offset by $(\Delta x, \Delta y)$, at least in the area that remains inside the kernel after this offset.
Due to this equivalence, and the fact that $- L_x < \Delta x < L_x$ and $- L_y < \Delta y < L_y$, all possible interactions that a magnet at site $q$ can experience can be stored in a single $(2L_x-1) \times (2L_y-1)$ matrix $\vc{\mathcal{K}}^{(q)}_{ab}$, a ``unit cell kernel''.

\xfig[1.0]{2_Hotspice/Kernel_IP_Pinwheel.pdf}{
	\label{fig:2:Kernel_IP_Pinwheel}
	Two examples showing the kernel shifting with respect to the ASI grid for multiplication (convolution), when a magnet switches.
	The ASI itself is shown on the left.
	Thin dotted lines indicate individual cells in the simulation grid while thick solid lines delineate the boundaries of unit cells.
	Letters indicate the index of a magnet within its unit cell.
	Each site in the unit cell corresponds to a kernel, shown in the right.
	The values stored in the kernel are designed such that shifting the switching magnet to the centre of the kernel and multiplying overlapping cells ($s_{ab}$ of the ASI with $\vc{K}_{ab}$ of the kernel) yields the magnetostatic interaction energy between the switching magnet and all other magnets at positions $ab$.
	The figure shows this for two example magnets indicated in \textcolor{lightblue}{blue} and \textcolor{lightred}{red}.
}

\cref{fig:2:Kernel_IP_Pinwheel} shows the structure of such unit cell kernels $\vc{\mathcal{K}}$ to clarify the interpretation of how they store the magnetostatic interaction strength between a particular magnet and any other magnet in the system.
This is most easily illustrated by an example.
Consider the blue magnet in the $5 \times 5$ pinwheel lattice on the left side of the figure, which occupies position $q=\mathrm{A}$ in the unit cell.
The corresponding $9 \times 9$ kernel $\vc{\mathcal{K}}^\mathrm{(A)}$ is shown schematically, also in blue.
It stores the magnetostatic interactions in such a way that, when the ASI grid is shifted onto the kernel to put the blue magnet at the centre of the kernel, the interaction strength of the blue magnet with any other magnet in the ASI is stored in the kernel elements where those other magnets end up.
So, in the figure, these are the elements in kernel A shown to contain a grey magnet\footnote{
	The exact values stored at these elements are omitted as they are irrelevant to illustrate the concept. See \cref{fig:2:Kernel_PBC} for an example of the particular values stored in kernel A for a larger $16 \times 16$ lattice.
}. \par
The concept is illustrated a second time, now for the red magnet in the pinwheel ASI, which instead occupies position $q=\mathrm{B}$ in the unit cell and therefore uses a different kernel; the red kernel $\vc{\mathcal{K}}^\mathrm{(B)}$. \\\par

This construction can be used to efficiently update the magnetostatic energy $E_\mathrm{MC}$ of all magnets whenever a magnet switches.
Recall that the grid stores the state $s_i$ of all magnets in a matrix $\vc{S}$ and the size of their magnetic moment $\mu_i$ in $\vc{\upmu}$.
When a magnet at unit cell index $q$ switches, the change of magnetostatic energy for all other magnets can be calculated by shifting $\vc{S}$ in the same way as the ASI was shifted in \cref{fig:2:Kernel_IP_Pinwheel}, and performing a pointwise multiplication (Hadamard product, denoted by $\odot$) with the respective elements of the kernel $\vc{\mathcal{K}}^{(q)}$.
Denoting the grid index of the switching $q$-site magnet as $(x,y)$ and using the states $\vc{S}$ after the switch, this yields a new matrix
\begin{equation}
	\label{eq:2:Kernel_update}
	\Delta \vc{E_\mathrm{MC}} = 2\mu_{xy} s_{xy}\vc{\upmu} \odot \vc{S} \odot \ab(\vc{\mathcal{K}}^{(q)}_{ab})_{\substack{L_x < a + x \leq 2L_x \\ L_y < a + y \leq 2L_y}} \quad\mathrm{,}
\end{equation}
containing the change in magnetostatic energy $\Delta E_\mathrm{MC}$ for each grid point, which can then simply be added to the current values of $E_\mathrm{MC}$. \par
The kernel is also used to initialise the magnetostatic energy $\left. E_\mathrm{MS} \right|_{t=0}$ of each magnet at the start of the simulation.
This is done by performing a convolution ($*$) rather than a pointwise multiplication:
\begin{equation}
	\label{eq:2:Kernel_init}
	\left. \vc{E_\mathrm{MC}} \right|_{t=0} = \sum_q \vc{Q_q} \odot \vc{\upmu} \odot \vc{S} \odot \ab((\vc{\upmu} \odot \vc{S}) * \vc{\mathcal{K}}^{(q)}) \mathrm{,}
\end{equation}
where only the central $L_x \times L_y$ area of the convolution is calculated and the matrix $\vc{Q_q}$ contains 1 at sites with unit cell index $q$, otherwise 0. \\\par

The exact values stored in the kernel depend on whether any finite-size corrections to the magnetostatic interaction (\cref{sec:2:finite}) were used and whether the ASI is of the IP or OOP type. For example, an IP kernel using the point dipole approximation contains elements
\begin{equation}
	\label{eq:2:Kernel_detailed}
	\vc{\mathcal{K}}^{(q)}_{ab} = \frac{\mu_0}{4 \pi} \frac{
		u_x^{(q)} u_x^{(ab)} \ab(1 - 3 \ab(\Delta x)^2) + u_y^{(q)} u_y^{(ab)} \ab(1 - 3 \ab(\Delta y)^2) - 3\Delta x \Delta y \ab(u_x^{(q)} u_y^{(ab)} + u_x^{(ab)} u_y^{(q)})
	}{
		\sqrt{\ab(\ab(\Delta x)^2 + \ab(\Delta y)^2)^5}
	} \mathrm{,}
\end{equation}
with $\Delta x$ and $\Delta y$ the x- and y-distance between the central magnet $(q)$ and a magnet at position $(ab)$ in the kernel (if a magnet exists at the corresponding point in the ASI grid) and $\vc{u}^{(i)}$ a unit vector along the easy axis of magnet $i$. \par
With this form for the kernel, the magnetostatic interaction between a magnet $i$ at grid-index $(v,w)$ and another magnet $j$ at index $(v+x, w+y)$ can be calculated as
\begin{equation}
	\label{eq:2:Kernel_unitcell}
	E_{\mathrm{MS},i,j} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(q_i)}_{L_x+x, L_y+y} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(q_j)}_{L_x-x, L_y-y} \quad \mathrm{,}
\end{equation}
if the indexation of $\vc{\mathcal{K}}^{(q)}$ starts at $(1,1)$.
Note that $\mu_i$ and $\mu_j$ were not included in the unitcell-kernel to allow magnets in different unit cells but with the same unitcell-index $q$ to have a different magnetic moment.
This versatility comes at the cost of two additional multiplications per interaction, which has a non-negligible performance impact as \cref{eq:2:Kernel_unitcell} is by far the most common operation performed during a meaningful simulation. \\\par

\subsubsection{Periodic boundary conditions}
The rectilinear grid enables the straightforward implementation of first-order periodic boundary conditions (PBC), which account for the eight nearest replicas of the ASI --- two horizontally, two vertically and four diagonally.
An open-boundary kernel $\vc{\mathcal{K}}^{(q)}$ can be transformed into a PBC kernel by just adding eight offset copies of the kernel to itself.
Specifically, four copies are shifted along the axes by $\pm L_x$ or $\pm L_y$, while the remaining four are shifted diagonally by $(\pm L_x, \pm L_y)$.
This is illustrated in~\cref{fig:2:Kernel_PBC} for an example of pinwheel ASI. \par

\xfig[1.0]{2_Hotspice/Kernel_PBC.pdf}{
	\label{fig:2:Kernel_PBC}
	Comparison of the magnetostatic interaction kernel for open (left) and periodic (right) boundary conditions, for a $16 \times 16$ pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}) using the point dipole approximation.
	The magnet of the unit cell site associated to this particular kernel is shown in the center with a black outline.
	The color of other magnets indicates the magnitude of the magnetostatic interaction between the central and the colored magnets if they both point `up'. This alignment is preferable for \textcolor{blue}{blue} magnets (low energy) and unstable for \textcolor{red}{red} magnets (high energy).
}

This construction works because elements near the edge of a kernel represent interactions between distant magnets on opposite sides of the ASI, which are the interactions most affected by PBC.
Since the middle region of the open-boundary kernel already stores the interaction between nearby magnets, PBC can be applied by re-using these central values and moving the kernel by $\pm L_x$ and/or $\pm L_y$. \\\par

This approach has the advantage that PBC do not impact performance, as they are baked into the kernel, but is limited to ASI consisting of an integer number of unit cells as systems with truncated unit cells along an edge can not properly be tiled.
Higher-order PBC --- beyond only the 8 nearest copies --- can not be calculated based on an open-boundary kernel since it does not include interactions with such distant magnets.
Due to the rapid $1/r^3$ decline of the magnetostatic interaction over distance, the use for such higher-order PBC would be very limited, and they were therefore not implemented.
% For very small systems, higher-order PBC can be relevant, but it is recommended to instead increase the system size to increase the fidelity of the Monte Carlo simulation.
% To explain the usage of the magnetostatic kernel, we can possibly refer to ``Real-space observation of emergent magnetic monopoles and associated Dirac strings in artificial kagome spin ice'', which notes that the energy is just a convolution of the interaction potential with the lattice sites at which there are magnets (which, for the Ewald summation, means that in Fourier space it is just a multiplication of the Fourier-transformed versions of those functions), which is pretty much what we are doing during multi-switching.

\subsubsection{Perpendicular magnetostatic kernel} % If not yet explained in detail earlier, talk about the perpendicular kernel etc. here
If the simulation accounts for the asymmetric energy barrier, as detailed in \cref{sec:2:E_B_eff}, then two additional kernels are needed for each site in the unit cell. \par
The first is calculated for a situation where the central magnet maintains its usual orientation while all other magnets are rotated counter-clockwise by \ang{90}.
This kernel will be used whenever a magnet switches, to update the perpendicular magnetostatic energy $\left. E_{\mathrm{MC}} \right|_{\perp}$ of all other magnets.
The underlying equation is very similar to \cref{eq:2:Kernel_detailed}, but with $u_x^{(q)} \rightarrow -u_y^{(q)}$ and $u_y^{(q)} \rightarrow u_x^{(q)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(q)}$ in \cref{eq:2:Kernel_update}. \par
The second additional kernel is the opposite of the first one; the central magnet is rotated \ang{90} counter-clockwise while all other magnets maintain their usual orientation.
This kernel is needed only once, to calculate the initial value of $\left. E_{\mathrm{MC}} \right|_{\perp}$ for all magnets.
The underlying equation for this one is also very similar to \cref{eq:2:Kernel_detailed}, but now with $u_x^{(ab)} \rightarrow -u_y^{(ab)}$ and $u_y^{(ab)} \rightarrow u_x^{(ab)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(q)}$ in \cref{eq:2:Kernel_init}.

\subsubsection{Numerical error with cut-off kernel} % TODO: expand + FIGURE
A performance improvement presents itself by cutting off the magnetostatic kernel at a certain distance, though only in the case of open BC.
Due to the underlying grid, the most natural way of achieving this is to simply reduce the size of the kernel from $2L-1 \times 2L-1$ to a smaller $2K-1 \times 2K-1$ central region.
Note that the number of cells has to remain odd for the convolution to still place the magnet in the center. \par
However, this will result in inaccurate interaction energies being stored.
When multiple magnets switch, this error will accumulate.
Luckily, the error introduced in this manner is bounded from above: whenever a magnet switches back, the error its original switch introduced is cancelled (barring some remaining floating-point error).

\subsection{Multi-switching in Metropolis-Hastings} \label{sec:2:MultiSwitch}
The standard Metropolis-Hastings algorithm selects a magnet at random and subsequently decides whether or not to switch it.
Since this algorithm is designed primarily for sampling equilibrium states rather than capturing the temporal evolution of the system, a straightforward performance improvement can be achieved by selecting multiple magnets simultaneously rather than sequentially.
This allows for better usage of the parallel processing capabilities of the GPU, as the magnetostatic energy can then be updated using a convolution.

\subsubsection{Minimal distance between sampled magnets} % Derive equation
Since all magnets affect each other through the magnetostatic interaction, simultaneously switching nearby magnets that significantly affect each other's switching energy may result in unphysical behaviour.
Take for example the extreme case where it would be allowed to switch all magnets in a system at once: the total energy would not change after such a simultaneous step and an infinite loop results where the energy of the system remains high.
In a less extreme case, convergence to a low-energy or equilibrium state may be slowed down significantly. \par
Therefore, to avoid such issues, we enforce a minimum distance $\rmin$ between selected magnets.
The particular criterion we will use is as follows: two simultaneously sampled magnets should never be able to affect each other's switching probability by more than some user-adjustable factor $0 < Q \leq 1$, where we commonly use $Q=0.01$.
This principle is illustrated in~\cref{fig:2:MultiSwitch_proof}.

\xfig[1.0]{2_Hotspice/MultiSwitch_proof.pdf}{
	\label{fig:2:MultiSwitch_proof}
	Consider two states: the initial state `i' (top), and the final state `f' where magnet 1 has switched (bottom).
	The surrounding magnets stay unchanged.
	In both states, magnet 2 has a certain acceptance probability for switching $P_2$ if it is selected by the Metropolis-Hastings algorithm, determined by the associated energy change $\Delta E$.
	This switch of magnet 1 induces a change in switching probability $\Delta P_2$ of magnet 2.
	The limitation that we impose, is that the Metropolis-Hastings algorithm is only allowed to simultaneously sample magnet 1 and 2 if $\abs{\Delta P_2} = \abs{P_2^{(f)} - P_2^{(i)}} \leq Q$.
}

We will now derive an inequality for the minimal distance $\rmin$ as a function of $Q$.
\newtheorem{inequality}{Inequality}
\begin{inequality}
	The minimal distance $\rmin$ between magnets 1 and 2, such that the acceptance probability $P(\Delta E)$ of one of them can not change by more than $0 < Q \leq 1$ when the other switches, is upper bounded by
	\begin{equation}
		r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \, \underset{i}{\max}(\mu_i)}{\pi Q} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}}} \triangleq \rmin \mathrm{.}
		\label{eq:2:MultiSwitch_inequality}
	\end{equation}
\end{inequality}

\begin{proof}
	Consider~\cref{fig:2:MultiSwitch_proof}.
	In the initial situation (top left panel), magnet 2 has an acceptance probability $P_2^{(i)}$ due to its possible change in energy $\Delta E^{(i)}$.
	Once magnet 1 switches (bottom left panel), magnet 2 has an acceptance probability $P_2^{(f)} = P_2^{(i)} + \Delta P_2$ where a switch would incur a change of $\Delta E^{(f)}$ in energy.
	Our requirement is that $\abs{\Delta P_2} \leq Q$, with $0 < Q \leq 1$.
	We therefore want to determine the minimal distance between magnets 1 and 2 such that $\abs{\Delta P_2} \leq Q$. \par
	For any function $P(\Delta E)$, we can move this constraint from $\Delta P_2$ to the more manageable quantity $\Delta E^{(f)} - \Delta E^{(i)}$ by writing the upper bound
	\begin{equation}
		\abs{\Delta P} \leq \abs{\Delta E^{(f)} - \Delta E^{(i)}} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}} \leq Q \mathrm{.}
		\label{eq:2:MultiSwitch_proof_leq_Q}
	\end{equation}
	The only long-range interaction present in the systems under consideration here is the magnetostatic interaction given by \eqref{eq:2:E_MS}, repeated here for convenience,
	\begin{equation*}
		E_{\mathrm{MS},1,2} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_1 \bcdot \vc{\mu}_2}{\abs{\vc{r}_{12}}^3} - \frac{3(\vc{\mu}_1 \bcdot \vc{r}_{12}) (\vc{\mu}_2 \bcdot \vc{r}_{12})}{\abs{\vc{r}_{12}}^5}) \mathrm{.}  \tag{\ref*{eq:2:E_MS}}
	\end{equation*}
	We do not consider any finite-size corrections, since their long-range effects are negligible and any reasonable value of $Q$ will result in a value $\rmin$ far larger than the magnet size.
	The magnetostatic energy in the point-dipole approximation is bounded by
	\begin{equation}
		\abs{E_{\mathrm{MS},1,2}} \leq \frac{\mu_0 \abs{\vc{\mu}_1} \abs{\vc{\mu}_2}}{2 \pi r_{12}^3} = \frac{\mu_0 \max(\mu_1, \mu_2)}{2 \pi r_{12}^3} \mathrm{,}
		\label{eq:2:MultiSwitch_proof_EMS_leq}
	\end{equation}
	with this extremum being achieved for two dipoles $\vc{\mu}_1$ and $\vc{\mu}_2$ aligned in the same direction along their common axis $\vc{r}_{12}$.
	From the symmetry of the magnetostatic interaction, and considering the states in~\cref{fig:2:MultiSwitch_proof}, the unknown term in~\cref{eq:2:MultiSwitch_proof_leq_Q} can be expanded as 
	\begin{align*}
		\abs{\Delta E^{(f)} - \Delta E^{(i)}} &= \abs{-\cancel{2 \sum_{k=3}^N E_{\mathrm{MS},2,k}^{(f)}} -2 E_{\mathrm{MS},2,1}^{(f)} +\cancel{2 \sum_{k=3}^N E_{\mathrm{MS},2,k}^{(i)}} +2 E_{\mathrm{MS},2,1}^{(i)}} \\
		&= 2\abs{E_{\mathrm{MS},2,1}^{(f)} + E_{\mathrm{MS},2,1}^{(i)}} = 4 \abs{E_{\mathrm{MS},2,1}^{(i)}} \numeq{\ref{eq:2:MultiSwitch_proof_EMS_leq}}{\leq} \frac{2 \mu_0 \max(\mu_1, \mu_2)}{\pi r_{12}^3} \mathrm{,}
	\end{align*}
	since only magnet 1 is different between the initial and final states, hence $E_{\mathrm{MS},2,1}^{(f)} = - E_{\mathrm{MS},2,1}^{(i)}$.
	Combining this with \eqref{eq:2:MultiSwitch_proof_leq_Q} gives the sufficient condition
	\begin{equation}
		\frac{2 \mu_0 \max(\mu_1, \mu_2)}{\pi r_{12}^3} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}} \leq Q \mathrm{.}
	\end{equation}
	To get a single value for $\rmin$ that is valid throughout the ASI, replacing $\max(\mu_1, \mu_2)$ by $\underset{i}{\max}(\mu_i)$ finally yields the expected expression for $\rmin$:
	\begin{equation*}
		r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \, \underset{i}{\max}(\mu_i)}{\pi Q} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}}} \triangleq \rmin \mathrm{.} \tag{\ref*{eq:2:MultiSwitch_inequality}}
	\end{equation*}
\end{proof}
This equation is an upper bound for $\rmin$ thoughout the entire ASI.
To apply this inequality in \hotspice, all that remains is to plug in the derivative of the Metropolis-Hastings acceptance probability function $P_\mathrm{MH}(\Delta E, T) = \min(1, \exp(-\Delta E/k_B T))$, into~\cref{eq:2:MultiSwitch_inequality}.
Since
\begin{equation}
	\abs{\frac{\partial P_\mathrm{MH}}{\partial \Delta E}} = \left.\begin{cases}
		\frac{1}{k_B T} \exp(\frac{-\Delta E}{k_B T}) \quad &\mathrm{if} \quad \Delta E > 0 \\
		0 \quad &\mathrm{if} \quad \Delta E < 0
	\end{cases} \right\}
	\leq \frac{1}{k_B T} \mathrm{,}
\end{equation}
this finally gives
\begin{equation}
	r \geq \sqrt[3]{\frac{2 \mu_0 M_{sat}^2 V^2}{\pi Q k_B T}} \mathrm{.}
\end{equation}
Recall that we are using the Metropolis-Hastings acceptance probability in \hotspice's rejection-based kinetic Monte Carlo algorithm (\cref{sec:2:Dynamics_MH}) rather than the Glauber dynamics acceptance probability $P_\mathrm{GD}(\Delta E, T) = \frac{\exp(-\Delta E/k_B T)}{1+\exp(-\Delta E/k_B T)}$, because they both satisfy ergodicity and detailed balance but the Glauber acceptance probability is always more likely to reject a switch~\cite{bit-player_MCvsGlauber}. % TODO: since I say "recall" here, we must discuss the acceptance probability in the algorithms already.
We can now see, however, that the Glauber acceptance probability is not all bad when using multi-switching.
The extremum of its derivative is 4 times smaller than that of the Metropolis-Hastings acceptance probability, so \cref{eq:2:MultiSwitch_inequality} tells us that the minimal distance between samples can be decreased by a factor $\sqrt[3]{1/4} \approx 0.63$ and therefore $\sqrt[3]{16} \approx 2.52$ times more magnets can switch at once.
As such, it can become debatable which acceptance probability is more efficient in a given situation.

\subsubsection{Selection algorithms} % TODO
\paragraph{Poisson disc}
\paragraph{Grid-select}
\paragraph{Hybrid grid-poisson}

\subsection{Performance}
\subsubsection{History}
The performance of \hotspice has been improved throughout development in various ways. \par
At an early stage of development, the implementation of \hotspice was adjusted to allow calculations to be performed on either a central processing unit (CPU) or a graphics processing unit (GPU).
Both types of processor are typically present in modern computers and are designed for different tasks.
The CPU can execute diverse, sequential tasks in rapid succession, while the GPU is optimized for parallel computing and therefore excels at performing the same calculation across a large amount of data simultaneously~\cite{owens2008gpu}.
In our simulations, the number of magnets in the ASI has a major impact on performance, and often forms the determining factor as to whether calculation on the GPU rather than CPU will result in a faster simulation~\cite{lee2010debunking}. \par
In the following paragraphs, we present a chronological overview of various improvements to the calculation of the magnetostatic interaction, with a performance comparison between CPU and GPU for the first and final methods.
Two tables are shown:~\cref{tab:2:perf_init} for the initialisation time, and~\cref{tab:2:perf_switch} for the time it takes to perform 5000 switches using the N\'eel update algorithm, taking into account the magnetostatic interaction between all magnets.
During initialisation, the magnetostatic kernel is constructed, after which the starting energy $E_i$ of all magnets is calculated.
In the tables, only the last 3 rows use the kernel as it was explained in~\cref{sec:2:Kernels}; the other rows use less efficient kernels as explained below. \par % Can't really call it a "lookup table" if it is being convolved etc., right?
This test was benchmarked on an NVIDIA GeForce RTX 3080 Mobile GPU and an 11th Gen Intel\textregistered{} Core\texttrademark{} i7-11800H @ 2.30GHz.
Simulation times displayed serve an illustrative purpose: absolute values on other hardware may vary significantly, though the general trends within the tables should remain similar.

% Q: should we use L or N=L²? I would prefer L because at some point we talk about 2D indexation, so L makes it clearer that we have a 2D system.
\xtable[tab:2:perf_init]{\textbf{Initialisation time} for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 0.5s & 6.8s & 34.0s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 2.2s & 5.4s & 11.9s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 1.0s & 3.4s & 7.6s & 14.5s & 91.6s & ? \\
		\hline
		GPU | \makecell{Unitcell-kernel (see \S\ref{sec:2:Kernels})} & 2.4s & 4.6s & 9.7s & 16.3s & 66.2s & 730.4s \\
		\hline \hline
		GPU | \makecell{Final version\\(convolution initialization)} & 1.0s & 1.0s & 1.0s & 1.0s & 1.6s & 23.9s \\
		\hline
		CPU | \makecell{Final version\\(convolution initialization)} & 0.014s & 0.2s & 1.0s & 3.2s & 53.1s & ? \\
		\hline
	\end{tabular}
}

\xtable[tab:2:perf_switch]{\textbf{Time for 5000 switches} using the N\'eel algorithm, for various simulation sizes $L$ (i.e. $L \times L = N$ magnets). `Mem' indicates excessive memory consumption, `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 9.4s & 217.6s & ? & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 5.0s & 13.6s & 51.8s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 7.1s & 7.2s & 8.2s & 9.0s & 13.0s & ? \\
		\hline
		GPU | \makecell{Unitcell-kernel (see \S\ref{sec:2:Kernels})} & 7.8s & 7.9s & 8.3s & 8.9s & 11.8s & 36.9s \\
		\hline \hline
		GPU | \makecell{Final version\\(2D indexation)} & 6.8s & 7.0s & 7.2s & 7.6s & 10.2s & 35.9s \\
		\hline
		CPU | \makecell{Final version\\(2D indexation)} & 0.56s & 1.3s & 2.5s & 4.3s & 41s & ? \\
		\hline
	\end{tabular}
}

\paragraph{From CPU to GPU}
Originally, \hotspice performed all calculations on the CPU. % Ing the beninging
This caused the simulation time to rise dramatically as the system size grew (e.g. first row in \cref{tab:2:perf_switch}).
For early versions of \hotspice with an unoptimized kernel, this made it impractical to simulate systems larger than $50 \times 50$ magnets.
By simply switching to GPU-based calculations using the \python{CuPy} library (second row), this upper limit was increased significantly.
The limiting factor instead became memory consumption of the unoptimized kernel. \par
Do note, however, that for small systems the GPU performs worse than the CPU.
This remains true independent of the kernel implementation.
GPUs are optimized for parallel processing, and therefore reach a bottleneck when the simulation takes on a more sequential nature~\cite{owens2008gpu}.
Simulating a single switch requires several distinct operations that do not lend themselves to parallelisation.
When operating on small arrays, this means relatively fewer parallel calculations can be performed, thereby increasing the importance of being able to perform distinct mathematical operations in quick succession.
The CPU excels at the latter.

\paragraph{Kernel improvements}
The first two rows `\textit{Pairwise kernel}' in the tables use the na\"ive kernels $\vc{k}$ (or, equivalently, $\vc{K}$) described at the start of \cref{sec:2:Kernels}.
Their values were simply arranged in a big $N \times N$ matrix $\vc{D}$ which stored the magnetostatic interaction between each pair of magnets.
While this allowed the magnetostatic interaction energy of each magnet to easily be calculated by a single matrix product $\vc{D}\vc{s}$, the initialisation time on CPU (first row in~\cref{tab:2:perf_init}) bears witness to the size of this matrix, as $t_\mathrm{init} \propto L^4 = N^2$.
This is because calculating the initial magnetostatic interaction energy $E_\mathrm{MS}$ with this kernel structure requires iterating over all magnets and summing the interaction energy with each other magnet.
On GPU, parallelisation of this sum in each such iteration approximately reduces this to $\propto L^2 = N$. \par
Clearly, for large arrays, the need to store $\order{N^2}$ elements quickly becomes prohibitive.
For a system as small as $170 \times 170$, this type of kernel already exceeded the \SI{8}{\giga\byte} of available memory, as indicated in the table by ``Mem''.
For periodic lattices, the matrix will contain many identical values, wasting a lot of memory.
While it was possible to alleviate this by instead using a sparse matrix and limiting the interaction distance, memory usage was not the only issue: also the performance per switch (\cref{tab:2:perf_switch}) was rapidly declining for increasing $N$, both on CPU and GPU. % Also: sparse matrices probably have more indexation overhead
Clearly, a more efficient approach was required. \\\par

An extreme solution would be to use no kernel at all, as in the third row `\textit{No kernel}'.
Instead, for every switching magnet, its interaction energy with all other magnets was calculated from scratch.
The tables show that this clearly gets rid of the memory issue and yields a surprisingly fast simulation.
However, the initialisation for larger systems now takes a long time, because without a kernel this requires $\order{N^2}$ operations resulting in $t_\mathrm{init} \propto N=L^2$ on GPU for small systems.
Still, in all situations it is beneficial not to use the unoptimized kernel. \\\par

To reduce the initialisation time, two improvements had to be combined. \par
First, a kernel as described earlier in~\cref{sec:2:Kernels} had to be implemented, which overcomes the memory issue by using the periodic nature of most ASI to only store interactions for a single unit cell.
This is used in the fourth row `\textit{Unitcell-kernel}', but does not constitute a significant improvement on its own aside from enabling $1000 \times 1000$ systems for the first time. \par
Secondly, however, this new kernel enables the usage of a convolution to initialise the magnetostatic interaction energy $E_\mathrm{MS}$, rather than the sequential sum used before.
This is what the final two rows `\textit{Final version}' use, where we once again compare CPU and GPU as this is the final version.
Another minor improvement in the final version was to use 2D indexation (rather than a flat index $i = n_x y + x$ which did not synergize well with the convolution), yielding another \SI{15}{\percent} performance increase.

\paragraph{Final performance}
In the end, the upper limit for feasible calculation time has become $L \approx 1000$ on GPU and $L \approx 400$ on CPU.
Comparing the last two rows of \cref{tab:2:perf_switch} reveals that GPU outperforms CPU for $L \gtrapprox 300$.
However, this is for the N\'eel algorithm which only performs a single switch at a time.
Performing multiple switches simultaneously in Metropolis-Hastings benefits greatly from the unitcell-kernel, as it can use the same principle of convolution as was already used in the initialisation.
With this algorithm, the GPU can maintain the advantage for systems as small as $L \gtrapprox 60$.

\subsubsection{Multi-switching}
The N\'eel algorithm, used in the preceding discussion about performance, does not implement a multi-switching procedure.
This makes it very inefficient for large systems: for a system of $N$ magnets to change its macrostate significantly, on the order of $N$ switches must occur.
Even though the time required to perform a single switch with the N\'eel algorithm stays roughly constant up to $L<400$, the time per Monte Carlo sweep will scale linearly with the number of magnets if each iteration only switches a single magnet.
The ability to perform multiple switches simultaneously in the Metropolis-Hastings algorithm (described earlier in \cref{sec:2:MultiSwitch}) alleviates this problem, allowing for a significantly higher amount of Monte Carlo sweeps per second. \\\par
\cref{fig:2:Performance} shows the performance of the multi-switching algorithm as a function of system size, both for calculation on the CPU (a) and GPU (b).
The multi-switching algorithm selects a certain number of magnets per second, as depicted by the blue curve: these are candidates for switching.
Dividing this value by $N$, the number of magnets in the system, gives the number of Monte Carlo sweeps per second, depicted by the black curve.
This is the main performance metric in Monte Carlo simulations. \par
Only a subset of the selected magnets will switch, as depicted by the red curve.
This switching rate is highly dependent on the specific conditions of the simulation --- particularly the ratio $\EBeff/T$ --- making it infeasible to make general statements.
There exist situations where no magnets switch or where any selected magnet switches.
In the figure, values of $a$, $T$, and $\EB$ were chosen which result in a reasonable switching rate.

\xfigsnocap[0.47]{2_Hotspice/Performance_CPU.pdf}{2_Hotspice/Performance_GPU.pdf}{
	Performance of OOP square ASI as a function of system size, on \textbf{(a)} CPU and \textbf{(b)} GPU. The Metropolis-Hastings algorithm was used with multi-switching $Q=0.05$, lattice spacing $a = \SI{1}{\micro\metre}$, temperature $T = \SI{100}{\kelvin}$ and energy barrier $\EB = 0$.
	\label{fig:2:Performance}
} % Q: Performance for IP ASI might be worse. Add a Pinwheel figure as well?

There is once again a stark contrast between CPU and GPU. \par
On CPU, the sampling rate starts off rather constant for small systems, as overhead dominates.
For $N>100$ magnets, the sampling rate increases proportional to $N$, leading to a stable performance (MCS/s) independent of $N$.
However, performance drops dramatically once the system size increases beyond $\approx 80 \times 80$. % While this is reminiscent of a lack of garbage collection while building the figure, this is not the case here as the drop remains even when starting the figure at $L=80$.
We hypothesize that this is due to exceeding a CPU cache size. \par
%On the CPU used for~\cref{fig:2:Performance} (the aforementioned i7-11800H), the L1 and L2 cache hold \SI{48}{\kilo\byte} and \SI{1.25}{\mega\byte}, respectively.
% Q: Are any of the below hypotheses reasonable? Which is most reasonable?
% Hypothesis 1: The size occupied by a 72x72 ASI is 1.25MB. It is not unreasonable that, soon after reaching this point, some arrays can no longer be stored on this cache level, impacting performance.
% Hypothesis 2: The size of mm.m at 78x78 is 48kB, so we might be exceeding the L1 cache beyond this point.
% Hypothesis 3: The size of the dipole kernel at this point is around 220kB. Doesn't seem to match up with anything.
% TODO: Revisit this at some point, it is odd that on another CPU (Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz) the cut-off happens at the same system size, indicating that it is not cache-size-related because that CPU has 256kB L2 and 32kB L1, both different from the UGent laptop.
The GPU, on the other hand, hits its stride for exactly those systems that are too large for the CPU to handle.
Around $70 \times 70$, the sampling rate increases rapidly, resulting in an increased number of MC sweeps per second (MCS/s) that reach a local maximum for $200 \times 200$ magnets.
Beyond this, the sampling rate stagnates, eventually settling at around \SI{150e3}{samples\per\second}, corresponding to an ever decreasing MCS/s as the parallelism of the GPU gets exhausted.

\subsubsection{RNG}
Another factor which may contribute to the performance is the Random Number Generator (RNG).
Both update algorithms rely on generating a large amount of random numbers to select the next magnet(s) to switch. % TODO: talk a bit about XORWOW etc. and how it actually doesn't really impact performance

\subsection{Package structure} % Structure of Hotspice, explain all modules (core, ASI, energies, (config? with GPU)...)
Hotspice is written as a Python 3.10 package and can perform simulations on either the CPU or GPU.
The optimal hardware choice depends on the size of the ASI and the update scheme used.
By default, \hotspice runs on the CPU using the popular NumPy and SciPy libraries.
For GPU-accelerated array manipulation, the CuPy v11.4~\cite{CuPy} library is used, but this is an optional dependency. \par
\paragraph{\python{hotspice.config}: GPU/CPU choice}
By default, \hotspice runs on the CPU. To run on the GPU, the environment variable \python{HOTSPICE_USE_GPU} must be set to \python{"true"} before the the \hotspice package is loaded in a Python script with \python{import hotspice}. It is not possible to switch between GPU/CPU within a script.
\subsubsection{ASI}
\paragraph{\python{hotspice.ASI}: predefined ASI lattices}
This module provides two abstract classes from which all ASIs should inherit: \python{hotspice.ASI.IP_ASI} or \python{hotspice.ASI.OOP_ASI}.
Various lattices are available, as previously shown in~\cref{fig:2:ASIs}.
These all follow the same pattern: \python{hotspice.ASI.<ASI_name>(a, n, **kwargs)}, with two positional arguments.
The first argument is the lattice parameter, as defined by the red indicator in \cref{fig:2:ASIs}.
The second argument is the size of the underlying grid (one can also specify \python{nx} and \python{ny} separately).
For the predefined lattices in the \python{hotspice.ASI} module, these two parameters are enough information to create a rudimentary ASI.
\paragraph{Energies}\label{sec:2:API_energies}
Three predefined energy contributions are provided in the \python{hotspice.energies} module, though they can be accessed from the main \python{hotspice} namespace because they are used so often.
The magnetostatic interaction is implemented by the \python{hotspice.DipolarEnergy} or \python{hotspice.DiMonopolarEnergy} classes, with the latter using the monopole approximation to calculate the interaction.
By default, an ASI object takes only the \python{hotspice.DipolarEnergy} into account.
When relevant, the user has to explicitly add a \python{ZeemanEnergy} or \python{ExchangeEnergy}.
It is possible to set longer-range exchange interactions (next-nearest neighbour etc.) by manually setting the \python{local_interaction} field of an \python{ExchangeEnergy} object.
% TODO: create a custom "API" environment that we can put near all the different parts of the model to show code outside the main text?
\subsubsection{RC} % io and experiments modules
\subsubsection{Utilities} % Explain utils module (but only those functions/classes relevant to users) and perhaps mention the existence of plottools
\paragraph{GUI}
A graphical user interface (GUI) is available for \hotspice, which allows the user to directly interact with the ASI and observe changes in realtime. It can be run by calling \python{hotspice.gui.show(mm)}, with \python{mm} the ASI object, resulting in the window shown in~\cref{fig:2:GUI}.

\xfig[1.0]{2_Hotspice/GUI.png}{
	The \hotspice graphical user interface.
	This example shows a spatially averaged view of an $80 \times 50$ pinwheel ASI, \SI{2}{\micro\second} after it was initialised in a random state.
	\label{fig:2:GUI}
}

The state of the ASI is prominently displayed, and the bottom left panel shows a few useful statistics such as the elapsed time. The ASI display is controlled by the central bottom panel, which changes between 4 display modes.
By default, the magnetisation is shown as an averaged field, with the averaging method determined by the ASI lattice.
The second mode displays individual arrows, which either show each magnet's magnetisation direction $s_i \vc{u}_i$ or the effective field $\vc{B}_{\mathrm{eff},i}$ it experiences.
In the third mode, the energy contributions ($E_\mathrm{MC}$, $E_\mathrm{Z}$, $E_\mathrm{Exch}$ and their sum $E_i$) and the resulting effective energy barrier $\EBeff$ for each magnet can be shown, both along the easy and hard axis.
The last option is to show the spatial distribution (i.e., the value for each magnet) of some parameters like the temperature $T$, shape anisotropy $\EB$ and size of the magnetic moment $\mu$. \par
Buttons in the sidebar on the right allow the user to interact with the ASI by progressing through time, setting an initial state, or --- for more complex simulations --- apply custom functions.
By clicking with the mouse on the ASI plot, it is also possible to interact with the ASI at a granular level, switching one or multiple magnets. Finally, the red box in the bottom right controls the update algorithm.

\subsection{Advantages and disadvantages of the \hotspice approach} % Hindsight is 20/20
\subsubsection{Grid} % TODO
\subsubsection{Input/output RC}
\subsubsection{GPU/CPU}
In hindsight, for the purposes of this thesis it was not necessary to go through all the effort to port \hotspice to the GPU.
In~\cref{ch:Applications}, I rarely use large systems, and often use the N\'eel algorithm for which multi-switching was not implemented.
Therefore, the vast majority of calculations in that chapter will be performed on CPU. 
In hindsight, we mostly worked on small systems with the N\'eel algorithm (to simulate RC in OOP ASI), making all the effort of allowing \hotspice to run on the GPU unnecessary. Still, the effort was not wasted as it has expanded the envelope of applicability for \hotspice, particularly when using the Metropolis-Hastings algorithm.
The fact that the current implementation requires making the choice to use GPU or CPU before the \python{import hotspice} statement can be limiting in specific situations. A spiritual successor to \hotspice should address this limitation.

\section{Verification} % See paper
Now that we know the ins and outs of the model used by \hotspice, all that remains is to verify its correct implementation.
To this end, we simulate several systems of increasing complexity for which analytical solutions are available.
The examples discussed here are all equilibrium problems, so we will use the Metropolis-Hastings algorithm to verify that it indeed samples the equilibrium state space.
Special attention will be given to the issue of ``critical slowing down'' which plagues this algorithm near phase transitions.
\subsection{Hexagon}
Before progressing to equilibrium problems, we briefly check that the magnetostatic interaction and its underlying kernels all work as expected.
The magnetostatic interaction energy of any given arrangement of nanomagnets can be calculated analytically and compared to \hotspice. \par
As an example that is neither too simple nor complex, we consider a regular hexagon in a vortex state. The energy of all magnets should equal the following analytical solution, which is the sum of the magnetostatic interaction energy between nearest neighbours $\circled{1}$, next-nearest neighbours $\circled{2}$ and magnets on opposite sides of the hexagon $\circled{3}$:
\begin{align*}
	E_{\mathrm{MS},i} =&\, \circled{1} + 2 \times \circled{2} + 2 \times \circled{3} \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} -2\frac{\mu_0 m^2}{4\pi \frac{3 \sqrt{3} a^3}{8}} \Big[3\cos^2(\pi/3) - \cos(2\pi/3)\Big] -2\frac{\mu_0 m^2}{4\pi \frac{a^3}{8}} \Big[3\cos^2(\pi/6) - \cos(\pi/3)\Big] \\
	=&\, -\frac{\mu_0 m^2}{4\pi a^3} \bigg[29+\frac{20}{3\sqrt{3}}\bigg] \mathrm{.}
\end{align*}
The following Python function verifies that, indeed, \hotspice gives the expected total energy $E_{\mathrm{MS},i}$ for all 6 magnets $i$ in the system.
\begin{lstlisting}
import numpy as np
import hotspice

def test_hexagon(l=470e-9, s=10e-9, m=1.1278401e-15):
	mm = hotspice.ASI.IP_Kagome(a := (l+2*s)/np.tan(30*np.pi/180), nx=5, ny=3, moment=m, pattern="vortex")
	E_sim = mm.get_energy('dipolar').E[mm.m.astype(bool)]
	E_exact = -1e-7*m*m/a**3*(29+20*np.sqrt(3)/9)
	print("OK" if np.allclose(E_sim, E_exact, atol=0) else "Fail")

test_hexagon() # OK
\end{lstlisting}

\subsection{Non-interacting spin ensemble}

When an external magnetic field of magnitude $B$ is applied to a non-interacting ensemble of Ising spins, it follows directly from the partition function that the average magnetization follows the relation
\begin{equation}
	\frac{\langle M \rangle}{M_0} = \tanh\ab(\frac{\mu B}{\kBT}) \mathrm{.}
\end{equation}
\cref{fig:2:Noninteracting} demonstrates that \hotspice{} correctly reproduces the expected result.

\sidefig{2_Hotspice/Verification/Noninteracting_IP.pdf}{
	\label{fig:2:Noninteracting}
	Average magnetisation of a non-interacting ensemble of spins, as a function of the applied magnetic field magnitude $B$.
	\newline\newline\newline
}

\subsection{Exchange-coupled Ising system}
\label{sec:2:Verification_OOP_Exchange}
The 2D square-lattice exchange-coupled Ising model is one of the few exactly solvable systems in statistical physics~\cite{ExactlySolvedModelsStatMech}.
An analytical solution is known for the temperature-dependence of its average magnetisation
\begin{equation}
	M = \sqrt[8]{1 - \sinh^{-4}(2J/\kBT)} \mathrm{,}
	\label{eq:2:Verification_exchange_M}
\end{equation}
with $J$ the exchange coupling constant~\cite{Correlations2DIsing,IsingSpontaneousMagnetization,coey2010magnetism}.
This implies that the system exhibits a second-order phase transition at the critical temperature~\cite{ExactlySolvedModelsStatMech}
\begin{equation}
	T_c = \frac{2J}{\kB \ln(1 + \sqrt{2})} \mathrm{.}
\end{equation}
Furthermore, an analytical solution is available for the nearest-neighbour correlation
\begin{equation}
	\langle \sigma_{1} \sigma_{2} \rangle = 
	\begin{cases}
		\sqrt{1+k} \ab[\frac{1-k}{\pi}K(k) + \frac{1}{2} \ab] &\text{for } T < T_c \mathrm{,} \\ 
		\sqrt{1+k} \ab[\frac{1-k}{\pi k}K(1/k) + \frac{1}{2} \ab] &\text{for } T > T_c \mathrm{.} \\ 
	\end{cases}
\end{equation}
with $K$ the complete elliptic integral of the first kind and $k=\sinh^{-2}(2J/\kBT)$~\cite{Correlations2DIsing}. \\\par

The result of a \hotspice simulation of this Ising system is shown in \cref{fig:2:Verification_exchange}, as calculated for an $800 \times 800$ lattice.
Maximal multi-sampling ($Q=+\infty$) for the Metropolis-Hastings algorithm was used to verify that --- at least for systems with such short-range coupling --- this sort of extreme multi-sampling still converges to the correct solution.
Since the theoretical curves for $M$ and $\langle \sigma_{1} \sigma_{2} \rangle$ are monotonically decreasing~\cite{MCinStatPhys}, the final state from the previous temperature step was retained as the starting point for the next step. This preserves progress already made towards equilibrium and avoids the disruption that resetting to the uniform state would cause. \par

\xfig{2_Hotspice/Verification/OOP_Exchange.pdf}{
	\label{fig:2:Verification_exchange}
	\textbf{(a)} Average magnetisation and \textbf{(b)} nearest-neighbour (NN) correlation as a function of temperature.
	Markers show the \hotspice result for an $800 \times 800$ OOP square-lattice exchange-coupled Ising system using the Metropolis-Hastings algorithm with maximal multi-switching.
	The simulation was performed for 3 values of $N$ --- the number of Monte Carlo steps per site performed at each temperature step.
	Discrepancies due to critical slowing down above $T_c$ improve with increasing $N$.
}

The \hotspice result corresponds well to the theoretical predictions both below $T_c$ and in the high-temperature limit.
However, just above $T_c$, the average magnetisation evolves only slowly towards the expected value.
Increasing the number of Monte Carlo sweeps per temperature step brings the system closer to the theoretical equilibrium and reduces the temperature range above $T_c$ where the system has not yet reached equilibrium again.
This slow convergence near the critical point is a symptom of the well-known phenomenon called ``critical slowing down''.
% TODO: why the discrepancy in the correlation?

\subsubsection{Critical slowing down}
Critical slowing down (CSD) originates from a divergence in the autocorrelation time $\tau$ near a critical point, causing subsequent Monte Carlo configurations to be highly correlated~\cite{NumericalDynamicalNiedermayer,CompStatPhys,StatisticalMechanicsAlgorithmsComputations}.
As a result, the system explores the phase space very slowly, particularly with single-spin flip algorithms like Metropolis-Hastings~\cite{StatisticalMechanicsAlgorithmsComputations}.
Although cluster algorithms like the Wolff algorithm~\cite{Wolff} can mitigate this effect, they are not intended for application beyond the 2D Ising system.
% TODO: extend this discussion
% Relevant quotes: ``It is above all in the critical region that the Metropolis-Hastings algorithm is less accurate, because update moves based on single-spin flips are no longer physically relevant in this region,'' and ``Reworded differently, critical slowing down is particularly stark in the context of Metropolis-Hastings algorithms owing to the drastically low pace at which the Markov chain explores the whole phase space when two successive states differ by at most one spin value.'' from the thesis `Phase Transitions in Long-range Spin Models: The Power of Generalized Ensembles'. The book StatisticalMechanicsAlgorithmsComputations says ``Around the critical temperature, the local Monte Carlo algorithm is increasingly slow. This is because the distribution of the total magnetization becomes wide: between the high-temperature regime where it is sharply peaked at zero magnetization, and the low-temperature regime with its double peak structure, the system passes through a regime where the probability distribution of the magnetization is essentially flat for almost all values of M (see Fig. 5.15). Such a distribution is extremely difficult to sample with a single spin-flip algorithm.''

\subsection{Exchange- and magnetostatically-coupled Ising system}
Including long-range magnetostatic interactions into an exchange-coupled square-lattice Ising system significantly alters its behaviour.
While the temperature $T$ remains an important system parameter, now another determining factor is the ratio $\delta = E_{\mathrm{exch},i,j}/E_{\mathrm{MS},i,j}$ ($j \in \mathcal{N}_i$), which represents the balance between the exchange coupling and magnetostatic interaction for nearest neighbours.
Analytical predictions remain possible in this system at zero temperature: for $\delta < 0.85$, the magnetostatic coupling dominates, leading to a checkerboard state.
As $\delta$ increases, the ever-stronger exchange coupling leads to the formation of ferromagnetic domains, which organize into stripes due to the magnetostatic interaction, with the stripe width determined by $\delta$~\cite{StripedDipolarIsing}.

\xfig{2_Hotspice/Verification/OOP_Dipolar.pdf}{
	\label{fig:2:Verification_dipolar}
	Nearest-neighbour (NN) correlation in an exchange-coupled system with long-range magnetostatic interactions included, as a function of relative NN magnetostatic/exchange coupling $\delta$. Transitions occur at $\delta=0.85$ and 2.65, indicated by dotted lines. Insets show the magnetisation state with growing stripe domains: white corresponds to spin `up', black to `down'.
}

The average stripe width is reflected in the NN correlation $\langle S_i S_{i+1} \rangle$, as shown in~\cref{fig:2:Verification_dipolar} for a \hotspice simulation.
Because the analytical theory is valid at zero temperature, but we are using the Metropolis-Hastings algorithm which requires non-zero temperature, we will be using a sufficiently low temperature (\SI{50}{\kelvin}) for this simulation.
Consistent with the theoretical predictions of~\ccite{StripedDipolarIsing}, for $\delta < 0.85$ a checkerboard state exists with $\langle S_i S_{i+1} \rangle = -1$.
In the range $0.85 < \delta < 2.65$, stripe domains with a width of 1 row are preferred, leading to $\langle S_i S_{i+1} \rangle = 0$.
Beyond $\delta=2.65$, a 2-row width becomes preferable with $\langle S_i S_{i+1} \rangle = 0.5$.
Increasing $\delta$ further leads to ever wider stripe domains, and in the limit $\delta \rightarrow +\infty$ the correlation approaches $\langle S_i S_{i+1} \rangle \rightarrow 1$.

\subsection{Square-to-pinwheel transition angle}\label{sec:2:Verification_IP_SquarePinwheel}
The in-plane square and pinwheel ASI lattices can be continuously transformed into each other by rotating each individual magnet by \ang{45}.
Despite being this closely related, their ground state magnetic ordering differs significantly.
Square ASI has an antiferromagnetic (AFM) ground state, where all vertices have a net zero magnetisation.
Meanwhile, pinwheel ASI exhibits superferromagnetic order, where all magnets with similarly-oriented easy axes are magnetized in the same direction~\cite{ApparentFMpinwheel}.
Therefore, a critical angle $\ang{0} < \alpha_c < \ang{45}$ must exist where the ground state transitions between these two extremes. \par
For the dipole model, theoretical calculations predict this transition at $\alpha_c = \arcsin(\sqrt{3}/3) = \ang{35.3}$~\cite{AFM-FM-transition-Pinwheel,MagicAngle}.
For the dumbbell model, the transition angle depends on the distance $d$ between monopoles, but is always larger than for the dipole model~\cite{AFM-FM-transition-Pinwheel}. \\\par
The result of a \hotspice simulation using both models is shown in \cref{fig:2:Pinwheel_angle}.
To quantify this transition, we measure the fraction of vertices with net zero magnetisation --- this value is 0 for superferromagnetic order while it is 1 for AFM order.
For the dipole model, the transition occurs at $\approx \ang{35}$ as expected, while the dumbbell model indeed transitions at a larger rotation angle.
% TODO: debate (with myself) whether or not to look for some more experimental references to include the following sentences.
% While this transition was experimentally observed to be gradual from \ang{35} to nearly \ang{45}, these experiments likely could not reach an equilibrium state due to defects, rapid quenching, or finite correlation times, all of which freeze domain walls in the system. Metropolis-Hastings samples the equilibrium state space, resulting in the sharp transition seen in the figure.

\xfig[0.6]{2_Hotspice/Verification/Pinwheel_angle.pdf}{
	Fraction of vertices with net zero magnetisation at equilibrium, as square ASI (left) transitions to pinwheel ASI (right) by rotating individual magnets.
	The theoretical transition angle $\alpha_c \approx \ang{35.3}$ for the dipole model is indicated by the vertical dotted line.
	\label{fig:2:Pinwheel_angle}
}

\chapter{Methods: ``\hotspice'' simulator for artificial spin ice}\label{ch:Hotspice}
\glijbaantje{Truth is much too complicated to allow anything but approximations.}{John von Neumann}

To assess the potential of \link{reservoir computing}{Reservoir Computing} in (perpendicular-anisotropy) \link{artificial spin ice}{Artificial Spin Ice}, as is the main topic of this thesis, a simulation framework is needed to efficiently explore the impact of various system parameters and input methods on reservoir performance. \par
To this end, the \hotspice simulator was developed, as introduced in~\cref{sec:1:Modelling_macrospin}.
In this chapter, we discuss its underlying model and several model improvements that have been implemented, and assess their accuracy in simulating the behaviour of ASI.
These improvements differ in their choice of Monte Carlo spin-flip algorithm and their calculation of the \xref{magnetostatic interactions} and \xref{effective energy barriers}.

\vspace*{\fill}
\begin{center}
	\centering\rule{0.6\linewidth}{0.4pt}
\end{center}
\vspace{-1.5em}
\begin{center}
	The \hotspice simulator discussed in this chapter\\
	is open-source and available on \href{https://github.com/bvwaeyen/Hotspice}{GitHub}.
\end{center}
\vspace{-2.2em}
\begin{center}
	\centering\rule{0.6\linewidth}{0.4pt}
\end{center}
\vspace*{\fill}

\newpage
\vspace*{\fill}
\begin{adjustwidth}{2em}{2em}
	\vspace{0em}
	\begin{center}
		\centering\rule{0.75\linewidth}{0.4pt}
	\end{center}
	\vspace{0em}
	\begin{center}
		\textbf{Material from this chapter has also been published in:} \\
	\end{center}
	\vspace{0em}
	\begin{adjustwidth}{0em}{1.5em}
		\begin{itemize}
			\item[\cite{MAES-24}] J.~Maes, D.~De~Gusem, I.~Lateur, J.~Leliaert, A.~Kurenkov, and B.~Van Waeyenberge.
			\newblock The design, verification, and applications of Hotspice: a Monte Carlo simulator for artificial spin ice.
			\newblock \emph{Computer Physics Communications}, page 109643, 2025.
		\end{itemize}
	\end{adjustwidth}
	\vspace{1em}
	\begin{adjustwidth}{0.125\linewidth}{0.125\linewidth}
		\textbf{Author contributions} ---
		\underline{J.M.} carried out the majority of the research and wrote the manuscript with input from all authors.
		D.D.G. contributed the dumbbell model and kagome reversal example, while I.L. contributed the pinwheel clocking example.
		% The collaboration with A.H. in \cite{KUR-24} informed practical design choices for the software.
		B.V.W. and J.L. supervised the project.	\par
		
		\vspace{1em}
		\textbf{Software contributions} ---
		B.V.W. conceived the \hotspice project and provided the original prototype script, which \underline{J.M.} substantially expanded and developed into the full simulator presented in the paper.
		The dumbbell model was implemented by D.D.G. \par
		
		\vspace{1em}
		\textbf{This chapter} expands upon all aspects of the paper unrelated to reservoir computing, particularly the software implementation of \hotspice, which is detailed in \cref{sec:2:Implementation}.
	\end{adjustwidth}
	\vspace{-0.5em}
	\begin{center}
		\centering\rule{0.75\linewidth}{0.4pt}
	\end{center}
\end{adjustwidth}
\vspace*{\fill}

\newpage
\section{Artificial spin ice model}\indexlabel[tosection]{point dipole model}
In single-domain IP nanomagnets, the magnetisation prefers to align along the fixed \xref{easy axis} of the geometry, while for OOP magnets a strong interfacial anisotropy with the substrate causes a preferential orientation along the $z$-axis.
Either way, it is natural to use an Ising-like approximation for simulating such single-domain nanomagnets.
The easy axis, position $\vc{r}_i$ and size of the magnetic moment\footnote{
	\label{fn:2:moment_integral}
	The size of the magnetic moment $\mu_i$ corresponds to the total ground state magnetic moment $\norm{\int_{\Omega_i} \vc{M}(\vc{r})d\vc{r}}$, with $\Omega_i$ the geometry of magnet $i$ and $\vc{M}(\vc{r})$ its magnetisation in the twofold degenerate ground state.
	Due to edge relaxation effects, this is slightly smaller than $M_\mathrm{sat} V_i$, where $M_\mathrm{sat}$ is the material's saturation magnetisation.
} $\mu_i$ of each magnet $i$ are fixed and they are only allowed to switch between the `up' ($\uparrow$, $s_i=1$) and `down' ($\downarrow$, $s_i=-1$) magnetisation states along the easy axis.
Thus, the total magnetic moment vector $\vc{\mu}_i$ of magnet $i$ can be expressed as
\begin{equation}
	\vc{\mu}_i = s_i \mu_i \vc{u}_i \mathrm{,}
\end{equation}
where $s_i = \pm 1$, and $\vc{u}_i$ is a fixed unit vector parallel to the easy axis. \par
The switching rate between these two states is determined by the energy barrier that separates them, as well as the temperature $T$. % A canonical ensemble is used, though each magnet can have a different effective temperature $T_i$.
For an isolated nanomagnet, the energy barrier $\EB$ originates from its \xref{shape anisotropy} and can, for instance, be estimated from \link{micromagnetic theory}{micromagnetic simulations}.
However, interactions with other magnets or \xref{external magnetic fields} modify the \xref{energy landscape}, leading to an \link{effective energy barrier}{effective barrier} which we will denote as $\EBeff$~\cite{leo2021chiral}.
Each magnet can have a unique \xref{magnetic moment} size $\mu_i$, temperature $T_i$ and energy barrier $E_{\mathrm{B},i}$.
This enables, for instance, modelling some of the disorder due to \link{lithography}{lithographic} variations by assigning a different shape anisotropy to each magnet, typically sampled from a Gaussian distribution with mean $\EB$ and standard deviation $\sigma(\EB)$~\cite{DisorderGroundStateASI}. \\\par

Due to the periodic nature of many ASIs, \hotspice chooses to perform the simulation on a rectilinear grid as this allows efficient matrix and array manipulations, as well as straightforward implementation of both open and \xref{periodic boundary conditions} (PBC).
Each grid point may or may not contain a magnet, and the magnets must either all be of the IP type, or all OOP.
The benefits and details of this approach will be elaborated on in~\cref{sec:2:Implementation}.
Even though this implementation does not allow complete freedom in the placement of magnets, many popular ASI lattices can be constructed in this manner.
\cref{fig:2:ASIs} showcases the 12 lattices that \hotspice provides out-of-the-box. \par
The pinwheel and square lattices come in two variants, related by a global \ang{45} rotation of the entire lattice~\cite{ApparentFMpinwheel}.
In finite ASI, this results in different types of ASI edges due to the Cartesian character of the underlying grid, which can alter the dynamics of the ASI.
Furthermore, the unit cell for lucky-knot pinwheel (b) and open square (d) is more compact than for the more popular diamond pinwheel (a) and closed square (c), resulting in faster simulation~\cite{AdvancesASI}. \par % REF shows more popular lattices
Magnets in the pinwheel lattices (a) and (b) are placed at the same location as in the square lattices (c) and (d), respectively, but with each magnet rotated by \ang{45}.
The same can be said of the triangle (f) and kagome (g) lattices where individual magnets are rotated by \ang{90}.
The Cairo lattice (h) can be continuously deformed into the long-island Shakti lattice~\cite{ShaktiCairo,ShaktiCairoSquare}, but note that the \xref{point dipole model} is no longer appropriate for the latter; instead, a dumbbell model (see~\cref{sec:2:Dumbbell}) would be more accurate.
The remaining four IP and four OOP lattices are also related: the magnets in the OOP lattices (i)-(l) are positioned at the vertices where magnets meet in their respective IP counterparts (e)-(h). \par
By omitting certain magnets from these lattices, several others like the Tetris~\cite{Saglam2022Tetris,nisoli2018topologytetris} and short-island Shakti~\cite{gilbert2014emergent} lattices can be constructed.

\xfig[1.0]{2_Hotspice/ASIs.pdf}{
	\label{fig:2:ASIs}
	Predefined \xlabel[nolabel]{artificial spin ice} (ASI) lattices available in \hotspice.
	The unit cell of each lattice is delineated by a central dark grey rectangle.
	The red indicator defines the lattice parameter $a$.
	In the Ising approximation, the magnetisation of in-plane magnets (top) aligns along the major axis of the depicted ellipses.
	Out-of-plane magnets (bottom) are illustrated as circles.
}

\newpage
\section{Energy calculation}\label{sec:2:Energy}
%The energy of the system is an essential quantity in these Monte Carlo simulations.
In this section, we will list the various energy contributions used in \hotspice, explain how we may account for the finite size of real \link{single-domain nanomagnet}{nanomagnets} within this Ising-like model, and show how the \xref{effective energy barrier} $\EBeff$ is calculated.

\subsection{Energy contributions}
Three \idx{energy contributions} have been implemented\footnote{
	Users can implement more energy contributions by inheriting from the \python{hotspice.Energy} class and implementing the \python{abstractmethod}s, taking care to correctly account for open or periodic BC when necessary.
} in \hotspice.
\begin{enumerate}
	\item The \emph{\xlabel{magnetostatic interaction} energy} between magnets $i$ and $j$
	\begin{equation}
		\label{eq:2:E_MS}
		E_{\mathrm{MS},i,j} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\norm{\vc{r}_{ij}}^3} - \frac{3(\vc{\mu}_i \bcdot \vc{r}_{ij}) (\vc{\mu}_j \bcdot \vc{r}_{ij})}{\norm{\vc{r}_{ij}}^5}) \mathrm{,}
	\end{equation}
	with $\mu_0$ the vacuum permeability and $\vc{r}_{ij} = \vc{r}_j - \vc{r}_i$ the vector connecting the two magnetic dipoles $\vc{\mu}_i$ and $\vc{\mu}_j$. \par
	This is the main interaction dictating how separate nanomagnets influence each other, causing the typical properties of the various ASI lattices, such as \xref{superferromagnetism} in pinwheel ASI~\cite{li2018pinwheel}.
	Because of its importance, this is the only interaction \hotspice considers by default when an ASI is created.
	An ASI stores a list of energy contributions: when needed, the user must explicitly add other energy contributions to this list; see \cref{app:API}.
	This avoids wasting calculations on energies not relevant to the simulation.
	
	\item The \idx{Zeeman energy} of an \xlabel{external magnetic field} $\vc{B}_\mathrm{ext}$ interacting with magnet $i$
	\begin{equation}
		\label{eq:2:E_Z}
		E_{\mathrm{Z},i} = -\vc{\mu}_i \bcdot \vc{B}_\mathrm{ext} \mathrm{,}
	\end{equation}
	where $\vc{B}_\mathrm{ext}$ can be set for each magnet individually. \par
	This energy contribution provides a means for the outside world to interact with the system, and is therefore indispensable when we will be investigating \xref{reservoir computing} later on.
	Even if input is provided through other means than an external field, this energy contribution can often still be used by considering an effective field instead.
	
	\item The \textit{\xlabel{exchange coupling} energy} between nearest neighbours (NNs) $i$ and $j$
	\begin{equation}
		\label{eq:2:E_exch}
		E_{\mathrm{exch},i,j} = J \frac{\vc{\mu}_i \bcdot \vc{\mu}_j}{\mu_i \mu_j} \mathrm{,}
	\end{equation}
	with $J$ the exchange coupling constant, which is constant throughout the ASI. \par
	This interaction is rarely present in ASI, but can for example be relevant in interconnected ASI --- whether by design or due to limited lithographic accuracy.
	We will encounter an example of the latter in \cref{sec:3:MFM}.
\end{enumerate}

The combined \idx{interaction energy} $E_i$ of a single magnet $i$ with its environment is then given by
\begin{equation}
	\label{eq:2:E}
	E_i = E_{\mathrm{Z},i} + \sum_j E_{\mathrm{MS},i,j} + \sum_{j \in \mathcal{N}_i} E_{\mathrm{exch},i,j} \mathrm{,}
\end{equation}
where $\mathcal{N}_i$ is the collection of nearest neighbours of magnet $i$.
Which magnets are included in this collection depends on the ASI lattice and which site of the unit cell magnet $i$ is in, and can be defined separately for each ASI lattice. \par
Note that all terms in \cref{eq:2:E} simply change sign\footnote{
	Energy contributions which are independent of the state $s_i$, such as the magnetostatic self-energy (i.e., the energy due to the magnetisation profile $\vc{M}(\vc{r})$ throughout the magnet), are not included in \cref{eq:2:E} because they are constant and therefore do not affect the ASI dynamics.
} when magnet $i$ switches ($\vc{\mu}_i \rightarrow -\vc{\mu}_i$): if the switch occurs at time $t$, then $E_i(t + dt) = -E_i(t - dt)$.
%As such, $E_i$ represents the total interaction energy of a magnet with its `neighbours',\footnote{
%	In this context, a `neighbour' of a magnet can be interpreted more broadly as all magnets it interacts with through a particular energy contribution. For example, the magnetostatic interaction considers all magnets to be `neighbours', unless the user has explicitly set a maximum interaction distance.
%}.
Therefore, the change in energy of the ASI when that magnet switches is simply $\Delta E_{i,1\rightarrow2} = -2 E_i$.
This is called the \idx{switching energy}, and we will see in the next few sections that it plays a central role in both algorithms used for simulating system dynamics as well as the calculation of the \xref{effective energy barrier} $\EBeff$.
It is therefore very advantageous that the switching energy is so cheap to compute. \par
When the simulation is initialised, the energy contributions are calculated for all magnets.
Each magnet therefore stores a value in memory for each of the terms in \cref{eq:2:E}.
Whenever a magnet switches, the energies of all magnets it interacts with are updated accordingly, which constitutes a major part of the calculation effort required for every step in the simulation.

\subsection{Finite-size corrections to the magnetostatic energy}\label{sec:2:finite}
\cref{eq:2:E_MS,eq:2:E_Z,eq:2:E_exch} approximate each nanomagnet as a \link{point dipole model}{point dipole}, but real nanomagnets have a finite spatial extent.
If one assumes a uniform magnetisation throughout each nanomagnet, then this finite size does not affect the \xref{Zeeman energy}, nor the \link{exchange coupling}{exchange energy} which can capture any shape-related effects by an appropriate choice of the exchange coupling $J$.
By an appropriate choice of $\vc{B}_\mathrm{ext}$, the Zeeman energy could even account for a deviation from uniform magnetisation near the edge of a magnet, as occurs in reality, but which will be neglected in the following. \par
The \xref{magnetostatic interaction}, however, depends on the relative position, orientation, and shape of all magnets.
This may result in inadequate simulation of closely spaced ASI where the true magnetostatic coupling can be significantly stronger than predicted by a point dipole approximation.
Therefore, two (mutually exclusive) improvements have been implemented in \hotspice, which rescale the magnetostatic interaction energy between magnets.

\subsubsection{Second-order correction for dipoles}\indexlabel[tosection]{finite dipole model}
\textit{Politi and Pini}~\cite{Dipolar2Dparticles} have presented a multipole expansion of the magnetostatic interaction, to account for the finite size of 2D nanomagnets (i.e., lateral dimensions $\gg$ thickness), assuming a uniform magnetisation.
This results in a second-order correction
\begin{equation}
	E_{\mathrm{MS},i,j} = E_{\mathrm{MS},i,j}^\mathrm{(0)} + E_{\mathrm{MS},i,j}^\mathrm{(2)} \mathrm{,}
\end{equation}
where $E_{\mathrm{MS},i,j}^\mathrm{(0)}$ is the original \link{point dipole model}{point dipole} \xref{magnetostatic interaction} given by~\cref{eq:2:E_MS}. \par
The second-order correction can be written as
\begin{equation}
	\label{eq:2:E_MS_order2}
	E_{\mathrm{MS},i,j}^\mathrm{(2)} = \frac{\mu_0}{4\pi} \frac{3\mathcal{I}_{ij}}{2} \Bigg[3\frac{\vc{\mu}_i^\mathrm{OOP} \bcdot \vc{\mu}_j^\mathrm{OOP}}{\norm{\vc{r}_{ij}}^5} + \frac{\vc{\mu}_i^\mathrm{IP} \bcdot \vc{\mu}_j^\mathrm{IP}}{\norm{\vc{r}_{ij}}^5} -5\frac{(\vc{\mu}_i^\mathrm{IP} \bcdot \vc{r}_{ij}) (\vc{\mu}_j^\mathrm{IP} \bcdot \vc{r}_{ij})}{\norm{\vc{r}_{ij}}^7} \Bigg] \mathrm{,}
\end{equation}
where $\vc{\mu}_i$ was split into its IP and OOP components, conveniently leading to separate IP and OOP terms as implemented in the two types of ASI in \hotspice.
The particular shape of the nanomagnets is encapsulated in the single scalar $\mathcal{I}_{ij} = (\mathcal{I}_i + \mathcal{I}_j)/2$.
These $\mathcal{I}$ are calculated similar to a moment of inertia:
\begin{equation}
	\mathcal{I}_i = \int_{\Omega_i} \norm{\vc{r} - \ab(\int_{\Omega_i} \vc{r} d\vc{r})}^2 d\vc{r} \mathrm{,}
\end{equation}
with $\Omega_i$ the geometry of magnet $i$.
We assume all magnets have the same shape, such that $\mathcal{I}_{ij} = \mathcal{I}_i = \mathcal{I}_j$.
For elliptic cylinders, these moments of inertia reduce to the simple expression $\mathcal{I}_{ij} = \frac{1}{16}(l^2 + w^2)$.
Therefore, \hotspice assumes the magnets in an ASI to be round (OOP ASI) or elliptical (IP ASI), and allows the user to set the length $l$ and width $w$ of the magnets comprising an ASI.
Although IP magnets are typically more stadium-shaped~\cite{EmergentChiralityRatchet,clocking-protocol}, the moment of inertia for an ellipse does not differ by much --- if necessary, an appropriate value of $l$ and $w$ can account for this.
%While this correction can be applied to both IP and OOP magnetic dipoles, it is most effective for OOP systems, as can be seen in~\cref{fig:2:MS_distance}.

\subsubsection{Dumbbell model}\label{sec:2:Dumbbell}\indexlabel[tosection]{dumbbell model}
Instead of representing a magnet as a \link{point dipole model}{point dipole}, one may instead choose to represent it as a pair of opposite \xlabel[tosection]{magnetic charge}s~\cite{MagneticMonopoles2008,MagneticMonopoleDynamics}.
This introduces a new parameter $d$: the effective distance between the north and south poles of a magnet, with respective positions $\vc{r_N}_i = \vc{r}_i + s_i\frac{d_i}{2}\vc{u}_i$ and $\vc{r_S}_i = \vc{r}_i - s_i\frac{d_i}{2}\vc{u}_i$.
An appropriate choice of $d_i$ (slightly smaller than the physical length $l$ of the nanomagnet~\cite{DDG_Masterproef}) allows this \emph{dumbbell model} to emulate the spatial extent of a real nanomagnet. \par
The north and south poles are assigned the magnetic charges $+q_i$ and $-q_i$, respectively, with $q_i=\mu_i/d_i$~\cite{MagneticMonopoles2008}.
This choice yields the same effective dipole moment at long distance.
The interaction energy between two magnetic charges $q$ and $q'$ can be derived from the magnetic version of Coulomb's law~\cite{ForceMagneticDipole} as
\begin{equation}
	E = -\int_\infty^{\vc{r}} \frac{\mu_0}{\num{4}\pi}\frac{qq'}{\norm{\vc{r}}^3} \vc{r} \cdot d\vc{r} = \frac{\mu_0}{\num{4}\pi} \frac{qq'}{\norm{\vc{r}}} \mathrm{.}
\end{equation}
The magnetostatic interaction energy between two nanomagnets is then the sum of their four mutual magnetic charge interactions, finally resulting in
\begin{equation}
	\label{eq:2:E_MS_mono}
	E_{\mathrm{MS},i,j} = \frac{\mu_0 \mu_i \mu_j}{4\pi d_i d_j} \Bigg(\frac{1}{\norm{\mathbf{r_N}_i - \mathbf{r_N}_j}} + \frac{1}{\norm{\mathbf{r_S}_i - \mathbf{r_S}_j}}\\ - \frac{1}{\norm{\mathbf{r_N}_i - \mathbf{r_S}_j}} - \frac{1}{\norm{\mathbf{r_S}_i - \mathbf{r_N}_j}}\Bigg) \mathrm{.}
\end{equation}
The minus sign in the equation appears because north and south poles have opposite charge.
\hotspice is limited to a single value of $d$ for all magnets, due to the structure of the kernels used to calculate the magnetostatic interaction, which will be discussed in~\cref{sec:2:Kernels:Structure}.

\subsubsection{Comparison}
To assess whether these two corrections constitute an improvement to the resulting \link{magnetostatic interaction}{magnetostatic energy}, we must quantify their impact by comparing them against a known solution.
For this, we use the \link{micromagnetic theory}{micromagnetic simulation} package \mumax~\cite{mumax3}, which can determine the magnetostatic interaction energy for a given arrangement of ferromagnetic material.
While this solution is not exact, as \mumax uses a \xlabel{finite-difference} (FD) discretisation, the result will approach the true value for sufficiently small FD cell sizes.
\cref{fig:2:MS_distance} compares the original \link{point dipole model}{point dipole approximation} and the two corrections (``\link{finite dipole model}{finite dipole}'' and ``\link{dumbbell model}{dumbbell}'') against the solution obtained with \mumax.

\vspace{-1.5em}
\xfig[1.0]{2_Hotspice/MS_distance.pdf}{
	\label{fig:2:MS_distance}
	Magnitude of the magnetostatic interaction between two magnets as a function of their normalised centre-to-centre distance, for the three \hotspice{} calculation methods (\link{point dipole model}{point dipole}, \link{finite dipole model}{second-order correction for dipoles}, and \link{dumbbell model}{dumbbell}) compared to a \link{micromagnetic theory}{micromagnetic} \mumax calculation assuming uniform magnetisation.
	OOP magnets are assumed to be circular with diameter $2r$, IP magnets are ellipses with length $l$ and width $w=4l/11$.
	Positions of north and south \xref{magnetic charges} used in the dumbbell model are shown as red {\color{red}$\odot$} and blue {\color{blue}$\odot$} dots and are a distance $d=0.9l$ apart.
}

The figure shows 3 typical arrangements of neighbouring magnets in an ASI: two circular OOP magnets and two elliptical IP neighbours aligned along their \link{easy axis}{easy} and hard axes.
The magnetostatic interaction energy between the pair of magnets, divided by $\mu^2$ to be independent of material parameters and magnet size, is shown as a function of their normalised centre-to-centre distance.
A uniform magnetisation was used in the \mumax simulation, as this is the assumption under which the corrections were derived and because the non-uniform lowest energy magnetisation state is size-dependent while the figure uses dimensionless units. \\\par

For out-of-plane (OOP) systems, the \xref{dumbbell model} is inadequate due to the small fringe fields and the limited thickness of the magnets.
Instead, the \link{finite dipole model}{second-order dipole correction} is more appropriate, yielding a significant improvement towards the \mumax result.
Still, a discrepancy remains for separations below $r_{ij}/2r \lessapprox 1.5$, which could be reduced by even higher-order corrections. \\\par

For IP systems, the dumbbell model constitutes a vast improvement over the standard \link{point dipole model}{point dipole} treatment.
The dumbbell model does, however, require an additional parameter $d$, which affects the interaction energy.
For the best correspondence with \mumax, the charge-to-charge distance $d$ should be set slightly shorter than the length $l$ of a magnet, typically around $d/l\approx0.9$. % Analytical: pi/4?
This adjustment accounts for the curvature of real nanomagnets, which results in a spatially distributed magnetic charge whose mean position is not at the end of the nanomagnet, but rather slightly closer to its centre.
Similar values for $d/l$ were previously found in~\ccite{DDG_Masterproef} for typical nanomagnet shapes like ellipses and stadiums. \par
In contrast, the second-order dipole correction has little effect in IP systems and can even increase the discrepancy with \mumax.
It emulates increased spatial extent and therefore always increases the interaction, but for magnets neighbouring along their hard axes (rightmost panel in~\cref{fig:2:MS_distance}) the point dipole model already overestimates the interaction. \par
In conclusion, the dumbbell model is preferred for IP systems, while the second-order dipole correction is most suitable for OOP systems.

\subsection{Effective energy barrier}\label{sec:2:E_B_eff}
To properly simulate ASI dynamics and hysteresis, knowledge of the \idx[tosection]{effective energy barrier} $\EBeff$, which separates the two magnetisation states of each magnet, is crucial.
Recall that this quantity is distinct from the \xref{shape anisotropy} $\EB$: the effective energy barrier $\EBeff$ is a modification of $\EB$ caused by the interaction with other magnets. 
This is illustrated in~\crefSubFigRef{fig:2:EB_meanbarrier}{a}, where \indexlabel{energy landscape}energy landscapes are shown for various values of the \xref{switching energy} $\Delta E$. \par

\vspace{-1em}
\xfig[1.0]{2_Hotspice/EB_meanbarrier.pdf}{
	\label{fig:2:EB_meanbarrier}
	Mean-barrier approximation (\ref{eq:2:EB_meanbarrier_original}, \textcolor{lightblue}{blue}) and its conditional form (\ref{eq:2:EB_meanbarrier_cases}, \textcolor{lightred}{red}) compared to the exact energy barrier (\ref{eq:2:EB_exact}, \textcolor{mplgrey}{grey}).
	Due to the limited information available to a \xref{mean-barrier model} (only $\EB$ and $\Delta E$), the exact solution assumes a sinusoidal shape anisotropy and a uniform \xref{external magnetic field} along the \xref{easy axis}.
	\textbf{(a)} Effective energy barrier $\EBeff$ as function of \xref{switching energy} $\Delta E$ for these approximations.
	\textbf{(b)} Energy landscape between the two stable states (black filled circles) for several values of the switching energy $\Delta E$.
	The energy landscapes without and with shape anisotropy are shown as dotted black and solid grey lines, respectively.
	An open circle indicates the halfway point between both stable states, while a star is put at the top of the exact energy barrier.
	On each landscape, the height of the barrier in the different approximations is indicated by a dotted horizontal line, indicating how they use different values on the energy landscape for their estimate.
}
\vspace{-1em}

The energy landscape for $\Delta E = 0$ reveals the shape anisotropy $\EB$ while slanted energy landscapes for $\Delta E \neq 0$ show how $\EBeff$ is affected by these interactions.
The figure shows that $\EBeff$ can be calculated at various levels of accuracy, which may yield different switching rates or even a different switching order~\cite{leo2021chiral}. \par
In this section, we will explore some of these approximations for $\EBeff$.
All of them make use of the switching energy $\Delta E$ between the current and opposite state of a magnet, since this is cheap to compute\footnote{
	Recall that the terms in \cref{eq:2:E} simply change sign when the magnetisation $\vc{\mu}$ of a magnet is reversed.
} as $\Delta E = -2 E$.
The subscript $i$ will be omitted for the remainder of this section; it is implied that the following discussion and equations apply to each magnet individually.

\subsubsection{Intrinsic barrier due to \xlabel{shape anisotropy}}\label{sec:2:shape_anisotropy}
An isolated single-domain nanomagnet often exhibits two stable magnetisation states separated by energy barriers.
In IP ASI, these states usually arise from \textit{shape anisotropy}, which originates from the \xlabel{demagnetising field} of the magnet itself.
This imposes an additional energy cost when the \xref{magnetisation} does not point along the preferential \idx{easy axis}, which is usually the longest axis of the magnet's geometry~\cite{PhD_Leliaert}. \par
This shape anisotropy can be quantified by the \xlabel{uniaxial anisotropy} constant $K_\mathrm{u} = \frac{\mu_0}{2} M_\mathrm{sat}^2 \Delta \mathcal{N}$, with $\Delta \mathcal{N}$ the difference between the \xlabel[tosection]{demagnetising factor}s along the IP easy and hard axes of a magnet~\cite{AdvancesASI,VogelFulcherTammannFreezing,andersson2016thermally}. % (when normalised as $\mathcal{N}_x + \mathcal{N}_y + \mathcal{N}_z = 1$)
These demagnetising factors can be calculated analytically for simple shapes like ellipsoids~\cite{EllipsoidDemag,EllipseDemag}.
Assuming that switching occurs by \xlabel{coherent rotation}\footnote{
	Depending on the magnet's properties, \xlabel{non-coherent magnetisation reversal} such as switching by \xlabel{domain wall nucleation} may be energetically cheaper.
	An accurate estimate of the energy barrier for such processes can, for instance, be determined through \link{micromagnetic theory}{micromagnetic simulations}.
}, this leads to an energy barrier $\EB = K_\mathrm{u} V$, with $V$ the magnet's volume.
Similar to the calculation of the magnetic moment $\mu$ in \cref{fn:2:moment_integral}, relaxation of the magnetisation profile $\vc{M}(\vc{r})$ near the surface of the magnet may cause the actual energy barrier to be slightly smaller than $K_\mathrm{u} V$.
\hotspice ignores the specifics of the reversal process by allowing the user to set an arbitrary value of $\EB$. \par
Since magnets in ASI are typically very flat, the shape anisotropy usually results in a preferential in-plane magnetisation direction.
OOP nanomagnets can nonetheless be obtained by properly choosing materials that exhibit a strong interfacial anisotropy, as for example occurs at the Co/Pt interface, resulting in a net preferential out-of-plane magnetisation.

\subsubsection{Mean-barrier model}
The simplest approximation of the effective energy barrier $\EBeff$ is the \idx{mean-barrier model}.
It assumes that the highest-energy state lies halfway between the two stable states.
This means the barrier height changes at half the rate at which the \xref{switching energy} $\Delta E$ changes, leading to the approximation
\begin{equation}
	\label{eq:2:EB_meanbarrier_original}
	\EBeff = \EB + \frac{\Delta E}{2} \mathrm{,}
\end{equation}
as it is often encountered in literature~\cite{MC_TemperatureDesorption,DirectionalEnergyBarrier,HyperCubicThermalASI}.
This method is illustrated in blue in~\cref{fig:2:EB_meanbarrier}. \par
However, this is a very crude approximation which does not account for the extreme cases where the interactions are so strong that the energy barrier effectively disappears.
In the model of \cref{eq:2:EB_meanbarrier_original}, this happens when $\abs{\Delta E}$ exceeds twice the \xref{shape anisotropy} $\EB$, leaving only one global minimum.
This can be seen in \crefSubFigRef{fig:2:EB_meanbarrier}{a} for $\Delta E = \pm 4 \EB$, where \cref{eq:2:EB_meanbarrier_original} (blue) clearly does not make physical sense.
To handle these situations, \hotspice modifies the calculation of the effective energy barrier $\EBeff$ as follows:
\begin{equation}
	\label{eq:2:EB_meanbarrier_cases}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} + \frac{\Delta E}{2} & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B}, \\
		\Delta E & \quad \text{otherwise}.
	\end{cases}
\end{equation}
This way, $\Delta E$ serves as the barrier when the original energy barrier disappears.
This adjustment is shown in red in~\cref{fig:2:EB_meanbarrier}: it is closer to the exact solution except for $-4 \EB < \Delta E < -2 \EB$.
Notwithstanding that the original mean-barrier approximation~\eqref{eq:2:EB_meanbarrier_original} lies closer to the exact solution in this range, we choose not to use it as this lies outside its valid range of $\abs{\Delta E} < 2 \EB$.

\subsubsection{Asymmetric barrier}\label{sec:2:E_B_asymm}\indexlabel[tosection]{asymmetric energy barrier}
The simple \xref{mean-barrier model} is insufficient for many IP ASI lattices.
In real nanomagnets, reversal by \xref{coherent rotation} can occur via two pathways; clockwise ($\clockwise$) or counter-clockwise ($\counterclockwise$) rotation of the magnetisation.
In an asymmetrical environment --- when the \xref{effective field} has a non-zero component perpendicular to the \xref{easy axis} --- one of these two rotation directions will be preferred~\cite{leo2021chiral,DirectionalEnergyBarrier}.
Take for example the pinwheel lattice (\crefSubFigRef{fig:2:ASIs}{a}): any two neighbouring magnets form a T-shape, so the magnet pointing into the side of the other will greatly influence whether the other magnet prefers $\clockwise$ or $\counterclockwise$ rotation~\cite{DirectionalEnergyBarrier}. % Q: does this need a figure?
% The mean-barrier model would take the energy difference $\Delta e$ between the two states; for two magnets in a T-shape, $\Delta E = 0$ for both, resulting in an unchanged energy barrier $\EB$ which is clearly incorrect in this situation.
Such an asymmetry can not occur in OOP ASI, so the asymmetric barrier is only applicable to IP ASI.
Accounting for the existence of these separate chiral switching channels profoundly affects the switching rates and transition kinetics, since switching will occur predominantly via the more favourable pathway~\cite{leo2021chiral}. \\\par

In the dipole model, this can be accounted for by considering the energy of each magnet in these transitional states along the hard axis.
Therefore, \hotspice tracks yet another quantity for every magnet: $E_\perp$\indexlabel{hard-axis interaction energy}, representing the \xref{interaction energy} of a magnet if it would point along $\vc{e}_z \times \vc{u}$, i.e., \ang{90} counter-clockwise from its normal \xref{magnetisation} direction $\vc{u}$.
Note that we are not deviating from the two-state Ising model: we are simply putting ``test dipoles'' along the hard axis to get a better estimate of $\EBeff$, but magnets will never end up in these states during a simulation. \par
In \cref{eq:2:EB_meanbarrier_cases}, $\EB$ essentially represented an estimate of the total energy in the transitional state.
With our newfound knowledge of the interaction energy $E_\perp$ in that state, our improved estimate of the total hard-axis energy becomes $\EB + E_\perp$.
Therefore, we get an expression for the effective barrier $\EBeff$ along the two rotation pathways ($\pm$) by simply substituting $\EB \rightarrow \EB + E_\perp$ in \cref{eq:2:EB_meanbarrier_cases}:
\begin{equation}
	\label{eq:2:EB_asymmetric}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} \pm \rho E_\perp + \frac{\Delta E}{2} & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B} \pm \rho E_\perp, \\
		\Delta E & \quad \text{otherwise}, \\
	\end{cases}
\end{equation}
which results in two different barriers if $E_\perp \neq 0$.
The parameter $\rho = \mu_\perp/\mu_\parallel > 0$ was introduced in \cref{eq:2:EB_asymmetric} to account for \xref{non-coherent magnetisation reversal} processes like \xlabel[nolabel]{domain wall nucleation} and propagation, which result in an effective reduction of the magnetic moment during reversal~\cite{leo2021chiral,TimeResolvedDynamicsSOT}.
Using a value $\rho < 1$ improves correspondence with experimental observations, as we will observe in~\cref{sec:2:Applications_reversal_Pinwheel}. \par
As a bonus, we can use the combined knowledge of $E$ and $E_\perp$ to implicitly define the \idx{effective field} $\vc{B}_\mathrm{eff}$ that any given magnet experiences by
\begin{equation}
	\label{eq:2:B_eff_implicit}
	\begin{cases}
		\vc{\mu} \,\cdot\, \vc{B}_\mathrm{eff} = E \mathrm{,} \\
		\norm{\vc{\mu} \times \vc{B}_\mathrm{eff}} = E_\perp  \mathrm{.}
	\end{cases}
\end{equation}
For IP ASI, this uniquely defines $\vc{B}_\mathrm{eff}$ since all vectors lay in-plane.
Solving \cref{eq:2:B_eff_implicit} for $\vc{B}_\mathrm{eff}$ yields an expression as function of $E$, $E_\perp$ and $\vc{\mu}$, valid for both IP and OOP ASI:
\begin{equation}
	\label{eq:2:B_eff_explicit}
	\vc{B}_\mathrm{eff} = - \frac{E \vc{\mu} + E_\perp \vc{e}_z \times \vc{\mu}}{\mu^2} \mathrm{.}
\end{equation}

\subsubsection{Exact solution}
The effective energy barrier $\EBeff$ can be calculated exactly if an analytical expression is known for the energy as a function of magnetisation angle $\theta$ relative to the magnet's \xref{easy axis}.
The \xref{shape anisotropy} creates an \xref{energy landscape} with two minima at $\theta=0$ and $\theta=\pi$, but the exact form of this landscape depends on the magnet's shape.
For ellipsoidal magnets, the \xref{energy landscape} is $-\frac{E_\mathrm{B}}{2} \cos{2\theta}$~\cite{neel1949theorie}.
Assuming a uniform magnetisation in each magnet\footnote{
	Only perfectly ellipsoidal magnets have uniform magnetisation in a uniform \xref{external magnetic field}~\cite{EllipsoidDemag,MaxwellElectricityMagnetism}.
}, the \link{magnetostatic interaction}{magnetostatic} and \link{Zeeman energy}{Zeeman} interactions add a term proportional to $\cos(\theta - \phi)$, with $\phi$ the angle of their combined \xref{effective field} $\vc{B}_\mathrm{eff}$.
Thus, the total landscape is a sum of two sines and can be fully characterised if $E$ and $E_\perp$ are known.
However, in the general case, this results in a transcendental equation
\begin{align*}
	\label{eq:2:EB_exact_transcendental}
	\frac{\partial E(\theta)}{\partial \theta} = 0 \iff & \frac{\partial}{\partial \theta} \ab(-\frac{\EB}{2} \cos{2\theta} - \mu B_\mathrm{eff} \cos(\theta - \phi)) = 0 \\
	\iff & \EB \sin{2\theta} + \mu B_\mathrm{eff} \sin(\theta - \phi) = 0 \mathrm{,} \numberthis
\end{align*}
which would require numerical approximation to solve.
Since this could significantly impact performance, this is not done in \hotspice. \\\par
In OOP ASI, however, the high degree of symmetry nonetheless allows an explicit expression to be obtained.
All relevant vectors ($\vc{B}_\mathrm{eff}$, $\vc{\mu}$ ...) in such systems point along the z-axis, causing the \link{hard-axis interaction energy}{hard-axis energy} to vanish ($E_\perp=0$) such that the equation is no longer transcendental.
Solving \cref{eq:2:EB_exact_transcendental} and applying the double-angle identities for $\cos(2\theta)$ and $\sin(2\theta)$ instead leads to the quadratic relation % Q: should I derive this from the d/dtheta equations or is this clear enough? (only needs double-angle rules for cos and sin basically)
\begin{equation}
	\label{eq:2:EB_exact}
	\widetilde{E_\mathrm{B}} = \begin{cases}
		E_\mathrm{B} \ab(\frac{\Delta E}{4 E_\mathrm{B}} + 1)^2 & \quad \text{if} \quad \abs{\frac{\Delta E}{2}} < E_\mathrm{B}, \\
		\Delta E & \quad \text{otherwise,} \\
	\end{cases}
\end{equation}
as has previously been described by \textit{Tannous and Gieraltowski}~\cite{StonerWohlfarth2008}.
This is the exact solution that was shown in grey in~\cref{fig:2:EB_meanbarrier}.
However, since OOP magnets are not ellipsoidal as assumed by the transcendental equation that originally led to \cref{eq:2:EB_exact_transcendental}, the applicability of \cref{eq:2:EB_exact} remains questionable.

\newpage % For better alignment
\section{Dynamics}\label{sec:2:Dynamics}
A Monte Carlo simulator would not be complete without an algorithm to change the state of the system.
Since the magnetisation of any magnet in the ASI may spontaneously switch due to thermal fluctuations, \hotspice evaluates the time evolution of the ASI in a stepwise manner using an \idx{update algorithm} that determines which magnet should switch next. \par
In particular, \hotspice uses \idx{kinetic Monte Carlo} (KMC) algorithms\footnote{Sometimes also referred to as dynamic Monte Carlo.}.
Generally speaking, these can be divided into two distinct classes: rejection-free and rejection KMC.
\hotspice implements algorithms of both types.
These techniques go by many names --- here, we shall refer to them as the \emph{\xref{first-switch method}} and \emph{\xref{Metropolis-Hastings sampling}}, respectively~\cite{gillespie1976general,PhysicalTimeKMC}.
The former is more suitable for simulating the temporal evolution of the system, while the latter can be used to sample the equilibrium distribution of the state space.

\subsection{First-switch method: temporal evolution}
\idx{N\'eel relaxation theory}~\cite{neel1949theorie} states that, for an isolated nanomagnet, the \xlabel{switching rate} $\nu$ is given by the \xlabel{N\'eel-Arrhenius} equation
\begin{equation}
	\label{eq:2:Néel}
	\nu = \nu_0 \exp\ab(-\frac{\EB}{\kBT}) \mathrm{,}
\end{equation}
with $\kBT$ the thermal energy and $\nu_0$ the so-called \xref{attempt frequency}.
For mutually interacting magnets, $\EB$ can be replaced by the \xref{effective energy barrier} $\EBeff$ in \cref{eq:2:Néel}. \par
In the general case where the energy barriers for clockwise and counter-clockwise rotation during switching differ, these two switching channels ($\circlearrowright$ and $\circlearrowleft$) will separately follow~\cref{eq:2:Néel}, so their switching frequencies must be combined.
This yields the total switching rate as presented by Koraltan~\etal~\cite{DirectionalEnergyBarrier},
\begin{equation}
	\label{eq:2:Néel_2}
	\nu = \nu_\circlearrowleft + \nu_\circlearrowright = \frac{\nu_0}{2} \ab[\exp\ab(-\frac{\EBeffLeft}{\kBT}) + \exp\ab(-\frac{\EBeffRight}{\kBT})] \mathrm{,}
\end{equation}
where a halved attempt frequency $\nu_0/2$ was assigned to either switching channel such that~\cref{eq:2:Néel_2} reduces to~\cref{eq:2:Néel} in the case of $\EBeffLeft=\EBeffRight$~\cite{leo2021chiral}.
% Note that it does not matter which barrier corresponds to clockwise or counter-clockwise rotation; only the height of both barriers matters when determining the switching rate of a magnet.

\paragraph{Attempt frequency\indexlabel{attempt frequency}}
An estimate of $\nu_0$ for coherent magnetisation reversal can be obtained from the limit $\EB \rightarrow 0$, where the \xref{switching rate} $\nu$ should approach the \xlabel{gyromagnetic precession frequency} of the magnetisation of a nanomagnet.
This is reported to be on the order of \SIrange{e9}{e10}{\hertz}~\cite{BrownThermalFluctuations,bean1959superparamagnetism}.
As $\EB \rightarrow 0$, \cref{eq:2:Néel} implies that $\nu \rightarrow \nu_0$, so we use $\nu_0=\SI{e10}{\hertz}$~\cite{JM_Masterproef}. \par % Each oscillation is an attempt to leave this energy minimum.
An order-of-magnitude estimate often suffices, since any small (i.e., $\sim \kBT$) error on $\EBeff$ yields an exponential change of the switching rate.
Furthermore, $\nu_0$ only affects the elapsed time, not the switching order, so its impact on the simulation is limited.
If necessary, a more precise value for $\nu_0$ can be obtained from experiments. \par
It must be noted that values of $\nu_0$ on the order of \SI{e10}{\hertz} are mostly valid for reversal by \xref{coherent rotation}, as was already assumed during the calculation of the \link{effective energy barrier}{effective barriers}.
For other reversal processes (e.g., domain wall-mediated reversal), entropic contributions can cause the attempt frequency to differ by many orders of magnitude, since its interpretation as a rotation frequency then no longer applies~\cite{ArrheniusPrefactor,RetentionTimeMeyerNeldel}.
In such cases, $\nu_0$ can also exhibit a significant dependence on other factors like temperature or the energy barrier itself~\cite{AttemptFreqTemperature,RetentionTimeMeyerNeldel}. % AttemptFreqTemperature says: ``When this energy barrier is the result of a collective statistical behavior of many constituents, it may contain an intrinsic temperature dependence which has to be carefully taken into account, in particular when the prefactor is interpreted in terms of an attempt frequency.''

\paragraph{Algorithm}
Knowledge of the \xref{switching rate} $\nu$ can be used to construct a rejection-free kinetic Monte Carlo method, first presented by \textit{Gillespie}~\cite{gillespie1976general}, referred to as the ``first-switch'' method.
Each iteration, as presented in~\cref{alg:2:FirstSwitchSingle}, will increment the elapsed time by a certain duration $t \leq t_\mathrm{max}$, where the maximum time $t_\mathrm{max}$ can be set by the user to prevent excessively long switching times.
These iterations are then repeated until the desired elapsed time is reached. \par
The maximum time $t_\mathrm{max}$ (default value of one second) prevents the simulation from advancing too far into the future.
For example, it is needed when a time-dependent \xref{external magnetic field} is applied to the lattice: for a sinusoidal signal of frequency $f$, using $t_{\mathrm{max}}=20/f$ will ensure that the waveform is captured in sufficient detail.
Furthermore, it prevents unrealistic timescales, as the exponential character of the \xref{N\'eel-Arrhenius} law can cause switching times to become much longer than what could ever be observed experimentally. \par
\begin{algorithm}[Single iteration of the ``first-switch'' method\indexlabel{first-switch method}]
	\label{alg:2:FirstSwitchSingle}
	\begin{enumerate}[rightmargin=15pt]
		\item Calculate the \xref{effective energy barriers} of all magnets (i.e., $\EBeff{}_{,i}$ for OOP ASI, $\EBeffLeft{}_{,i}$ and $\EBeffRight{}_{,i}$ for IP ASI, $\forall i$) based on their \link{interaction energy}{interaction energies} $E_i$, as determined by the current magnetisation state.
		\item Calculate the \xref{switching rate} $\nu_i$ of each magnet using \cref{eq:2:Néel_2}.
		\item Generate a random switching time interval $\Delta t_i$ for each magnet $i$, sampled from an exponential distribution with mean value $1/\nu_i$.
		\item Determine which magnet $j$ has the smallest such time $\Delta t_j = \min_i \Delta t_i$.
		\item Finally, $t_\mathrm{max}$ determines whether a switch occurs.
		\begin{itemize}
			\item \textit{If $t + \Delta t_j \leq t_\mathrm{max}$}: increment the elapsed time $t$ by $\Delta t_j$ and switch magnet $j$.
			\item \textit{If $t + \Delta t_j > t_\mathrm{max}$}: increment the elapsed time $t$ by $t_\mathrm{max}$ without switching a magnet.\footnote{The maximum time $t_\mathrm{max}$ means that, strictly speaking, this is no longer a rejection-free algorithm.}
		\end{itemize}
	\end{enumerate} % NOTE: this is different from the BKL algorithm, yet still gives the exact same result.
\end{algorithm}
% Note that this algorithm still gives the correct switching rate $\nu$, even though the switching time $t_i$ is sampled randomly multiple times before magnet $i$ typically switches. This is because, the more magnets there are, the harder it gets for an arbitrary magnet $i$ to have the shortest randomly sampled time (and thus to switch), but the elapsed time increment after each switch will be smaller as well, and these two effects cancel perfectly to end up with the same switching rate as if the magnet would be the only one switching.
A situation can occur where $\EBeff < 0$, in which case real magnets would switch deterministically.
In \hotspice, we choose to apply~\cref{eq:2:Néel_2} regardless, even though this likely yields non-physical random switching times $\Delta t_i \ll 1/\nu_0$.
Therefore, we must interpret the elapsed time $t$ as being ``instantaneous'' if $t < 1/\nu_0$.
The alternative --- limiting $\Delta t_i$ to be $\geq 1/\nu_0$ --- is not possible, as this would overestimate the elapsed time $t$. \par
In the realm of rejection-free KMC, two commonly used algorithms are the very similar \idx{Bortz-Kalos-Lebowitz}~\cite{nfoldMCalgorithm} and \idx{Gillespie}~\cite{gillespie1976general} algorithms.
Though Gillespie also discusses the first-switch method, these two algorithms differ slightly from~\cref{alg:2:FirstSwitchSingle}, yet all yield identical results.
Their different strategies for calculating random switching times and selecting the next switch are discussed by \textit{Gibson and Bruck}~\cite{GibsonBruck}, alongside an optimised algorithm.

\subsection{Metropolis-Hastings: sampling equilibrium states}\label{sec:2:Dynamics_MH}
\idx{Metropolis-Hastings sampling} is a rejection-based KMC method designed to sample the state space at thermal equilibrium, where the probability of each state appearing is proportional to their \xlabel{Boltzmann factor} $\exp(-E/\kBT)$~\cite{IntroductionMC,kyimba2006comparisonIsingAlgorithms}.
Hence, in contrast to the \xref{first-switch method}, Metropolis-Hastings sampling is not intended to accurately model the system's transient dynamics.
Instead, it is more suitable for examining equilibrium statistical properties of ASI, e.g., the average magnetisation, heat capacity, correlations...~\cite{ApparentFMpinwheel} \par

\paragraph{Algorithm}
A rejection-based KMC method works by selecting magnets at random and then deciding, with a certain probability $P$, whether to switch them.
In particular, the Metropolis-Hastings algorithm repeats the steps outlined in~\cref{alg:2:MetropolisHastingsSingle}.
\vspace{-1em}
\begin{algorithm}[Single iteration of Metropolis-Hastings sampling]
	\label{alg:2:MetropolisHastingsSingle}
	\begin{enumerate}
		\item Select a magnet\footnote{
			For enhanced performance, multiple sufficiently distant magnets can be selected simultaneously, as will be explored in~\cref{sec:2:MultiSwitch}.
		} $i$ at random (with all magnets equally likely to be chosen).
		\item Calculate the energy change $\Delta E_i$ if this magnet were to switch.
		\item Switch the magnet with an \idx{acceptance probability}
		\begin{equation}
			\label{eq:2:MH_acceptance}
			P_{\mathrm{MH},i} = \begin{cases}
				\exp(-\Delta E_i/\kBT), & \text{if } \Delta E_i > 0 \mathrm{,} \\
				1 & \text{otherwise} \mathrm{.}
			\end{cases}
		\end{equation}
		\item \textit{Optional}:
		Increment the elapsed time $t$ by
		\begin{equation}
			\label{eq:Metropolis_time}
			\Delta t = -\frac{\exp\ab(\EBeff\big/\kBT\ab) \ln{\chi}}{N \nu} \mathrm{,}
		\end{equation}
		with $N$ the number of magnets in the system and $\chi$ a uniformly distributed random variable in the half-open interval $(0,1]$~\cite{PhysicalTimeKMC}. % Q: which \EBeff to use in the asymmetric barrier case? Hotspice uses the smallest barrier. Can also use an exponentially weighted average like in the first-switch method (\exp(x) = \frac{1}{\exp(-a) + exp(-b)}).
	\end{enumerate}
\end{algorithm}

\paragraph{Elapsed time in rejection KMC}
While the \xref{first-switch method} relies on an explicit calculation of the elapsed time, Metropolis-Hastings sampling does not strictly require this knowledge.
Therefore, the last step of~\cref{alg:2:MetropolisHastingsSingle} --- the calculation of the elapsed time --- is optional. \par
For a long time, the notion of a well-defined measure for the elapsed time in rejection KMC was controversial~\cite{nfoldMCalgorithm,GlauberTimescale_sadiq1984,MCSim_StatPhys}. % Refs: `nfoldMCalgorithm` mentions both GD & MH acceptance prob., but says that GD can have a timescale (their equation looks a lot like in PhysicalTimeKMC) yet Metropolis cannot (due to the max()?), which is no longer true. `GlauberTimescale_sadiq1984' uses a less rigorously defined yet similar expression as in `PhysicalTimeKMC`. Don't use this reference when referring to the lack of a timescale, because this one seems to be one of the better solutions presented in literature, yet not very rigorous. Also `Lattice Kinetic Descriptions for Bulk Reaction-Diffusion Processes: Application to Alloys under Irradiation' p.215 says that ``The relationship between the simulation time and the physical time has been widely debated [MCSim_StatPhys], and it is sometimes assumed that the two time scales are proportional.''
Often, the number of performed \xref{Monte Carlo sweeps}\footnote{
	A Monte Carlo \textit{step} refers to attempting to switch a single magnet.
	By a \idx{Monte Carlo sweep} (MCS), we refer to $N$ attempted switches when the simulation contains $N$ magnets~\cite{NumericalDynamicalNiedermayer}. % Can also be called a "macrostep"~\cite{bit-player_MCvsGlauber}.
	Note that no distinction between attempted switches and actual switches exists in the \xref{first-switch method}.
} was used as a crude measure, but eventually a formal derivation for the physical time scale in rejection KMC was presented by \textit{Serebrinsky}~\cite{PhysicalTimeKMC}, who derived~\cref{eq:Metropolis_time}. \par
Note that the \xref{effective energy barrier} $\EBeff$ only appears in the optional calculation of the elapsed time.
Energy barriers do not affect the \xref{acceptance probability} in the Metropolis-Hastings algorithm because they have no effect on the equilibrium state~\cite{DynamicalGlassyBehaviour}.

\paragraph{Disambiguation: Glauber dynamics}
The Metropolis-Hastings \xref{acceptance probability} in~\cref{eq:2:MH_acceptance} is the main reason why this algorithm samples the equilibrium state space.
However, it is not unique: there exist other forms of the acceptance probability which also correctly sample this equilibrium.
This is the reason why there often exists confusion between the \link{alg:2:MetropolisHastingsSingle}{Metropolis-Hastings algorithm} and the so-called \idx{Glauber dynamics}\footnote{
	The term ``Glauber dynamics'' is a misnomer~\cite{bit-player_MCvsGlauber}; the acceptance probability in~\cref{eq:2:GD_acceptance} was first derived by \textit{Flinn and McManus}~\cite{flinn1961TransitionProbability}, and later used by \textit{Glauber}~\cite{glauber1963time}.
}~\cite{flinn1961TransitionProbability,glauber1963time,flinn1974MCIsing}, both examples of \xlabel{Markov chain Monte Carlo} (MCMC) algorithms. % REF: flinn1974 possibly includes the first still images of the Ising system simulated using Monte Carlo methods.
These two methods differ only in their choice of acceptance probability~\cite{jang2004stochastic}: Glauber dynamics instead uses % According to~\cite{bit-player_MCvsGlauber}, an additional difference lies in their cell visitation strategies: MH is said to visit each cell exactly once per sweep, often in a scanline order, while GD visits cells randomly (on average missing \SI{37}{\percent} of cells each run). Other sources do not seem to corroborate this.
\begin{equation}
	\label{eq:2:GD_acceptance}
	P_\mathrm{GD} = \frac{\exp(-\Delta E / \kBT)}{1 + \exp(-\Delta E / \kBT)} \mathrm{.}
\end{equation}

For a MCMC algorithm to sample the equilibrium state space, it must satisfy two conditions: \idx{detailed balance} and \idx{ergodicity}.
\begin{itemize}
	\item The principle of \textit{detailed balance} states that, in a system at equilibrium, each elementary process must also be in equilibrium with its reverse process.
	This can be expressed mathematically as $\pi_i P_{ij} = \pi_j P_{ji}, \forall i,j$, with $P_{ij}$ the Markov transition probability from state $i$ to $j$, and with $\pi_i$ and $\pi_j$ the equilibrium probabilities of these states appearing.
	Since all magnets are equally likely to be chosen in step 1 of~\cref{alg:2:MetropolisHastingsSingle}, it can be shown that both $P_\mathrm{MH}$ and $P_\mathrm{GD}$ result in an algorithm that satisfies detailed balance~\cite{kyimba2006comparisonIsingAlgorithms}.
	\item To guarantee that a Monte Carlo Markov chain algorithm will eventually reach this equilibrium, it must be \textit{ergodic}, i.e., from a given state it must be possible to reach any other state via some route~\cite{kyimba2006comparisonIsingAlgorithms}.
	This is clearly satisfied by~\cref{alg:2:MetropolisHastingsSingle}, for both $P_\mathrm{MH}$ and $P_\mathrm{GD}$, as the acceptance probability is strictly positive at non-zero temperatures.
\end{itemize}
As such, \cref{alg:2:MetropolisHastingsSingle} is guaranteed to reach equilibrium eventually.
However, the rate of convergence may vary, as noted in~\cref{sec:2:Verification_OOP_Exchange}: end users must take care to perform sufficient Monte Carlo steps to ensure thermalisation. % MH seems to be the most physically accurate from a theoretical point of view, though a point can be made for either of them depending on the physics of the problem~\cite{jang2004stochastic}.

\vspace{-1em}
\sidefig[0.6]{2_Hotspice/RejectionKMC.pdf}{
	\label{fig:2:RejectionKMC}
	\mbox{Acceptance} probability used by Metro\-polis-Hastings and Glauber dynamics, as a function of the \xref{switching energy} $\Delta E$.
	\newline\newline % Newlines center the caption w.r.t. plot
}
\vspace{-1em}

These two common choices for the acceptance probability, $P_\mathrm{MH}$ and $P_\mathrm{GD}$, are compared in~\cref{fig:2:RejectionKMC}.
Both $P_\mathrm{MH}$ and $P_\mathrm{GD}$ result in an algorithm that satisfies ergodicity and detailed balance, so they both yield the same statistical values for quantities like the average magnetisation.
However, Glauber dynamics explores the state space more slowly because it is always more likely to reject a switch --- especially for small $\Delta E / \kBT$ --- since $P_\mathrm{GD}(\Delta E) < P_\mathrm{MH}(\Delta E), \forall \Delta E$~\cite{jang2004stochastic}.
Therefore, \hotspice uses the Metropolis-Hastings acceptance probability.
For a broader overview of Monte Carlo methods, we refer to Ref.~\cite{IntroductionMC}, which may further clarify the at times confusing naming present throughout literature.

\newpage
\section{Implementation}\label{sec:2:Implementation}
Now that the physical principles underlying the simulator have been extensively discussed, we turn our attention to its software implementation.
The introductory \cref{sec:2:Implementation:Grid} provides a brief motivation for our choice to implement the ASI on a rectilinear grid.
One of its main benefits is the existence of unit cells, which enable efficient \textit{kernels} for computing the \xref{magnetostatic interaction}. \par
These kernels are discussed in detail in~\cref{sec:2:Kernels}, which consists of four subsections.
First, \cref{sec:2:Kernels:Structure} explains their internal structure and how exactly they are used to calculate and update the magnetostatic interaction.
\cref{sec:2:Kernels:PBC} then explains how these kernels facilitate the implementation of \xref{periodic boundary conditions}.
Next, \cref{sec:2:Kernels:Perp} briefly notes how similar kernels can be used for the calculation of $\left. E_{\mathrm{MS}} \right|_{\perp}$, as used for the \xref{asymmetric energy barrier}.
Finally, \cref{sec:2:Kernels:Error} evaluates the effect of truncating the magnetostatic interaction beyond a certain distance, to reduce the computational load. \par
After this, \cref{sec:2:MultiSwitch} explores one particular performance-enhancing feature: the simultaneous sampling of multiple magnets during \xref{Metropolis-Hastings sampling}, facilitated by the grid-based kernel implementation.
In this context, a lower bound is derived for the minimum separation between simultaneously sampled magnets. \par
Subsequently, \cref{sec:2:Implementation:Performance} assesses the performance of \hotspice for both CPU and GPU calculations, considering the various kernels proposed in~\cref{sec:2:Kernels:Structure}.
The performance impact of the aforementioned simultaneous sampling of magnets during Metropolis-Hastings sampling is also assessed. \par
Finally, this section ends with a short retrospective on the choices made during the development of this software.

\subsection{Grid}\label{sec:2:Implementation:Grid}
In \hotspice, an ASI is represented on a tiled \xlabel[tosection]{rectilinear grid} of \xlabel[tosection]{unit cell}s --- where the grid spacing need not be uniform --- with magnets positioned at selected grid points.
This construction is illustrated in~\cref{fig:2:Unitcells} for a small in-plane kagome ASI.

\vspace{-1em}
\sidefig{2_Hotspice/Unitcells.pdf}{
	\label{fig:2:Unitcells}
	Implementation of kagome ASI on a \xref{rectilinear grid}, as indicated by dotted lines.
	Unit cells are delineated by solid lines.
	For kagome ASI, a uniform grid suffices, though not all grid points are occupied and a different spacing along the $x$- and $y$-directions is required.
	Colors relate equivalent magnets --- while a smaller unit cell with 3 magnets can be identified, such a unit cell would not be rectangular.
	%\newline % Newlines center the caption w.r.t. plot
}

This choice was made based on the trade-off between calculation efficiency and the freedom to place magnets anywhere.
We opted to prioritise efficiency and accept the geometrical restriction, as most ASI research focuses on periodic lattices.
All quantities are therefore stored in 2D matrices of size $N_x \times N_y$, upon which operations can be performed efficiently.
We will denote these matrices with large bold upright symbols: e.g., $\vc{S}$ contains the states $s_i$, $\vc{E_\mathrm{MS}}$ the magnetostatic energies $E_{\mathrm{MS},i}$, $\vc{\bigmu}$ the magnitudes $\mu_i$ of magnetic moments... \par
Despite the seemingly restrictive nature of the rectilinear grid,~\cref{fig:2:ASIs} illustrates its versatility in forming various periodic lattices, with only the Cairo lattices requiring grid non-uniformity.
As a bonus, \link{graphical user interface}{real-time visualisation} is simple and efficient using this approach, as the underlying matrix can directly be cast to a pixel image. \\\par
By leveraging the unit cell concept in periodic lattices and the efficient indexation of a rectilinear grid in computer memory, several aspects of the calculation can be performed more efficiently than for free-form ASI.
%The unit cell of each lattice in~\cref{fig:2:ASIs} is depicted as a grey rectangle.
Although non-rectilinear unit cells with fewer magnets can be identified for some lattices, such unit cells would increase complexity without significant benefit: the amount of unoccupied sites in a unit cell only increases the required memory, with little impact on the performance of the simulation.

\subsection{Kernels for magnetostatic interaction}\label{sec:2:Kernels}
Pre-calculated ``kernels'' are used to efficiently update the \xref{magnetostatic interaction} after each switch. 
While these kernels have to be calculated before the simulation starts, and therefore increase the initialisation time, the reduced runtime when simulating \link{sec:2:Dynamics}{dynamics} more than makes up for this.

\subsubsection{Structure and usage of unit cell kernels}\label{sec:2:Kernels:Structure}
For each magnet $i$, a kernel $\vc{k}^{(i)}$ stores the magnitude of the magnetostatic interaction between itself and all other magnets.
By calculating these values beforehand, when the ASI is created, the magnetostatic interaction energy between magnets $i$ and $j$ can readily be calculated as
\begin{equation}
	\label{eq:2:Kernel_k}
	E_{\mathrm{MS},i,j}= s_i s_j \vc{k}^{(i)}_j \mathrm{,}
\end{equation}
which can only change sign, since the states $s_i = \pm 1$ and $s_j = \pm 1$ are the only variables in the system. \par
Due to the fact that magnets are placed on a \xref{rectilinear grid}, the kernel corresponding to a magnet $i$ can also be written as an $N_x \times N_y$ matrix $\vc{K}^{(i)}_{ab}$ where $ab$ denotes a position on the ASI grid.
For example, $\vc{K}^{(i)}_{2,3}$ stores the magnetostatic interaction strength between magnet $i$ and the magnet at index\footnote{
	The indexation used throughout this section starts counting at 1, as is typical for matrix notation.
	This is not to be confused with indexation in source code, which starts at 0.
} $(2,3)$ on the grid.
If no magnet was placed at index $(a,b)$ on the grid, then $\vc{K}^{(i)}_{ab} = 0, \forall i$. \\\par

However, storing such a kernel for all magnets $i$ would require storing $\order{N^2}$ values in memory. % Furthermmore, many values would be zero, though the same (but less extreme) can be said of the unit cell kernel. Note: small \vc{k} has no zeros if a lookup table is used to assign an index to each magnet.
By leveraging \link{unit cell}{unit cells}, this storage requirement can be reduced to $\order{N}$.
Each occupied grid point in the unit cell is assigned a unique index $\kappa = 1,\dots,\widetilde{N}$, with $\widetilde{N}$ the number of magnets in a single unit cell.
Hence, each magnet in the ASI is associated with a specific value of $\kappa$. \par
This way, all magnets with the same value of $\kappa$ share the same layout of surrounding magnets\footnote{
	In some ASIs, it is possible that different sites inside a single unit cell also experience the same surrounding layout.
	This can be used to further improve memory usage (but not performance) by only storing unique kernels.
	This was not implemented as this constitutes only a minor improvement; for the lattices in~\cref{fig:2:ASIs}, this would at best half the number of kernels.
}, apart from a different cut-off at the border in case of open boundary conditions.
Therefore, if two magnets occupy equivalent positions $\kappa$ in the unit cell --- say, $i$ at index $(x,y)$ on the grid and $j$ at $(x+\Delta x, y+\Delta y)$ --- then
\begin{equation}
	\label{eq:2:Kernel_equivalence}
	\vc{K}^{(i)}_{ab} = \vc{K}^{(j)}_{a+\Delta x,b+\Delta y} \quad \mathrm{,} \quad \forall a,b:
	\begin{cases}
		-\Delta x < a \leq N_x - \Delta x \\
		-\Delta y < b \leq N_y - \Delta y \\
		% Or more rigorously, but even more overcomplicated:
		% \max(-\Delta x, 0) \leq a < N_x - \min(\Delta x, 0) \\
		% \max(-\Delta y, 0) \leq b < N_y - \min(\Delta y, 0) \\
	\end{cases} \quad\mathrm{.}
\end{equation}
In other words, their kernels are identical apart from an offset by $(\Delta x, \Delta y)$, at least in the area that remains inside the kernel after this offset.
Due to this equivalence, and the fact that $- N_x < \Delta x < N_x$ and $- N_y < \Delta y < N_y$, all possible interactions that a magnet at site $\kappa$ can experience can be stored in a single $(2N_x-1) \times (2N_y-1)$ matrix $\vc{\mathcal{K}}^{(\kappa)}_{ab}$, a \idx{unit cell kernel}.

\xfig[1.0]{2_Hotspice/Kernel_IP_Pinwheel.pdf}{
	\label{fig:2:Kernel_IP_Pinwheel}
	Two examples showing the kernels $\vc{\mathcal{K}}^{(\kappa)}$ shifting with respect to the ASI grid for element-wise multiplication when a magnet switches.
	\textbf{Left}: the ASI itself, with thin dotted lines showing individual cells in the \link{rectilinear grid}{simulation grid} while thick solid lines delineate unit cell boundaries.
	Letters indicate the index of a magnet within its unit cell.
	\textbf{Right}: each site in the unit cell corresponds to a kernel.
	The centre of a kernel is indicated by the shaded row and column.
	The values stored in the kernel are designed such that shifting the centre of the kernel onto the switching magnet and multiplying overlapping cells ($s_{ab}$ of the ASI with $\vc{\mathcal{K}}_{ab}$ of the kernel) yields the \xref{magnetostatic interaction} energy between the switching magnet and all other magnets at positions $ab$.
	The figure shows this for two example magnets indicated in \textcolor{lightblue}{blue} and \textcolor{lightred}{red}.
}

\cref{fig:2:Kernel_IP_Pinwheel} shows the structure of such unit cell kernels $\vc{\mathcal{K}}$ to clarify the interpretation of how they store the magnetostatic interaction strength between a particular magnet and any other magnet in the system.
This is most easily illustrated by an example.
Consider the blue magnet in the $5 \times 5$ pinwheel lattice on the left side of the figure, which occupies position $\kappa=\mathrm{A}$ in the unit cell.
The corresponding $9 \times 9$ kernel $\vc{\mathcal{K}}^\mathrm{(A)}$ is shown schematically, also in blue.
It stores the magnetostatic interactions in such a way that, when the ASI grid is shifted onto the kernel to put the blue magnet at the centre of the kernel, the interaction strength of the blue magnet with any other magnet in the ASI is stored in the kernel elements where those other magnets end up.
So, in the figure, these are the elements in kernel A shown to contain a grey magnet\footnote{
	The exact values stored at these elements are omitted as they are irrelevant to illustrate the concept. See \cref{fig:2:Kernel_PBC} for an example of the particular values stored in kernel A for a larger $16 \times 16$ lattice.
}. \par
The concept is illustrated a second time, now for the red magnet in the pinwheel ASI, which instead occupies position $\kappa=\mathrm{B}$ in the \xref{unit cell} and therefore uses a different kernel; the red kernel $\vc{\mathcal{K}}^\mathrm{(B)}$.
Note that this structure easily allows a cut-off radius for the magnetostatic interaction to be imposed by setting the relevant kernel values to zero. \\\par

This construction can be used to efficiently update the \link{magnetostatic interaction}{magnetostatic energy} $E_\mathrm{MS}$ of all magnets whenever a magnet switches.
\link{rectilinear grid}{Recall} that the grid stores the state $s_i$ of all magnets in a matrix $\vc{S}$ and the size of their magnetic moment $\mu_i$ in $\vc{\bigmu}$.
When a magnet at unit cell index $\kappa$ switches, the change of magnetostatic energy for all other magnets can be calculated by shifting $\vc{S}$ in the same way as the ASI was shifted in \cref{fig:2:Kernel_IP_Pinwheel}, and performing a pointwise multiplication --- the \idx{Hadamard product}, denoted by $\odot$ --- with the respective elements of the kernel $\vc{\mathcal{K}}^{(\kappa)}$.
Denoting the grid index of the switching $\kappa$-site magnet as $(x,y)$ and using the states $\vc{S}$ after the switch, this yields a new matrix
\begin{equation}
	\label{eq:2:Kernel_update}
	\Delta \vc{E_\mathrm{MS}} = 2\mu_{xy} s_{xy}\vc{\bigmu} \odot \vc{S} \odot \ab(\vc{\mathcal{K}}^{(\kappa)}_{ab})_{\substack{N_x < a + x \leq 2N_x \\ N_y < a + y \leq 2N_y}} \quad\mathrm{,}
\end{equation}
containing the change in magnetostatic energy $\Delta E_\mathrm{MS}$ for each grid point, which can then simply be added to the current values of $E_\mathrm{MS}$. \newpage
The kernel is also used to initialise the magnetostatic energy $\left. E_\mathrm{MS} \right|_{t=0}$ of each magnet at the start of the simulation.
This is done by performing a convolution (denoted by $*$) rather than a pointwise multiplication:
\begin{equation}
	\label{eq:2:Kernel_init}
	\left. \vc{E_\mathrm{MS}} \right|_{t=0} = \sum_\kappa \vc{I_\kappa} \odot \vc{\bigmu} \odot \vc{S} \odot \ab((\vc{\bigmu} \odot \vc{S}) * \vc{\mathcal{K}}^{(\kappa)}) \mathrm{,}
\end{equation}
where only the central $N_x \times N_y$ area of the convolution is calculated and the matrix $\vc{I_\kappa}$ contains 1 at sites with unit cell index $\kappa$, otherwise 0.
Such a convolution can be calculated very efficiently using the \xlabel{fast Fourier transform}. \\\par

The exact values stored in the kernel depend on whether any \link{sec:2:finite}{finite-size corrections} to the magnetostatic interaction were used and whether the ASI is of the IP or OOP type.
For example, an IP kernel using the \link{point dipole model}{point dipole approximation} contains elements
\begin{equation}
	\label{eq:2:Kernel_detailed}
	\vc{\mathcal{K}}^{(\kappa)}_{ab} = \frac{\mu_0}{4 \pi} \frac{
		\begin{multlined}
			u_x^{(\kappa)} u_x^{(ab)} \ab(1 - 3 \ab(\Delta x)^2) + u_y^{(\kappa)} u_y^{(ab)} \ab(1 - 3 \ab(\Delta y)^2) - {}\\
			3\Delta x \Delta y \ab(u_x^{(\kappa)} u_y^{(ab)} + u_x^{(ab)} u_y^{(\kappa)})
		\end{multlined}
	}{
		\sqrt{\ab(\ab(\Delta x)^2 + \ab(\Delta y)^2)^5}
	} \mathrm{,}
\end{equation}
with $\Delta x$ and $\Delta y$ the x- and y-distance between the central magnet $(\kappa)$ and a magnet at position $(ab)$ in the kernel (if a magnet exists at the corresponding point in the ASI grid) and $\vc{u}^{(i)}$ a unit vector along the \xref{easy axis} of magnet $i$. \par
With this form for the kernel, the \xref{magnetostatic interaction} between a magnet $i$ at grid-index $(v,w)$ and another magnet $j$ at index $(v+x, w+y)$ can be calculated as
\begin{equation}
	\label{eq:2:Kernel_unitcell}
	E_{\mathrm{MS},i,j} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(\kappa_i)}_{N_x+x, N_y+y} = (s_i \mu_i) (s_j \mu_j) \vc{\mathcal{K}}^{(\kappa_j)}_{N_x-x, N_y-y} \quad \mathrm{,}
\end{equation}
if the indexation of $\vc{\mathcal{K}}^{(\kappa)}$ starts at $(1,1)$.
Note that $\mu_i$ and $\mu_j$ were not included in the \xref{unit cell kernel} to allow magnets in different unit cells but with the same unit cell index $\kappa$ to have a different magnetic moment $\mu$.
This versatility comes at the cost of two additional element-wise multiplications per interaction, which has a non-negligible performance impact as~\cref{eq:2:Kernel_unitcell} is by far the most common operation performed during the simulation of system dynamics. \\\par

\subsubsection{Periodic boundary conditions}\label{sec:2:Kernels:PBC}
The \xref{rectilinear grid} enables the straightforward implementation of first-order \idx[tosection]{periodic boundary conditions} (PBC), which account for the eight nearest replicas of the ASI --- two horizontally, two vertically and four diagonally.
An open-boundary kernel $\vc{\mathcal{K}}^{(\kappa)}$ can be transformed into a PBC kernel by just adding eight offset copies of the kernel to itself.
Specifically, four copies are shifted along the axes by $\pm N_x$ or $\pm N_y$, while the remaining four are shifted diagonally by $(\pm N_x, \pm N_y)$.
This is illustrated in~\cref{fig:2:Kernel_PBC} for an example of pinwheel ASI. \par
This construction works because elements near the edge of a kernel represent interactions between distant magnets on opposite sides of the ASI, which are the interactions most affected by PBC.
Since the middle region of the open-boundary \link{unit cell kernel}{kernel} already stores the interaction between nearby magnets, PBC can be applied by re-using these central values and moving the kernel by $\pm N_x$ and/or $\pm N_y$.

\vspace{-1.5em}
\xfig[1.0]{2_Hotspice/Kernel_PBC.pdf}{
	\label{fig:2:Kernel_PBC}
	\indexlabel{open boundary conditions}
	Comparison of the \xref{magnetostatic interaction} \link{unit cell kernel}{kernel} $\vc{\mathcal{K}}^\mathrm{(A)}$ for open (left) and periodic (right) boundary conditions, for a $16 \times 16$ pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}) using the \link{point dipole model}{point dipole approximation}.
	For this ASI size, the 8 copies for PBC are offset by $\pm 16$ grid cells along each axis, resulting in the 8 repetitions visible in the right panel. 
	The magnet of the \xref{unit cell} site associated to this particular kernel is shown in the centre with a black outline --- its 8 copies lay just outside the kernel.
	The colour of magnets indicates the magnitude of the magnetostatic interaction between the central and the coloured magnets if they both point `up'.
	This alignment is preferable for \textcolor{blue}{blue} magnets (low energy) and unstable for \textcolor{red}{red} magnets (high energy).
}
\vspace{-1em}

This approach has the advantage that PBC do not impact performance, as they are baked into the kernel, contrary to the alternative of using a \xlabel[nolabel]{circular convolution}.
However, this method is limited to ASI consisting of an integer number of \xref{unit cells} as systems with truncated unit cells along an edge can not properly be tiled.
Higher-order PBC --- beyond only the 8 nearest copies --- can not be calculated based on an open-boundary kernel since it does not include interactions with such distant magnets.
Due to the rapid $1/r^3$ decline of the \xref{magnetostatic interaction} over distance, the use for such higher-order PBC would be very limited for 2D systems, and they were therefore not implemented.
% For very small systems, higher-order PBC can be relevant, but it is recommended to instead increase the system size to increase the fidelity of the Monte Carlo simulation.
% To explain the usage of the magnetostatic kernel, we can possibly refer to ``Real-space observation of emergent magnetic monopoles and associated Dirac strings in artificial kagome spin ice'', which notes that the energy is just a convolution of the interaction potential with the lattice sites at which there are magnets (which, for the Ewald summation, means that in Fourier space it is just a multiplication of the Fourier-transformed versions of those functions), which is pretty much what we are doing during multi-switching.

\subsubsection{Hard-axis magnetostatic kernel}\label{sec:2:Kernels:Perp}
If the simulation accounts for the \xref{asymmetric energy barrier}, as detailed in~\cref{sec:2:E_B_asymm}, then two additional kernels are needed for each site in the unit cell. \par
The first is calculated for a situation where the central magnet maintains its usual orientation while all other magnets are rotated counter-clockwise by \ang{90}.
This kernel will be used whenever a magnet switches, to update the \link{hard-axis interaction energy}{hard-axis magnetostatic energy} $\left. E_{\mathrm{MS}} \right|_{\perp}$ of all other magnets.
The underlying equation is very similar to \cref{eq:2:Kernel_detailed}, but with $u_x^{(\kappa)} \rightarrow -u_y^{(\kappa)}$ and $u_y^{(\kappa)} \rightarrow u_x^{(\kappa)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(\kappa)}$ in \cref{eq:2:Kernel_update}. \par
The second additional kernel is the opposite of the first one; the central magnet is rotated \ang{90} counter-clockwise while all other magnets maintain their usual orientation.
This kernel is needed only once, to calculate the initial value of $\left. E_{\mathrm{MS}} \right|_{\perp}$ for all magnets.
The underlying equation for this one is also very similar to \cref{eq:2:Kernel_detailed}, but now with $u_x^{(ab)} \rightarrow -u_y^{(ab)}$ and $u_y^{(ab)} \rightarrow u_x^{(ab)}$ substituted.
It is used in the same way as the kernel $\vc{\mathcal{K}}^{(\kappa)}$ in \cref{eq:2:Kernel_init}.

\subsubsection{Numerical error with truncated kernel}\label{sec:2:Kernels:Error}
Since the kernel is nearly 4 times larger than the simulation domain, a straightforward performance improvement presents itself: \indexlabel[tosection]{truncated kernel}{truncating} the \xref{magnetostatic interaction} at a certain distance.
Due to the underlying grid, the most natural way of achieving this is to reduce the size of the kernel from $(2N_x-1) \times (2N_y-1)$ to a smaller $(2\widetilde{R}-1) \times (2\widetilde{R}-1)$ central region, with $1 < \widetilde{R} < \min(N_x,N_y)$.
Note that the kernel size has to remain odd for the convolution to still place the switching magnet in the centre. \par
However, such truncation will result in inaccurate calculation of the interaction energies: whenever a magnet switches, $E_\mathrm{MS}$ of distant magnets will not be updated.
As an increasing number of magnets switch, this error will accumulate, as shown in \cref{fig:2:Kernel_cutoff} for the particular case of a pinwheel ASI where the original $199 \times 199$ kernel was truncated to a size of $41 \times 41$.
After a certain amount of switches, however, the error is observed to stagnate.
The reason for this is that, whenever a magnet switches back, the error introduced by its original switch gets cancelled, leaving only some residual \xlabel{floating-point error} in its wake.
This also explains why the plateau starts after $\approx N$ switches ($N=5000$ magnets in the figure): at that point, each magnet will have switched once on average and will begin randomly switching back.

\xfig[1.0]{2_Hotspice/Kernel_cutoff.pdf}{
	\label{fig:2:Kernel_cutoff}
	Accumulation of error in the \link{magnetostatic interaction}{magnetostatic energy} $E_\mathrm{MS}$ for a simulation of $100 \times 100$ pinwheel ASI, when using a truncated $41 \times 41$ kernel instead of the usual $199 \times 199$ kernel.
	The ASI starts in the uniform state and evolves to a random state due to high temperature.
	\textbf{(a)} Mean and maximum error of the magnetostatic energy $E_\mathrm{MS}$ throughout the ASI, as a function of the number of switches after initialising the system in the uniform state.
	\textbf{(b)} Final situation at the right edge of panel (a), i.e., after $\approx \SI{5e6}{}$ random switches.
	The absolute error $\abs{E_\mathrm{err}}$ is the absolute difference between the simulations with (``Approximation'') and without (``Exact $E_\mathrm{MS}$'') a truncated kernel.
}

The error is highly dependent on the situation and size of the truncated kernel.
The closer to equilibrium the system is initialised, the smaller the error will be while the simulation explores this equilibrium.
For example, for the particular situation in~\cref{fig:2:Kernel_cutoff} --- a pinwheel ASI at high temperature starting in the uniform state --- the absolute error $\abs{E_\mathrm{err}}$ for a $41 \times 41$ truncated kernel is on average a little over \SI{5}{\percent} of the mean exact magnetostatic energy $\llangle \abs{E_\mathrm{MS}} \rrangle$ across the ASI (blue curve).
For some magnets, though, the relative error exceeds \SI{15}{\percent} (red curve). \\\par

Note that PBC require extra attention when the magnetostatic kernel is truncated, as this procedure cuts off the eight offset kernel images.
Previously, \cref{eq:2:Kernel_update} was applied when a magnet switches, where the kernel was cut-out appropriately to the size of the ASI to allow a \link{Hadamard product}{pointwise multiplication}.
With open boundaries, the truncated kernel presents no issue, but with PBC the cut-out should be applied with care, such that it properly wraps around the edges.
This wrapping can instead also be achieved by performing a \xlabel{circular convolution} of the truncated kernel with an $N_x \times N_y$ array whose only non-zero element is at the position of the switching magnet.

\subsection{Multi-switching in Metropolis-Hastings\indexlabel{multi-switching}}\label{sec:2:MultiSwitch}
The standard \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm} selects a magnet at random and subsequently decides whether or not to switch it.
Since this algorithm is designed primarily for sampling equilibrium states rather than capturing the temporal evolution of the system, a straightforward performance improvement can be achieved by selecting multiple magnets simultaneously rather than sequentially.
This allows for better usage of the parallel processing capabilities of the GPU, as the \link{magnetostatic interaction}{magnetostatic energy} can then be updated using a convolution. \\\par
Previously, when only a single magnet switched at any instant, the magnetostatic energy was updated by \cref{eq:2:Kernel_update}, which appropriately cut out a piece of the kernel for pointwise multiplication.
This process is no longer usable if we intend to use multi-switching as a method to efficiently parallelise Metropolis-Hastings iterations: not much performance can be gained if every switching magnet requires a different cut-out.
Instead, a far more efficient method to calculate the change in magnetostatic energy whenever multiple magnets switch is to perform a convolution of the kernel with an $N_x \times N_y$ array whose only non-zero elements are the values $s_i$ at the positions of the switching magnets. \par
For a single switch, convolution with a truncated kernel is far slower than the original \link{Hadamard product}{pointwise multiplication} of \eqref{eq:2:Kernel_update}.
In fact, with the hardware used for this thesis, only for $\gtrsim 20$ \link{multi-switching}{simultaneous switches} does a convolution of \xref{truncated kernels} become more performant than a sequential sum of offset non-truncated kernels.
Therefore, \hotspice allows the user to fine-tune both the size of the truncated kernel as well as this threshold between sum and convolution to optimise performance on a particular system.

\subsubsection{Minimal distance between sampled magnets}\label{sec:2:MultiSwitch:rmin} % Derive equation
Since all magnets affect each other through the magnetostatic interaction, simultaneously switching nearby magnets that significantly affect each other's \xref{switching energy} may result in unphysical behaviour.
Take for example the extreme case where it would be allowed to switch all magnets in a system at once: the total energy would not change after such a simultaneous step and an infinite loop results where the energy of the system remains high.
In a less extreme case, convergence to a low-energy or equilibrium state may be slowed down significantly.
Therefore, to avoid such issues, we enforce a minimum distance $\rmin$ between selected magnets. \par
The particular criterion we will use is as follows: two simultaneously sampled magnets should never be able to affect each other's \xref{acceptance probability} by more than some user-adjustable factor $0 < Q \leq 1$, where we commonly use $Q=0.01$.
%This principle is illustrated schematically in~\cref{fig:2:MultiSwitch_proof}.
We will now derive an inequality for the minimal distance $\rmin$ as a function of $Q$.
\newtheorem{inequality}{Inequality}
\begin{inequality}
	The minimal distance $\rmin$ between magnets 1 and 2, such that the \xref{acceptance probability} $P(\Delta E)$ of one of them can not change by more than $0 < Q \leq 1$ when the other switches, is upper bounded by
	\begin{equation}
		\label{eq:2:MultiSwitch_inequality}
		r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \, \max(\mu^2)}{\pi Q} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}}} \triangleq \rmin \mathrm{.}
	\end{equation}
\end{inequality}

%\xfig[1.0]{2_Hotspice/MultiSwitch_proof.pdf}{
%	\label{fig:2:MultiSwitch_proof}
%	Consider two states: the initial state `i' (top), and the final state `f' where magnet 1 has switched (bottom).
%	The surrounding magnets stay unchanged.
%	In both states, magnet 2 has a certain \xref{acceptance probability} for switching $P_2$ if it is selected by the \link{alg:2:MetropolisHastingsSingle}{Metropolis-Hastings algorithm}, determined by the associated energy change $\Delta E$.
%	This switch of magnet 1 induces a change in acceptance probability $\Delta P_2$ of magnet 2.
%	The limitation that we impose, is that the Metropolis-Hastings algorithm is only allowed to simultaneously sample magnet 1 and 2 if $\abs{\Delta P_2} = \abs{P_2^{(f)} - P_2^{(i)}} \leq Q$.
%}

\begin{proof}
	%Consider~\cref{fig:2:MultiSwitch_proof}.
	Consider the initial state $(i)$ and the final state $(f)$ where magnet 1 has switched.
	At first, magnet 2 has an acceptance probability $P_2^{(i)}$ due to its possible change in energy $\Delta E^{(i)}$.
	After magnet 1 has switched, magnet 2 has an acceptance probability $P_2^{(f)} = P_2^{(i)} + \Delta P_2$ where a switch of magnet 2 would incur a change of $\Delta E^{(f)}$ in energy.
	Our requirement is that $\abs{\Delta P_2} \leq Q$, with $0 < Q \leq 1$.
	We therefore want to determine the minimal distance between magnets 1 and 2 such that $\abs{\Delta P_2} \leq Q$. \par
	For any function $P(\Delta E)$, we can move this constraint from $\Delta P_2$ to the more manageable quantity $\Delta E^{(f)} - \Delta E^{(i)}$ by writing the upper bound
	\begin{equation}
		\label{eq:2:MultiSwitch_proof_leq_Q}
		\abs{\Delta P} \leq \abs{\Delta E^{(f)} - \Delta E^{(i)}} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}} \leq Q \mathrm{.}
	\end{equation}
	The only long-range interaction present in the systems under consideration here is the \xref{magnetostatic interaction} given by \eqref{eq:2:E_MS}, repeated here for convenience,
	\begin{equation*}
		E_{\mathrm{MS},1,2} = \frac{\mu_0}{4 \pi} \ab(\frac{\vc{\mu}_1 \bcdot \vc{\mu}_2}{\norm{\vc{r}_{12}}^3} - \frac{3(\vc{\mu}_1 \bcdot \vc{r}_{12}) (\vc{\mu}_2 \bcdot \vc{r}_{12})}{\norm{\vc{r}_{12}}^5}) \mathrm{.}  \tag{\ref*{eq:2:E_MS}}
	\end{equation*}
	We do not consider any \link{sec:2:finite}{finite-size corrections}, since their long-range effects are negligible and any reasonable value of $Q$ will result in a value $\rmin$ far larger than the magnet size.
	The magnetostatic energy in the \link{point dipole model}{point-dipole approximation} is bounded by
	\begin{equation}
		\label{eq:2:MultiSwitch_proof_EMS_leq}
		\abs{E_{\mathrm{MS},1,2}} \leq \frac{\mu_0 \norm{\vc{\mu}_1} \norm{\vc{\mu}_2}}{2 \pi r_{12}^3} \mathrm{,}
	\end{equation}
	with this extremum being achieved for two dipoles $\vc{\mu}_1$ and $\vc{\mu}_2$ aligned in the same direction along their common axis $\vc{r}_{12}$.
	From the symmetry of the magnetostatic interaction, % and considering the states in~\cref{fig:2:MultiSwitch_proof}, 
	the unknown term in~\cref{eq:2:MultiSwitch_proof_leq_Q} can be expanded as 
	\begin{align*}
		\abs{\Delta E^{(f)} - \Delta E^{(i)}} &= \abs{-\cancel{2 \sum_{k=3}^N E_{\mathrm{MS},2,k}^{(f)}} -2 E_{\mathrm{MS},2,1}^{(f)} +\cancel{2 \sum_{k=3}^N E_{\mathrm{MS},2,k}^{(i)}} +2 E_{\mathrm{MS},2,1}^{(i)}} \\
		&= 2\abs{E_{\mathrm{MS},2,1}^{(i)} - E_{\mathrm{MS},2,1}^{(f)}} = 4 \abs{E_{\mathrm{MS},2,1}^{(i)}} \numeq{\ref{eq:2:MultiSwitch_proof_EMS_leq}}{\leq} \frac{2 \mu_0 \norm{\vc{\mu}_1} \norm{\vc{\mu}_2}}{\pi r_{12}^3} \mathrm{,}
	\end{align*}
	since only magnet 1 is different between the initial and final states, hence $E_{\mathrm{MS},2,1}^{(f)} = - E_{\mathrm{MS},2,1}^{(i)}$.
	Combining this with \eqref{eq:2:MultiSwitch_proof_leq_Q} gives the sufficient condition
	\begin{equation}
		\frac{2 \mu_0 \norm{\vc{\mu}_1} \norm{\vc{\mu}_2}}{\pi r_{12}^3} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}} \leq Q \mathrm{.}
	\end{equation}
	To get a single value for $\rmin$ that is valid throughout the ASI, replacing $\norm{\vc{\mu}_1} \norm{\vc{\mu}_2}$ by $\max(\mu^2)$ finally yields the expected expression for $\rmin$:
	\begin{equation*}
		r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \, \max(\mu^2)}{\pi Q} \cdot \, \underset{\Delta E}{\max} \abs{\frac{\partial P}{\partial \Delta E}}} \triangleq \rmin \mathrm{.} \tag{\ref*{eq:2:MultiSwitch_inequality}}
	\end{equation*}
\end{proof}
This equation is an upper bound for $\rmin$ throughout the entire ASI.
To apply this inequality in \hotspice, all that remains is to plug in the derivative of the \link{Metropolis-Hastings sampling}{Metropolis-Hastings} \xref{acceptance probability} function $P_\mathrm{MH}(\Delta E, T) = \min(1, \exp(-\Delta E/k_B T))$, into~\cref{eq:2:MultiSwitch_inequality}.
Since
\begin{equation}
	\abs{\frac{\partial P_\mathrm{MH}}{\partial \Delta E}} = \left.\begin{cases}
		\frac{1}{k_B T} \exp(\frac{-\Delta E}{k_B T}) \quad &\mathrm{if} \quad \Delta E > 0 \\
		0 \quad &\mathrm{if} \quad \Delta E < 0
	\end{cases} \right\}
	\leq \frac{1}{k_B T} \mathrm{,}
\end{equation}
this finally gives
\begin{equation}
	r_{12} \geq \sqrt[3]{\frac{2 \mu_0 \max(\mu^2)}{\pi Q k_B T}} = \rmin \mathrm{.}
\end{equation}
\\\par
Recall that we are using the Metropolis-Hastings acceptance probability in \hotspice's rejection-based \xref{kinetic Monte Carlo} algorithm (\cref{sec:2:Dynamics_MH}) rather than the \xref{Glauber dynamics} acceptance probability $P_\mathrm{GD}(\Delta E, T) = \frac{\exp(-\Delta E/k_B T)}{1+\exp(-\Delta E/k_B T)}$, because they both satisfy \xref{ergodicity} and \xref{detailed balance} but the Glauber acceptance probability is always more likely to reject a switch~\cite{bit-player_MCvsGlauber}.
However, we can now see that the Glauber acceptance probability is not all bad when \xref{multi-switching} is used, as the extremum of its derivative is 4 times smaller than for the Metropolis-Hastings acceptance probability.
Hence, \cref{eq:2:MultiSwitch_inequality} tells us that the minimal distance between samples with Glauber dynamics can be a factor $\sqrt[3]{1/4} \approx 0.63$ smaller than with Metropolis-Hastings.
Consequently, $\sqrt[3]{16} \approx 2.52$ times more magnets can safely switch at once.
Therefore, Glauber dynamics can ultimately result in a greater number of switches overall, given that the Metropolis-Hastings acceptance probability is at most twice that of Glauber dynamics. Still, the optimal acceptance probability depends on the specifics of a simulation, as a large $\rmin$ will yield very few switches, which benefits Metropolis-Hastings. \par
One ingredient remains in order to perform Metropolis-Hastings multi-sampling: an efficient sampling algorithm that obeys this minimal distance requirement.
\hotspice provides three such algorithms: Poisson disc sampling, restricted stratified jittered grid sampling, and a hybrid approach between these two.
By default, the jittered grid algorithm is used to optimise performance.
Since a full discussion of such algorithms would lead us too far, these three selection algorithms are compared and discussed in more detail in~\cref{app:Selection_algorithms}.

\subsection{Performance}\label{sec:2:Implementation:Performance}
The performance of \hotspice has been progressively improved throughout its development.
In particular, the various types of magnetostatic kernels discussed in~\cref{sec:2:Kernels:Structure} were all designed to enhance efficiency and reduce memory usage.
Additionally, \hotspice was implemented in such a way that it can perform simulations on either a \xlabel{central processing unit} (CPU) or a \xlabel{graphics processing unit} (GPU).
Since the number of magnets $N$ in an ASI has a strong impact on performance, we will evaluate \hotspice's performance for a range of system sizes. \par
This section starts with a performance comparison of the different kernel implementations for both CPU- and GPU-based simulations.
Then, we assess the throughput of the Metropolis-Hastings algorithm with the multi-sampling procedure on both CPU and GPU, before concluding with a brief discussion on the random number generators used.

\paragraph{CPU and GPU}
Both these types of processor are typically present in modern computers and are designed for different tasks.
The CPU can execute diverse, sequential tasks in rapid succession, while the GPU is optimised for parallel computing and therefore excels at performing the same calculation across a large amount of data simultaneously~\cite{owens2008gpu}.
Therefore, the size of a simulated ASI often forms the determining factor as to whether calculation on GPU rather than CPU will result in a faster simulation~\cite{lee2010debunking}.
For larger systems, the parallel computing capabilities of GPUs can significantly improve performance over CPUs.
On the other hand, GPUs reach a bottleneck when the simulation takes on a more sequential nature or when the simulated system is small, as in such cases the overhead associated with GPU operations dominates~\cite{owens2008gpu}. \par
On CPU, \hotspice uses the popular \python{NumPy}~\cite{NumPy} and \python{SciPy}~\cite{SciPy} libraries.
To enable GPU-based calculations, we opted to use the \python{CuPy}~\cite{CuPy} library, for ease of implementation.
\python{CuPy} provides GPU-accelerated array manipulation using a similar API as \python{NumPy}, which makes it a drop-in replacement for most --- but not all --- array operations.
While a pure CUDA implementation could provide further performance optimisations, this has not been necessary at present as \python{CuPy} is sufficiently performant for the topics discussed in this thesis.

\paragraph{Hardware}\label{hardware}
The following benchmarks, like all other simulations throughout this thesis, were performed on an NVIDIA GeForce RTX 3080 Mobile GPU and/or an 11th Gen Intel\textregistered{} Core\texttrademark{} i7-11800H @ 2.30GHz (both released in 2021).
The reported simulation times serve an illustrative purpose: absolute values may vary significantly across different hardware, though the overall trends and relative performance should remain similar. \par

\subsubsection{Magnetostatic interaction and kernels}
The performance of the different magnetostatic kernel implementations is compared in Tables~\ref{tab:2:perf_init} and~\ref{tab:2:perf_switch}, whose rows are generally ordered by performance.
The first table shows the initialisation time, while the second table shows the time it takes to perform 5000 switches using the \xref{first-switch method}, taking into account the \xref{magnetostatic interaction} between all magnets.
During initialisation, the magnetostatic kernel is constructed, after which the starting energy $E_i$ of all magnets is calculated.
Only the last 3 rows use the \link{unit cell kernel}{magnetostatic kernel} $\vc{\mathcal{K}}^{(\kappa)}$ from~\cref{sec:2:Kernels:Structure}; the previous rows use the less efficient kernels considered earlier in that section.
CPU and GPU performance is compared at the start and end of the tables. \par
The first two rows \textit{``Pairwise kernel''} in the tables use the na\"ive kernels $\vc{k}^{(i)}$ (or, equivalently, $\vc{K}^{(i)}$) that were described at the start of~\cref{sec:2:Kernels:Structure} and used in \cref{eq:2:Kernel_k}.
These store the interactions between each magnet $i$ and all other magnets, resulting in many duplicated values.
All these kernels --- one for each magnet $i$ --- are arranged into a big $N \times N$ matrix $\vc{D}$ storing the \xref{magnetostatic interaction} between each pair of magnets.
This allows easy calculation of $E_\mathrm{MS}$ for each magnet by a single matrix product $\vc{D}\vc{s}$ with the column vector $\vc{s}=\{s_i\}$.
However, this is very inefficient: the initialisation time on CPU (first row in~\cref{tab:2:perf_init}) bears witness to the size of this matrix as $t_\mathrm{init} \propto L^4 = N^2$, making it impractical to simulate systems larger than $50 \times 50$ magnets.
On GPU, parallelisation of the calculation of this matrix $\vc{D}$ approximately reduces this to $\propto L^2 = N$. \par

% Q: should we use L or N=L²? I would prefer L because at some point we talk about 2D indexation, so L makes it clearer that we have a 2D system.
\vspace{-0.2em}
\xtable[tab:2:perf_init]{
	\textbf{Initialisation time} for various simulation sizes $L$ (i.e., $L \times L = N$ magnets).
	`Mem' indicates that the simulation required more memory than available on our benchmark system ($>\SI{8}{\giga\byte}$), while `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).
}{
	\begin{tabular}{r|c|c|c|c|c|c}
		\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
		\hline \hline
		CPU | \makecell{Pairwise kernel} & 0.5$\,$s & 6.8$\,$s & 34.0$\,$s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{Pairwise kernel} & 2.2$\,$s & 5.4$\,$s & 11.9$\,$s & Mem & Mem & Mem \\
		\hline
		GPU | \makecell{No kernel} & 1.0$\,$s & 3.4$\,$s & 7.6$\,$s & 14.5$\,$s & 91.6$\,$s & ? \\
		\hline
		GPU | \makecell{Unit cell kernel} & 2.4$\,$s & 4.6$\,$s & 9.7$\,$s & 16.3$\,$s & 66.2$\,$s & 730.4$\,$s \\ % (see \S\ref{sec:2:Kernels})
		\hline \hline
		GPU $\bigg|$ \makecell{Optimised\\\small{(convolution init.)}} & 1.0$\,$s & 1.0$\,$s & 1.0$\,$s & 1.0$\,$s & 1.6$\,$s & 23.9$\,$s \\
		\hline
		CPU $\bigg|$ \makecell{Optimised\\\small{(convolution init.)}} & 14$\,$ms & 0.2$\,$s & 1.0$\,$s & 3.2$\,$s & 53.1$\,$s & ? \\
		\hline
	\end{tabular}
}

\vspace{-2.5em}
\xtable[tab:2:perf_switch]{
\textbf{Time for 5000 switches} using the \xref{first-switch method}, for various simulation sizes $L$ (i.e., $L \times L = N$ magnets).
`Mem' indicates that the simulation required more memory than available on our benchmark system ($>\SI{8}{\giga\byte}$), while `?' indicates a prohibitively long simulation time ($\gtrsim \SI{1000}{\second}$).
}{
\begin{tabular}{r|c|c|c|c|c|c}
	\multicolumn{1}{r}{Simulation size $L=$} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{150} & \multicolumn{1}{c}{200} & \multicolumn{1}{c}{400} & 1000 \\
	\hline \hline
	CPU | \makecell{Pairwise kernel} & 9.4$\,$s & 217.6$\,$s & ? & Mem & Mem & Mem \\
	\hline
	GPU | \makecell{Pairwise kernel} & 5.0$\,$s & 13.6$\,$s & 51.8$\,$s & Mem & Mem & Mem \\
	\hline
	GPU | \makecell{No kernel} & 7.1$\,$s & 7.2$\,$s & 8.2$\,$s & 9.0$\,$s & 13.0$\,$s & ? \\
	\hline
	GPU | \makecell{Unit cell kernel} & 7.8$\,$s & 7.9$\,$s & 8.3$\,$s & 8.9$\,$s & 11.8$\,$s & 36.9$\,$s \\
	\hline \hline
	GPU $\bigg|$ \makecell{Optimised\\\small{(2D indexation)}} & 6.8$\,$s & 7.0$\,$s & 7.2$\,$s & 7.6$\,$s & 10.2$\,$s & 35.9$\,$s \\
	\hline
	CPU $\bigg|$ \makecell{Optimised\\\small{(2D indexation)}} & 0.56$\,$s & 1.3$\,$s & 2.5$\,$s & 4.3$\,$s & 41$\,$s & ? \\
	\hline
\end{tabular}
} % Other options besides "optimised implementation": "current implementation", "production version", "improved version"?
However, the limiting factor remains the excessive memory consumption of the unoptimised kernel, which unnecessarily stores $\order{N^2}$ elements. % For periodic lattices, the matrix will contain many identical values, wasting a lot of memory.
For instance, the pairwise kernel of a $170 \times 170$ system already exceeds the \SI{8}{\giga\byte} limit of our benchmark system, as indicated in the table by ``Mem''.
While adding memory is an option, a more effective solution is to reduce the kernel's memory usage.
Sparse matrices were considered, but not pursued as they introduce additional overhead that would not improve the rapidly declining performance per switch (\cref{tab:2:perf_switch}) for large $N$, both on CPU and GPU. % Limiting the interaction distance could improve this.
Clearly, a more efficient approach is needed. \\\par

Before considering the \xref{unit cell kernels} presented in~\cref{sec:2:Kernels:Structure}, we first consider an extreme solution to decrease memory usage: using no kernel at all, as done in the third row \textit{``No kernel''}.
Instead, for every switching magnet, its interaction energy with all other magnets is calculated from scratch.
The tables show that this clearly gets rid of the memory issue and yields a surprisingly fast simulation.
However, the initialisation for larger systems now takes a long time, because without a kernel this requires $\order{N^2}$ operations resulting in $t_\mathrm{init} \propto N=L^2$ on GPU for small systems.
Still, in all situations it is beneficial not to use the pairwise kernels $\vc{k}^{(i)}$. \\\par

To reduce the initialisation time, two improvements had to be combined.
First, a \xref{unit cell kernel} as described earlier in~\cref{sec:2:Kernels:Structure} had to be implemented, which requires far less memory by using the periodic nature of most ASIs to only store interactions for a single \xref{unit cell}.
This is used in the fourth row \textit{``Unit cell kernel''}, but does not yield a significant improvement on its own aside from enabling simulations of systems larger than $1000 \times 1000$ magnets.
Secondly, however, this new kernel enables the usage of a convolution to initialise the magnetostatic interaction energy $E_\mathrm{MS}$, rather than the sequential sum used before.
This is what the final two rows \textit{``Optimised implementation''} use, where we once again compare CPU and GPU as this is the most performant version overall.
Another minor optimisation was to use 2D indexation (rather than a flat index $i = N_x y + x$ which did not synergise well with the convolution), yielding another \SI{15}{\percent} performance increase. \newpage

In the end, the upper limit for feasible calculation time is $L \approx 1000$ on GPU and $L \approx 400$ on CPU, beyond which the calculation time rises sharply.
This is a vast improvement over the pairwise kernel, which could barely reach $L \gtrsim 100$.
Comparing the last two rows of \cref{tab:2:perf_switch} reveals that, as expected, CPU outperforms GPU for small systems using the \xref{first-switch method}.
This remains true independent of the kernel implementation, as simulating a single switch requires several distinct operations that do not lend themselves to parallelisation.
When operating on small arrays, relatively fewer parallel calculations can be performed, thereby increasing the importance of being able to perform distinct mathematical operations in quick succession, at which the CPU excels. \par
For the first-switch method, the cross-over between CPU and GPU only occurs at $L \gtrapprox 300$, which is far beyond what is required for the topics discussed in this thesis.
Performing multiple switches simultaneously in Metropolis-Hastings benefits greatly from the \xref{unit cell kernel}, as it can use the same principle of convolution as was already used in the initialisation.
Therefore, it can make greater use of parallel computation.
We will take a look at this next.

\subsubsection{Metropolis-Hastings with multi-switching}
The \xref{first-switch method}, used in the preceding discussion about performance, does not implement a \xref{multi-switching} procedure.
This makes it very inefficient for large systems: for a system of $N$ magnets to significantly change its macrostate, on the order of $N$ switches must occur.
Even though the time required to perform a single switch with the first-switch method stays roughly constant up to $L<400$, the time per \xref{Monte Carlo sweep} will scale linearly with the number of magnets if each iteration only switches a single magnet.
The ability to perform \link{multi-switching}{multiple switches} simultaneously during \indexlabel[nolabel]{Metropolis-Hastings sampling}\xref{Metropolis-Hastings sampling} alleviates this problem, allowing for a significantly higher amount of Monte Carlo sweeps per second. \par
\cref{fig:2:Performance} shows the performance of the multi-switching algorithm as a function of system size, both for calculation on the CPU (a) and GPU (b).

\vspace{-1em}
\xfigsnocap[0.47]{2_Hotspice/Performance_CPU.pdf}{2_Hotspice/Performance_GPU.pdf}{
	\label{fig:2:Performance}
	Performance of OOP square ASI as a function of system size, on \textbf{(a)} CPU and \textbf{(b)} GPU.
	The Metropolis-Hastings algorithm was used with \link{stratified jittered grid}{restricted SJG} \xref{multi-switching} using $Q=0.05$, lattice spacing $a = \SI{1}{\micro\metre}$, magnetic moment $\mu = \SI{0.16}{\femto\ampere\metre\squared}$ and temperature $T = \SI{100}{\kelvin}$.
}
\vspace{-1em}

The multi-switching algorithm selects a certain number of magnets per second, as depicted by the blue curve: these are candidates for switching.
Dividing this value by $N$, the number of magnets in the system, gives the number of \xref{Monte Carlo sweeps} per second (MCS/s), depicted by the red curve.
This is the main performance metric in Monte Carlo simulations. %\\\par
%Only a subset of the selected magnets will switch, as depicted by the orange curve.
%This switching rate is highly dependent on the specific conditions of the simulation --- particularly the ratio $\EBeff/T$ --- making general statements infeasible.
%There exist situations where no magnets switch or where any selected magnet switches.
%In the figure, values of $a$, $T$, and $\EB$ were chosen which result in a reasonable \xref{switching rate}. \\\par

There is once again a stark contrast between CPU and GPU.
On CPU, the sampling rate starts off rather constant for small systems, as overhead dominates.
For $N>100$ magnets, the sampling rate increases proportional to $N$, leading to a stable performance independent of $N$.
Beyond $\approx 80 \times 80$ magnets, however, performance drops dramatically. % While this is reminiscent of a lack of garbage collection while building the figure, this is not the case here as the drop remains even when starting the figure at $L=80$.
We hypothesise that this is due to exceeding a CPU cache size. \par
%On the CPU used for~\cref{fig:2:Performance} (the aforementioned i7-11800H), the L1 and L2 cache hold \SI{48}{\kilo\byte} and \SI{1.25}{\mega\byte}, respectively.
% Q: Are any of the below hypotheses reasonable? Which is most reasonable?
% Hypothesis 1: The size occupied by a 72x72 ASI is 1.25MB. It is not unreasonable that, soon after reaching this point, some arrays can no longer be stored on this cache level, impacting performance.
% Hypothesis 2: The size of mm.m at 78x78 is 48kB, so we might be exceeding the L1 cache beyond this point.
% Hypothesis 3: The size of the dipole kernel at this point is around 220kB. Doesn't seem to match up with anything.
% TODO: ? revisit this at some point, it is odd that on another CPU (Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz) the cut-off happens at the same system size, indicating that it is not cache-size-related because that CPU has 256kB L2 and 32kB L1, both different from the UGent laptop.
The GPU, on the other hand, hits its stride for exactly those systems that are too large for the CPU to handle.
Around $70 \times 70$, the sampling rate increases rapidly, resulting in an increased number of MCS per second with optimal performance achieved for $200 \times 200$ magnets.
Beyond this, the sampling rate stagnates, eventually settling at around \SI{150e3}{samples\per\second}, corresponding to an ever decreasing MCS/s as the parallelism of the GPU gets exhausted.
As such, the GPU and CPU are perfectly complimentary, with the GPU now maintaining the advantage for systems as small as $L \gtrapprox 80$.

\subsubsection{RNG}
Another factor which could affect performance is the choice of \xlabel[tosection]{random number generator} (RNG), since both update algorithms rely on generating random numbers to select the next magnet(s) to switch.
For the \xref{first-switch method}, each switch requires $\order{N}$ random numbers --- one random switching time for each magnet.
\xref{Metropolis-Hastings sampling}, on the other hand, only requires $\order{1}$ random numbers per switch --- randomly selecting one or multiple magnets and randomly switching them according to their \xref{acceptance probability}.
Both of these algorithms use exponentially distributed random numbers, which can easily be generated from a uniformly distributed random number $\chi$ by the transformation $-\ln{\chi}$. \par
On CPU, \hotspice uses\footnote{
	On CPU, \hotspice uses \python{numpy.random.default_rng}, which uses the PCG64 generator for \python{numpy}~\cite{NumPy} v1.17 and up.
} the PCG64~\cite{PCG64} generator.
For GPU calculations, NVIDIA's \code{cuRAND} library\footnote{
	On GPU, \hotspice uses \python{cupy.random.default_rng}, which itself uses the \code{cuRAND} library.
} provides several random number generators, of which the default XORWOW~\cite{XORWOW} generator is used.
The performance of these various types of GPU RNG depends on the amount of random values being generated in parallel (i.e., the size of the ASI) and the details of the GPU architecture used~\cite{RNG_GPU_evaluation}.
In \hotspice, no significant effect on performance has been noticed by changing the RNG method.

\subsection{Retrospective on the \hotspice implementation}
\subsubsection{Grid}
Throughout this section, we have seen that the use of a \xref{rectilinear grid} has significant implications on the efficiency, applicability and implementation of \hotspice.
One of the key advantages of this approach is that it enables fast and memory-efficient calculation of the \xref{magnetostatic interaction}, because all \xref{unit cells} can use \link{unit cell kernel}{common kernels}.
This eliminates the need to \link{truncated kernel}{truncate} interactions at a certain distance to prevent excessive memory usage --- though in many cases, the effect of considering interactions beyond a certain range may be negligible.
For comparison, the \texttt{flatspin} ASI simulator~\cite{flatspin}, which does not use an underlying grid, limits interactions to a certain radius without significantly affecting the overall dynamics.
In \hotspice, full-range interactions can be computed efficiently and performance of GPU simulations remains strong even for grid sizes up to $800 \times 800$. \par
While the grid structure is particularly well-suited for \link{fig:2:ASIs}{periodic ASI} that can be put on a regular lattice, simulating a few hundred randomly placed magnets is far less efficient but still possible.
As a result, highly irregular ASI structures, such as the evolutionary ASI developed by \textit{Penty and Tufte}~\cite{ASI_Evolutionary_ALife}, can not efficiently be simulated with this grid-based approach. \par
Moreover, \xref{multi-switching} in the \link{Metropolis-Hastings sampling}{Metropolis-Hastings scheme} benefits greatly from the grid structure, as it enables efficient \xref{selection algorithms} that ensure a minimal distance between samples.
In irregular ASI, maintaining sufficient spacing between selected magnets would be far more difficult and likely necessitate the use of an underlying `virtual' grid regardless.

\subsubsection{GPU/CPU}
In retrospect, porting \hotspice to the GPU was not necessary for the work published in this thesis, as the ASIs needed for the work presented in \cref{ch:Applications} are relatively small and are most often simulated using the \xref{first-switch method}.
Since rejection-free KMC does not support \xref{multi-switching}, the majority of computations in that chapter are performed on the CPU. \par
Nonetheless, GPU support has broadened the range of systems that \hotspice can efficiently simulate, in particular for the \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm}, making it a viable simulator to be used in other works beyond thesis.
%One remaining limitation is that the current implementation requires the user to choose between GPU and CPU before the \python{import hotspice} statement.
%This is a remnant from early development since GPU support was not planned from the start.
%Even though this rarely poses an issue, it is an unnecessary limitation that could be addressed in the future.

\subsubsection{Ensembles}
In many cases, simulations are repeated for statistical averaging or parameter variations.
With the current \hotspice implementation, these runs must be simulated separately.
By ``stacking'' simulations in memory --- extending the underlying 2D arrays to 3D --- such ensembles could be simulated far more efficiently.
This could particularly enhance GPU performance for small systems using the first-switch method~\cite{GillespieParallel}.
Although this would require extensive changes to the core of \hotspice, it can be a useful consideration when developing future software.

\newpage % For consistent formatting
\section{Verification}\label{sec:2:Verification}
A simulator is useless if it does not provide correct results.
Therefore, we must now verify the correct implementation of \hotspice.
To this end, we will simulate several systems of increasing complexity for which analytical solutions are available.
The examples discussed here are all equilibrium problems, so we will use \xref{Metropolis-Hastings sampling} to verify that it indeed samples the equilibrium state space.
Special attention will be given to the issue of ``\xref{critical slowing down}'' which plagues this algorithm near phase transitions.

\subsection{Magnetostatic interaction}
Before progressing to equilibrium problems, we briefly check that the \xref{magnetostatic interaction} and its underlying \link{unit cell kernel}{kernels} all work as expected.
The magnetostatic interaction energy of any given arrangement of nanomagnets can be calculated analytically and compared to \hotspice. \par
As an example that is neither too simple nor complex, we consider a regular hexagon in a vortex state.
The energy of all magnets should equal the following analytical solution, which is the sum of the magnetostatic interaction energy between nearest neighbours $\circled{1}$, next-nearest neighbours $\circled{2}$ and magnets on opposite sides of the hexagon $\circled{3}$:
\begin{align*}
	E_{\mathrm{MS},i} =&\, \circled{1} + 2 \times \circled{2} + 2 \times \circled{3} \\
	=&\, -\frac{\mu_0 \mu^2}{4\pi a^3} -2\frac{\mu_0 \mu^2}{4\pi \frac{3 \sqrt{3} a^3}{8}} \Big[3\cos^2(\pi/3) - \cos(2\pi/3)\Big] \\&-2\frac{\mu_0 \mu^2}{4\pi \frac{a^3}{8}} \Big[3\cos^2(\pi/6) - \cos(\pi/3)\Big] \\
	=&\, -\frac{\mu_0 \mu^2}{4\pi a^3} \bigg[29+\frac{20}{3\sqrt{3}}\bigg] \mathrm{.} \numberthis
\end{align*}
Indeed, \hotspice gives the expected total magnetostatic energy $E_{\mathrm{MS},i}$ for all 6 magnets $i$ in the system, as can be verified by running the \python{tests/test_hexagon.py} script provided alongside the source code.

\subsection{Non-interacting spin ensemble}
When an \xref{external magnetic field} of magnitude $B$ is applied to a non-interacting paramagnetic ensemble of Ising spins, the two stable states of each magnet are no longer degenerate.
Spins pointing toward the field will have an energy $E_1 = -\mu B$, while the opposite state has an energy $E_2 = \mu B$.
The probabilities $\pi_1$ and $\pi_2$ that a magnet occupies either state then follow directly from the canonical partition function $Z = \exp\ab(\frac{\mu B}{\kBT}) + \exp\ab(\frac{-\mu B}{\kBT})$ as $\pi_i = \frac{1}{Z} \exp\ab(\frac{-E_i}{\kBT})$.\indexlabel[nolabel]{Boltzmann factor}
Hence, at thermal equilibrium, the average magnetisation of such a paramagnetic system (in the direction of the magnetic field) follows the relation
\begin{equation}
	\frac{\langle M \rangle}{M_0} = \pi_1 - \pi_2 = \tanh\ab(\frac{\mu B}{\kBT}) \mathrm{,}
\end{equation}
with $M = \sum_i \mu_i s_i$ the total magnetic moment of the system and $M_0 = \sum_i \mu_i$.
\cref{fig:2:Noninteracting} demonstrates that \hotspice{} correctly reproduces the expected result.

\vspace{-1em}
\sidefig{2_Hotspice/Verification/Noninteracting_IP.pdf}{
	\label{fig:2:Noninteracting}
	Average magnetisation of a non-interacting ensemble of spins at thermal equilibrium, as a function of the \link{external magnetic field}{applied magnetic field} magnitude $B$.
	\newline % Newlines center the caption w.r.t. plot
}

\subsection{Exchange-coupled Ising system}\label{sec:2:Verification_OOP_Exchange}
The 2D square-lattice \link{exchange coupling}{exchange-coupled} Ising model is one of the few exactly solvable systems in statistical physics~\cite{ExactlySolvedModelsStatMech}.
An analytical solution is known for the temperature-dependence of its average magnetisation
\begin{equation}
	\label{eq:2:Verification_exchange_M}
	\frac{\langle M \rangle}{M_0} = \sqrt[8]{1 - \sinh^{-4}(2J/\kBT)} \mathrm{,}
\end{equation}
with $J$ the \xref{exchange coupling} constant~\cite{Correlations2DIsing,IsingSpontaneousMagnetization,coey2010magnetism}. \par
This implies that the system exhibits a second-order \xlabel{phase transition} at the critical temperature~\cite{ExactlySolvedModelsStatMech}
\begin{equation}
	T_\mathrm{c} = \frac{2J}{\kB \ln(1 + \sqrt{2})} \mathrm{.}
\end{equation}
Furthermore, an analytical solution is available for the nearest-neighbour correlation
\begin{equation}
	\label{eq:2:Verification_exchange_corr}
	\langle s_i s_{i+1} \rangle = 
	\begin{cases}
		\sqrt{1+\zeta} \ab[\frac{1-\zeta}{\pi}\mathrm{K}(\zeta) + \frac{1}{2} \ab] &\text{for } T < T_\mathrm{c} \mathrm{,} \\ 
		\sqrt{1+\zeta} \ab[\frac{1-\zeta}{\pi \zeta}\mathrm{K}(1/\zeta) + \frac{1}{2} \ab] &\text{for } T > T_\mathrm{c} \mathrm{.} \\ 
	\end{cases}
\end{equation}
with $\mathrm{K}$ the complete elliptic integral of the first kind and the parameter $\zeta$ defined as $\zeta=\sinh^{-2}(2J/\kBT)$~\cite{Correlations2DIsing}. \\\par

The result of a \hotspice simulation of this Ising system is shown in \cref{fig:2:Verification_exchange}, as calculated for an $800 \times 800$ lattice at various temperatures $T$.
Simulations were performed using different amounts of \link{Monte Carlo sweep}{Monte Carlo sweeps (MCS)} (i.e., the amount of attempted switches divided by the number of magnets $N$) per temperature step.
Maximal \xref{multi-switching} ($Q=+\infty$) during \xref{Metropolis-Hastings sampling} was used to verify that --- at least for systems with such short-range coupling --- this sort of extreme multi-switching still converges to the correct solution.
Since the theoretical curves for $\langle M \rangle / M_0$ and $\langle s_i s_{i+1} \rangle$ are monotonically decreasing, the final state from the previous temperature step was retained as the starting point for the next step.
This preserves progress already made towards equilibrium and avoids the disruption that resetting to the uniform state would cause~\cite{MCinStatPhys}. \par
The \hotspice result corresponds well to the theoretical predictions of~\cref{eq:2:Verification_exchange_M} and~\cref{eq:2:Verification_exchange_corr}, both below $T_\mathrm{c}$ and in the high-temperature limit.
The average magnetisation $\langle M \rangle / M_0$ follows the theoretical curve more closely than the NN correlation $\langle s_i s_{i+1} \rangle$.
However, just above $T_\mathrm{c}$, the average magnetisation $\langle M \rangle / M_0$ evolves only slowly towards the expected value.
Increasing the number of \link{Monte Carlo sweep}{MCS} per temperature step brings the system closer to (the theoretical) equilibrium and reduces the temperature range above $T_\mathrm{c}$ where the simulated system is not at equilibrium.
However, this yields diminishing returns, as every additional Monte Carlo step converges more slowly towards the equilibrium.
This slow convergence near the critical point is due to a well-known phenomenon known as ``\xref{critical slowing down}''.

\xfig{2_Hotspice/Verification/OOP_Exchange.pdf}{
	\label{fig:2:Verification_exchange}
	\textbf{(a)} Average magnetisation and \textbf{(b)} nearest-neighbour (NN) correlation as a function of temperature.
	Markers show the \hotspice result for an $800 \times 800$ OOP square-lattice \link{exchange coupling}{exchange-coupled} Ising system using \xref{Metropolis-Hastings sampling} with maximal \xref{multi-switching}.
	The simulation was performed 3 times for different amounts of \link{Monte Carlo sweep}{MCS}, which is the number of \link{Monte Carlo sweep}{Monte Carlo steps per magnet} per temperature step --- i.e., the amount of attempted switches divided by the number of magnets $N$.
	Discrepancies due to \xref{critical slowing down} above $T_c$ improve with increasing MCS.
}
% TODO: ? why the discrepancy in the correlation?

\paragraph{Critical slowing down\indexlabel{critical slowing down}}
As the correlation length $\xi$ diverges at a second-order \xref{phase transition}, the autocorrelation time $\Theta$ will increase as $L^z$ in finite systems of linear size $L$~\cite{NumericalDynamicalNiedermayer}.
The \xlabel{dynamical critical exponent} $z$ is typically $\gtrsim 2$ for local dynamics --- \textit{Niedermayer}~\cite{niedermayer1988general} notes that this is intuitively analogous to the problem of a random walk, where $\order{R^2}$ steps are required to traverse a distance $R$.
The significant rise in the autocorrelation time causes successive Monte Carlo configurations to be highly correlated, resulting in a very slow exploration of the phase space that impedes convergence towards equilibrium~\cite{NumericalDynamicalNiedermayer,CompStatPhys,StatisticalMechanicsAlgorithmsComputations}. \\\par
This phenomenon is termed \emph{critical slowing down} in second-order phase transitions --- supercritical slowing down is an analogous phenomenon for discontinuous transitions~\cite{PhD_Reynal}.
It is particularly problematic for single-spin flip algorithms like \xref{Metropolis-Hastings sampling}, as single-spin flips are no longer physically relevant in this region~\cite{PhD_Reynal}.
Such local-update Monte Carlo algorithms only change a small portion of the system in each step, resulting in minimal differences between successive states.
Consequently, even after many MCS, the number of statistically independent configurations remains low, as the large autocorrelation time yields a very low effective number of independent samples $N_\mathrm{eff} = N_\mathrm{MCS}/\Theta$~\cite{niedermayer1988general,BeatCriticalSlowingDown1990}. \\\par

While CSD is, to some extent, an inherent property of the physical system --- physical quantities like $\xi$ and $\Theta$ diverge near the \xref{phase transition} --- the choice of simulation algorithm also has a significant impact~\cite{PhD_Reynal}.
For example, since CSD originates from long-range correlations, the single-spin-flip nature of the \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm} exacerbates the issue posed by the divergent correlation length $\xi$.
Other algorithms have been devised that alleviate CSD through non-local spin updates, allowing them to act on collective modes~\cite{BeatCriticalSlowingDown1990}.
Among these are the group of \idx{cluster algorithms} --- first the \idx{Swendsen-Wang}~\cite{SwendsenWang} and later the \idx{Niedermayer}~\cite{niedermayer1988general} and \idx{Wolff}~\cite{Wolff} algorithms --- which construct clusters of magnets and flip all magnets in the cluster simultaneously~\cite{CompStatPhys}. % Also multi-grid Monte Carlo~\cite{edwards1991multi}
This generates more distinct states and, if the clusters are constructed correctly, reduces the autocorrelation time.
The clusters are constructed by starting from a `seed' magnet and adding its neighbours to the cluster with a certain probability.
For these cluster algorithms, the dynamical critical exponent $z \approx 0$ in the 2D \link{exchange coupling}{exchange-coupled} Ising system~\cite{NumericalDynamicalNiedermayer}, as compared to $z\approx2.17$ with the Metropolis-Hastings algorithm~\cite{DynamicExponentMetropolis}.
However, in a system with long-range interactions, like the \xref{magnetostatic interaction}, all magnets are ``neighbours'', and generating an appropriate cluster becomes non-trivial.
While methods exist to generate clusters in systems with long-range interactions~\cite{MC_spinLongRange}, they are not easily applied to the variety of systems that \hotspice supports.
For some lattices, though, specific algorithms are available that serve a similar purpose: a notable example of this are the ``loop move'' algorithms for IP square and kagome ASI, which suppress CSD and allow the ground state of these lattices to be reached~\cite{LoopMoves,ChargeOrderingKagome}.
Since CSD will not pose significant issues in this thesis, the Wolff algorithm has been implemented only for \link{exchange coupling}{exchange-coupled} systems in \hotspice.

\subsection{Exchange- and magnetostatically-coupled Ising system}
Including long-range \xref{magnetostatic interactions} into an exchange-coupled square-lattice Ising system significantly alters its behaviour.
While the temperature $T$ remains an important system parameter, now another determining factor is the ratio $\delta = E_{\mathrm{exch},i,j}/E_{\mathrm{MS},i,j}$ ($j \in \mathcal{N}_i$), which represents the balance between the \xref{exchange coupling} and magnetostatic interaction for nearest neighbours.
In terms of \hotspice parameters, we can write
\begin{equation}
	\delta = \frac{8 \pi J a^3}{\mu_0 \mu^2} \mathrm{,}
\end{equation}
with the magnetic moment $\mu$ constant for all magnets, and $a$ the NN distance in the square lattice.
Analytical predictions remain possible in this system at zero temperature: for $\delta < 0.85$, the magnetostatic coupling dominates, leading to a \xlabel[nolabel]{checkerboard state}~\cite{AgingIsingDipolar}.
As $\delta$ increases, the ever-stronger exchange coupling leads to the formation of ferromagnetic domains, which organise into stripes due to the magnetostatic interaction, with the stripe width determined by $\delta$~\cite{StripedDipolarIsing,FMmonolayer1993}. \par


\vspace{-1.5em}
\xfig{2_Hotspice/Verification/OOP_Dipolar.pdf}{
	\label{fig:2:Verification_dipolar}
	\link{nearest-neighbour correlation}{Nearest-neighbour (NN) correlation} in an \link{exchange coupling}{exchange-coupled} system with long-range \xref{magnetostatic interactions} included, as a function of relative NN magnetostatic/exchange coupling $\delta$.
	Transitions occur at $\delta=0.85$ and 2.65, indicated by dotted lines.
	Insets show the magnetisation state with growing \xlabel{stripe domains}: white corresponds to spin `up', black to `down'.
}
\vspace{-0.75em}

The average stripe width is reflected in the \idx{nearest-neighbour correlation} $\langle s_i s_{i+1} \rangle$, as shown in~\cref{fig:2:Verification_dipolar} for a \hotspice simulation.
The analytical theory is valid at zero temperature, but we are using the \link{Metropolis-Hastings sampling}{Metropolis-Hastings algorithm} which requires non-zero temperature.
Hence, we will use a sufficiently low temperature ($T \ll E_\mathrm{MS}$; here, $T=\SI{50}{\kelvin}$) for this simulation. % E_MS in this situation is around 200kB.


Consistent with the theoretical predictions of~\ccite{StripedDipolarIsing}, for $\delta < 0.85$ a \xref{checkerboard state} exists with $\langle s_i s_{i+1} \rangle = -1$.
In the range $0.85 < \delta < 2.65$, \xref{stripe domains} with a width of 1 row are preferred, leading to $\langle s_i s_{i+1} \rangle = 0$.
Beyond $\delta=2.65$, a 2-row width becomes preferable with $\langle s_i s_{i+1} \rangle = 0.5$.
Increasing $\delta$ further leads to ever wider stripe domains, and in the limit $\delta \rightarrow +\infty$ the correlation approaches $\langle s_i s_{i+1} \rangle \rightarrow 1$.
The phenomenon of CSD is also present here, albeit less severely.

\subsection{Square-to-pinwheel transition angle}\label{sec:2:Verification_IP_SquarePinwheel}
The in-plane square and pinwheel ASI lattices can be transformed into each other by rotating each individual magnet by $\alpha = \ang{45}$.
Despite being this closely related, they exhibit a very different ground state magnetic ordering.
Square ASI has an antiferromagnetic (AFM) ground state, where all vertices have a net zero magnetisation.
On the other hand, pinwheel ASI exhibits \idx{superferromagnetism}, where all magnets with similarly-oriented \link{easy axis}{easy axes} are magnetised in the same direction~\cite{ApparentFMpinwheel}.
Therefore, if we consider the continuous range of rotation angles $\ang{0} \leq \alpha \leq \ang{45}$, a critical angle $\ang{0} < \alpha_c < \ang{45}$ must exist where the ground state transitions between these two extremes. \par
For the dipole model, theoretical calculations predict this transition at $\alpha_c = \arcsin(\sqrt{3}/3) = \ang{35.3}$~\cite{AFM-FM-transition-Pinwheel,MagicAngle}.
For the \xref{dumbbell model}, the transition angle depends on the distance $d$ between the \xref{magnetic charges} within a magnet, but is always larger than for the dipole model~\cite{AFM-FM-transition-Pinwheel}. \\\par

The result of a \hotspice simulation using both models is shown in \cref{fig:2:Pinwheel_angle}.
To quantify this transition, we measure the fraction of vertices with net zero magnetisation --- this value is 0 for superferromagnetic order while it is 1 for AFM order.
We used the same lattice compression as described in~\ccite{AFM-FM-transition-Pinwheel}, making our lattice spacing $a=\SI{240}{\nano\metre} / \sin(\ang{45}+\alpha)$ angle-dependent: it varies from \SI{340}{\nano\metre} at $\alpha=\ang{0}$ to \SI{240}{\nano\metre} at $\alpha=\ang{45}$. 
For the dipole model, the transition occurs at $\approx \ang{35}$ as expected, while the dumbbell model (here with $d = \SI{220}{\nano\metre}$) indeed transitions at a larger rotation angle.

\vspace{-1em}
\xfig[0.7]{2_Hotspice/Verification/Pinwheel_angle.pdf}{
	\label{fig:2:Pinwheel_angle}
	Fraction of vertices with net zero magnetisation at equilibrium, as square ASI (left) transitions to pinwheel ASI (right) by rotating individual magnets.
	The theoretical transition angle $\alpha_c \approx \ang{35.3}$ for the dipole model is indicated by the vertical dotted line.
	The dumbbell model uses a charge-to-charge distance $d = \SI{220}{\nano\metre}$.
}
\vspace{-0.5em}

The transition in the dumbbell model seems less abrupt: a stable intermediate state exists at $\alpha=\ang{40}$ with significant random variation between samples, as indicated by the error bar. % A significant error bar is often indicative of a phase transition
A gradual transition has also been observed experimentally, though the reported range of angles $\alpha$ between which this occurs is often broader and both smaller~\cite{ProbingAFM-PMtransition} and larger~\cite{AFM-FM-transition-Pinwheel} rotation angles than $\ang{40}$ have been reported.
This variance between experiments may be due to differences in defects or quenching rate, whereas \link{Metropolis-Hastings sampling}{Metropolis-Hastings} samples the equilibrium state space, yielding the rather sharp transition seen in the figure.
% While these transitions were often experimentally observed to be gradual from \ang{35} to nearly \ang{45}, some of these experiments likely could not reach an equilibrium state due to finite correlation times, all of which freeze domain walls in the system.

\newpage
\section{Applications}
Whereas the previous section verified that \hotspice works as intended, this section will investigate the impact of the different model improvements proposed in~\cref{sec:2:Energy}.
Firstly, accounting for the \link{asymmetric energy barrier}{asymmetry} between the clockwise and counter-clockwise effective energy barriers is most noticeable in pinwheel ASI, where each magnet points perpendicularly into the side of its nearest neighbours.
Hence, a \xref{hysteresis} in pinwheel ASI will be used to investigate the importance of the parameter $\rho = \mu_\perp/\mu_\parallel > 0$ that models \xref{non-coherent magnetisation reversal} in~\cref{eq:2:EB_asymmetric}.
Secondly, the \link{finite dipole model}{finite-size corrections} are most impactful in ASI where magnets are aligned end-to-end, such as kagome ASI.
Therefore, we will also perform a hysteresis in kagome ASI to unveil some differences between the \xref{point dipole model} and \xref{dumbbell model}.
Note that hysteresis can not be simulated using \xref{Metropolis-Hastings sampling}, because it samples the equilibrium state space. % ignores the \xref{uniaxial anisotropy} (i.e., the energy barrier $\EB$).
Since a hysteresis only exists because the equilibrium is not reached within the timescale of the observation, Metropolis-Hastings sampling would therefore not capture the experimentally observed hysteresis at all.
Hence, the \xref{first-switch method} must be used, with an appropriate maximum switching time $t_\mathrm{max}$. 

\subsection{Pinwheel reversal}\label{sec:2:Applications_reversal_Pinwheel}\indexlabel[nolabel]{asymmetric energy barrier}
Pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a-b}) can be seen as consisting of two intertwined sublattices whose magnets are perpendicular to each other.
The ground state of this system consists of \link{superferromagnetism}{superferromagnetic} domains, where all magnets within each sublattice are magnetised in the same direction~\cite{EmergentChiralityRatchet,ApparentFMpinwheel,RC_ASI}.
This lattice exhibits \idx{hysteresis}: when a strong field drives it towards a uniform state, it will remain in that state after the field is removed since this is a ground state of the system. \par
Li~\etal~\cite{li2018pinwheel} performed an experimental study to observe the reversal of `diamond'-edge pinwheel ASI (\crefSubFigRef{fig:2:ASIs}{a}) under the influence of an \link{external magnetic field}{external field}.
Notably, when they applied the field at an oblique angle (\ang{30}) to the ASI edges, the reversal occurred in two distinct steps: the sublattice which was more aligned with the field (\ang{15} to the easy axis) reversed first, followed by the second sublattice (for which the field was at \ang{75}).
To accurately reproduce the second reversal step in simulation, the \link{asymmetric energy barrier}{asymmetry in the energy barrier} between clockwise and counter-clockwise switching must be accounted for, due to the near-perpendicular field (\ang{75}) that the second sublattice experiences. \par
We replicated this experiment using \hotspice for a few values of $\rho=\mu_\perp/\mu_\parallel$ to clearly observe the effect of the asymmetric barrier, resulting in the hysteresis loops shown in~\cref{fig:2:Hysteresis_Pinwheel}.
When possible, simulation parameters were derived from the experimental configuration: an ASI of $25 \times 25$ unit cells with a NN centre-to-centre distance of \SI{420}{\nano\meter} was used, at room temperature (\SI{300}{\kelvin}).
The \xref{magnetic moment} and \link{shape anisotropy}{energy barrier} of each magnet can be derived from the geometry of the $470\times170\times\SI{10}{\nano\metre}$ stadium-shaped magnets and the \xref{permalloy} \xref{saturation magnetisation} $M_\mathrm{sat}=\SI{800}{\kilo\ampere\per\metre}$.
This yields a magnetic moment $\mu = M_\mathrm{sat} V = \SI{0.59}{\femto\ampere\metre\squared}$, and a \mumax~\cite{mumax3} simulation reveals that the energy barrier of such magnets is $\approx \SI{60}{\electronvolt}$.
These values result in a good match with experiment as far as the field magnitude of the first reversal step is concerned.
The simulations used the \xref{point dipole model}; performing the same simulations with the \xref{dumbbell model} results in a nearly identical hysteresis since changing to the dumbbell model has little effect on pinwheel ASI.

\vspace{-1.6em}
\xfig{2_Hotspice/Hysteresis_Pinwheel.pdf}{
	\label{fig:2:Hysteresis_Pinwheel}
	\link{hysteresis}{Hysteresis} of pinwheel ASI for an \xref{external magnetic field} applied at \ang{30} with respect to the system edges.
	The parameter $\rho = \mu_\perp/\mu_\parallel$ modulates the effect of the \xref{asymmetric energy barrier} as described in~\cref{sec:2:E_B_asymm}.
	\textbf{(a)} Component of the average \xref{magnetisation} along the direction of the external field.
	The experimental hysteresis by Li~\etal~\cite{li2018pinwheel} is shown in black.
	\textbf{(b)} In-plane path of the average magnetisation vector.
	Triangles pointing up (down) correspond to the ramp-up (down) of the external field, whose direction is indicated by the double-headed arrow.
	\textbf{(c-e)} Snapshots of the system when passing through threshold values of $M_\parallel/M_{\parallel,\mathrm{sat}}$, in chronological order through the hysteresis loop.
	The colour of each pixel shows the local average magnetisation angle of the ASI, as encoded by the colour wheel shown on the right.
	Snapshots (1,5,9) are taken near the saturated $M_\parallel/M_{\parallel,\mathrm{sat}} \approx \pm 1$ state, (3) and (7) at the zero-average $M_\parallel/M_{\parallel,\mathrm{sat}} = 0$, and (2,4,6,8) at $M_\parallel/M_{\parallel,\mathrm{sat}} = \pm\sqrt{3}/3 \approx 0.58$, i.e., the value at the plateau between the two steps for $\rho=0$ and $\rho=0.4$.
}
\vspace{-1.25em}

In the experiment, the two reversal steps occurred gradually over a range of fields, resembling more of an S-curve rather than a sharp step.
This is due to a random spread on the \xlabel[nolabel]{coercive field} of each nanomagnet due to imperfections in lithography~\cite{fraleigh2017characterization}. % REF: 'fraleigh2017characterization' examines this spread in OOP square: they infer the coercive field distribution of the magnets in their system from the width of the hysteresis loop.
We modelled this in the simulation by introducing a random Gaussian variation on the energy barrier, with $\sigma(E_\mathrm{B})/\langle E_\mathrm{B} \rangle=\SI{7}{\percent}$ yielding the closest agreement to the experiment.
Note that the experimental \xref{hysteresis} curve is not symmetric, in contrast to our simulations, which Li~\etal attributed to a small sample movement during their measurement which changed the applied field angle with respect to the array. \par
As can be noted from the figure, the resulting \xref{hysteresis}, and particularly the \link{external magnetic field}{field} magnitude for the second reversal step, is highly dependent on the choice of $\rho=\mu_\perp/\mu_\parallel$.
When the \link{asymmetric energy barrier}{asymmetry of the energy barrier} is neglected (i.e., $\rho=0$, equivalent to using the \xref{mean-barrier model}), the field magnitude required for the second step is significantly overestimated, as seen in the blue curve of~\crefSubFigRef{fig:2:Hysteresis_Pinwheel}{a} ($\approx\SI{60}{\milli\tesla}$ vs. experiment $\approx\SI{25}{\milli\tesla}$).
Increasing $\rho$ reduces the field magnitude at which the second step occurs, but has little effect on the first step.
This is understandable, since the first step corresponds to magnets that are nearly aligned (\ang{15}) with the external field, so the asymmetry between the two switching channels is not as pronounced for them.
When $\rho \gtrapprox 0.6$, the second step disappears entirely, with the reversal instead occurring by domain nucleation at the edges of the ASI, as illustrated in \crefSubFigRef{fig:2:Hysteresis_Pinwheel}{e}. \par
Hence, $\rho \approx 0.4$ yields the best correspondence to the experiment.
A possible explanation for this is that the magnets in the experimental ASI do not switch via uniform rotation, as is assumed by the N\'eel-Arrhenius switching law (\cref{eq:2:Néel}).
Indications of non-uniform rotation or domain wall-mediated reversal were previously noticed by Morley~\etal~\cite{VogelFulcherTammannFreezing}.

\subsection{Kagome reversal}
In-plane kagome ASI (\crefSubFigRef{fig:2:ASIs}{g}) is a typical example of a \link{frustration}{frustrated} system: every vertex is formed by three magnets that must obey the two-in/one-out or two-out/one-in \xref{ice rule}.
Each magnet has four NN located near its endpoints, making this lattice an interesting testing ground for the \xref{dumbbell model}.
Both the \xref{point dipole model}~\cite{Chern2011} and dumbbell model~\cite{Moller2009} have been used in previous studies of kagome ASI.
The point dipole model tends to significantly underestimate the NN interaction, while the dumbbell model increases the \xref{magnetostatic interaction} energy between closely spaced magnets, potentially affecting the dynamics of the system.
We therefore expect the dumbbell model to provide a more accurate simulation of the kagome lattice than the point dipole model~\cite{flatspin,mengotti2011kagome}. \par
To illustrate this, we used \hotspice to reproduce the reversal process of kagome ASI as observed by Mengotti~\etal~\cite{mengotti2011kagome}.
They studied the hysteresis loop using \indexlabel[nolabel]{X-ray magnetic circular dichroism}\link{X-ray magnetic circular dichroism}{XMCD} to image the evolution of the system's microstate during a gradual increase of the \link{external magnetic field}{external field}.
We simulated a kagome ASI consisting of 173 magnets whose \link{shape anisotropy}{energy barrier} $\EB = \SI{120}{\electronvolt} \pm \SI{5}{\percent}$.
Kagome ASI contains 3 sublattices of magnets with different orientations: an increasing magnetic field $\vc{B}_\mathrm{ext}$ was parallel to the \xref{easy axis} of one of them, with a slight \ang{-3.6} offset to break the symmetry between the two other sublattices that are slanted with respect to the field.
This results in the creation of \idx{Dirac strings} of switched magnets. \par
As the field strength increases, the \xref{Zeeman energy} of all magnets rises.
This increase is fastest for those mostly parallel to the field, but their total \link{magnetostatic interaction}{magnetostatic energy} in the initial state is lower than for the other two sublattices.
The balance between these two contributions results in a \xlabel{critical energy barrier} $E_\mathrm{B}^\mathrm{c}$: when the energy barrier $E_\mathrm{B}$ is lower (higher) than this value, a slanted (parallel) magnet will flip first, as illustrated in~\cref{fig:2:kagome_reversal_EBc}.
Therefore, the dynamics may depend on the simulation method, as the dumbbell model increases the magnetostatic interaction energy between nearest neighbours.

\sidefig[0.45]{2_Hotspice/Hysteresis_Kagome_EBc.pdf}{
	\label{fig:2:kagome_reversal_EBc}
	The effect of an \xref{external magnetic field} on magnets in kagome ASI that are aligned with the field ($\,\boldsymbol{|}\,$) and slanted with respect to the field (\rotatebox[origin=c]{120}{$\boldsymbol{|}$}), for different values of the energy barrier.
	$E_\mathrm{MS}$ is the same for each $E_\mathrm{B}$, however it is higher for the slanted magnets compared to those parallel to the field.
	When $E_\mathrm{B}<E_\mathrm{B}^c$ ($E_\mathrm{B}>E_\mathrm{B}^c$), the slanted (parallel) magnet will flip first since the Zeeman energy $E_\mathrm{Z}$ increases faster for the parallel magnets.
	\newline
}
\vspace{-.75em}

The results, shown in~\cref{fig:2:kagome_reversal}, reveal a qualitative difference between the dumbbell and dipole models.
In the \xref{dumbbell model}, slanted magnets on the boundaries of the ASI flip first.
With the \xref{point dipole model}, however, bulk magnets parallel to the field (vertical in the figure) flip first~\cite{DDG_Masterproef}.
In the experiment of Mengotti~\etal~\cite{mengotti2011kagome}, \xref{Dirac strings} were observed to originate from slanted magnets, which is in agreement with the dumbbell simulation. % But they saw them in the bulk, which is not in agreement with the dumbbells.
Furthermore, using other ASI software that employs a \xref{Stoner-Wohlfarth model} (a type of point dipole model), \xref{Dirac strings} were observed to originate from magnets parallel to the field~\cite{flatspin}, similar to our results with a point dipole model.
This highlights the importance of accounting for the finite size of magnets to obtain a good estimate for the magnetostatic interaction between NN, which allows distinct dynamics to be captured as compared to a point dipole model.
% However, the exact energy barriers in the experiment were unclear, as they varied between magnets, with some barriers being lowered and others raised.

\vspace{-1.25em}
\xfig{2_Hotspice/Hysteresis_Kagome.pdf}{
	\label{fig:2:kagome_reversal}
	Reversal of kagome ASI under an \xref{external magnetic field} applied at \ang{-93.6} relative to the horizontal axis.
	The \link{dumbbell model}{dumbbell} and \link{point dipole model}{point dipole} models are compared at different external field magnitudes $B$.
	The critical field $B_C$ is \SI{26.7}{\milli\tesla} for the dumbbell model, but only \SI{23.9}{\milli\tesla} for the point dipole model.
	The NN centre-to-centre distance is \SI{500}{\nano\metre}, with a length $d=\SI{470}{\nano\metre}$ for the dumbbell model.
	The magnets have a \xref{magnetic moment} $\mu=\SI{1.1}{\femto\ampere\metre\squared}$, \link{shape anisotropy}{energy barrier} $\EB = \SI{120}{\electronvolt} \pm \SI{5}{\percent}$ and use the \xref{mean-barrier model} ($\rho=0$).
	Arrows represent the magnetic moment of the magnets, and the blue and red points ({\color{blue}{$\bullet$}} and {\color{red}{$\bullet$}}) represent the net change in charge of a vertex.
}

\newpage
\section{Conclusion}
Throughout this chapter, the underlying model and implementation of the \hotspice ASI simulator have been discussed in detail.
A couple of improvements to the underlying \xref{point dipole model} were proposed.
The first considers more accurate calculations for the \xref{magnetostatic interactions} by accounting for the finite size of magnets.
In OOP systems, it was found that more realistic values for the magnetostatic energy $E_\mathrm{MS}$ could be obtained through a \link{finite dipole model}{second-order correction}, while for IP systems a \xref{dumbbell model} is more effective in reproducing experimental results.
The influence of these corrections can result in qualitatively different dynamics as compared to a \xref{point dipole model}, as was for instance observed in the kagome lattice.
A second improvement concerns the calculation of the \xref{effective energy barrier}.
There, accounting for \link{asymmetric energy barrier}{asymmetric switching channels} proved key to, e.g., correctly reproduce \xref{coercive fields} in pinwheel ASI, provided that the reduced magnetisation during \link{non-coherent magnetisation reversal}{non-coherent reversal processes} was also accounted for.
While it is possible to obtain an exact solution for the energy barrier in OOP magnets, the assumption of a sinusoidal \xref{energy landscape} is likely inaccurate due to non-coherent reversal in real OOP nanomagnets. \par
Furthermore, two distinct algorithms have been implemented to handle switching events: the \xref{first-switch method} and \xref{Metropolis-Hastings sampling}.
They are both useful in different circumstances, as the former is best suited to simulate temporal (out-of-equilibrium) dynamics, while the latter can more efficiently explore equilibrium configurations.
In particular, the MH algorithm can benefit from \link{multi-switching}{multiple simultaneous switching events}, which is justifiable if these switching magnets are all sufficiently distant.
It was verified that this yields correct results for a set of test cases on various lattices, and the accuracy of the first-switch method was later assessed in pinwheel and kagome ASI through comparison with experimental results. \par
The usage of a grid to represent ASI brought with it several advantages, at the cost of some geometrical restrictions.
In particular, the grid enables the efficient calculation of the \xref{magnetostatic interaction} between all magnets in the system, whereas other codes typically implement a \link{truncated kernel}{truncation distance} to improve performance.
However, we found that truncation, even at a distance as far as 20 grid cells, incurs a \SIrange{5}{10}{\percent} error on the total magnetostatic interaction energy of a magnet.
Additionally, the grid facilitates the simultaneous sampling of sufficiently distant magnets in the MH algorithm. \\\par

These factors make \hotspice a versatile tool that can complement \link{micromagnetic theory}{micromagnetic simulations} in the study of ASI: by sacrificing the internal magnetisation details of individual nanomagnets, these higher-level approximations make it possible to simulate complex dynamics in larger arrays over significantly longer timescales.
We have shown that different systems call for different kinds of model improvements or update algorithms, of which we provided several notable examples: for instance, changing from a point dipole to a dumbbell model was found to have a significant impact on kagome ASI, whereas pinwheel ASI is barely affected by this. % not every model improvement is relevant to every lattice, nor does a single update scheme suit all simulation goals.
The ability of \hotspice to quickly sweep parameters and evaluate RC metrics facilitates the optimisation of ASI configurations and the identification of suitable input protocols; an ability that will be put to good use in the next chapter.
